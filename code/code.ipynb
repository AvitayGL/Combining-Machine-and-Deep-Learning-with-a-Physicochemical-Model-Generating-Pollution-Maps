{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "DEVELOPED BY : AVITAY GELTMAN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sklearn version: 1.2.0\n",
    "xgboost version: 0.90\n",
    "torch version: 1.10.1+cpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn version:  1.2.0\n",
      "XGBoost version:  0.90\n",
      "Torch version:  1.10.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "print(\"Scikit-learn version: \", sklearn.__version__)\n",
    "print(\"XGBoost version: \", xgb.__version__)\n",
    "print(\"Torch version: \", torch.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics\n",
    "from colorama import Fore\n",
    "from colorama import Style\n",
    "import warnings\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [
    {
     "data": {
      "text/plain": "                       time  hour            station region   type       X  \\\n0      01-Jan-2018 06:00:01     6          EILAT_IEC    EIL  Gener  194714   \n1      01-Jan-2018 06:00:01     6          GOLDWATER    EIL  Gener  194714   \n2      01-Jan-2018 06:00:01     6            YERUHAM    NGV  Gener  193051   \n3      01-Jan-2018 06:00:01     6     NEGEV_JUNCTION    NGV  Gener  183567   \n4      01-Jan-2018 06:00:01     6  RAMAT_HOVAV_SOUTH    NGV    NaN  180247   \n...                     ...   ...                ...    ...    ...     ...   \n39143  23-Dec-2020 13:00:01    13             IRONID    GUD  Trans  180414   \n39144  23-Dec-2020 13:00:01    13          EHAD_HAAM    GUD  Trans  187998   \n39145  23-Dec-2020 13:00:01    13              ARIEL    SHM  Gener  216136   \n39146  23-Dec-2020 13:00:01    13          YAD_AVNER    GUD  Gener  181757   \n39147  23-Dec-2020 13:00:01    13            RAANANA    SHC  Trans  189030   \n\n            Y       long        lat   HASL  ...         ZH       modCO  \\\n0      385170  34.949213  29.554107   49.0  ...  40.010006  134.363251   \n1      385170  34.949213  29.554107   49.0  ...  40.010006  134.363251   \n2      544467  34.928056  30.991001  486.0  ...  39.572067  145.912949   \n3      551454  34.828516  31.053768  382.0  ...  39.788353  148.871063   \n4      559163  34.793436  31.123191  331.0  ...  39.851860  150.837677   \n...       ...        ...        ...    ...  ...        ...         ...   \n39143  666788  34.790918  32.093806   12.0  ...  40.060757  142.744675   \n39144  667486  34.871234  32.100337   28.0  ...  40.240028  139.635483   \n39145  667823  35.169332  32.103810  546.0  ...  39.514626  121.745392   \n39146  669624  34.805033  32.119426   39.0  ...  39.958965  146.506393   \n39147  676220  34.881890  32.179126   54.0  ...  40.201035  134.747223   \n\n          modNO     modNO2      modO3    modPM10   modPM25    modSO2   NO2  \\\n0      0.069079   0.406160  43.204922   9.717587  4.607586  0.217570   6.9   \n1      0.069079   0.406160  43.204922   9.717587  4.607586  0.217570   6.9   \n2      0.366279   1.983642  38.630047   9.702694  5.669499  1.268337   4.3   \n3      0.036141   0.778355  38.563164  10.352297  5.708193  0.745236   3.7   \n4      0.299864   1.928364  36.792328  11.010432  6.628351  4.130837   2.4   \n...         ...        ...        ...        ...       ...       ...   ...   \n39143  3.144137  19.214502  24.201452   7.450971  5.922740  3.608224  14.8   \n39144  1.977291  12.908053  26.793800   7.404082  6.374056  1.958934  25.5   \n39145  0.305540   2.528420  37.275600   2.038297  1.546840  0.403958   6.4   \n39146  4.201608  22.097006  20.609669   7.887576  6.340599  4.807429   4.9   \n39147  1.475349  10.152210  30.175163   7.810445  6.313896  1.758563  12.6   \n\n             Err  \n0       6.493840  \n1       6.493840  \n2       2.316358  \n3       2.921645  \n4       0.471636  \n...          ...  \n39143  -4.414502  \n39144  12.591947  \n39145   3.871580  \n39146 -17.197006  \n39147   2.447790  \n\n[39148 rows x 46 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>hour</th>\n      <th>station</th>\n      <th>region</th>\n      <th>type</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>long</th>\n      <th>lat</th>\n      <th>HASL</th>\n      <th>...</th>\n      <th>ZH</th>\n      <th>modCO</th>\n      <th>modNO</th>\n      <th>modNO2</th>\n      <th>modO3</th>\n      <th>modPM10</th>\n      <th>modPM25</th>\n      <th>modSO2</th>\n      <th>NO2</th>\n      <th>Err</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>01-Jan-2018 06:00:01</td>\n      <td>6</td>\n      <td>EILAT_IEC</td>\n      <td>EIL</td>\n      <td>Gener</td>\n      <td>194714</td>\n      <td>385170</td>\n      <td>34.949213</td>\n      <td>29.554107</td>\n      <td>49.0</td>\n      <td>...</td>\n      <td>40.010006</td>\n      <td>134.363251</td>\n      <td>0.069079</td>\n      <td>0.406160</td>\n      <td>43.204922</td>\n      <td>9.717587</td>\n      <td>4.607586</td>\n      <td>0.217570</td>\n      <td>6.9</td>\n      <td>6.493840</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>01-Jan-2018 06:00:01</td>\n      <td>6</td>\n      <td>GOLDWATER</td>\n      <td>EIL</td>\n      <td>Gener</td>\n      <td>194714</td>\n      <td>385170</td>\n      <td>34.949213</td>\n      <td>29.554107</td>\n      <td>49.0</td>\n      <td>...</td>\n      <td>40.010006</td>\n      <td>134.363251</td>\n      <td>0.069079</td>\n      <td>0.406160</td>\n      <td>43.204922</td>\n      <td>9.717587</td>\n      <td>4.607586</td>\n      <td>0.217570</td>\n      <td>6.9</td>\n      <td>6.493840</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>01-Jan-2018 06:00:01</td>\n      <td>6</td>\n      <td>YERUHAM</td>\n      <td>NGV</td>\n      <td>Gener</td>\n      <td>193051</td>\n      <td>544467</td>\n      <td>34.928056</td>\n      <td>30.991001</td>\n      <td>486.0</td>\n      <td>...</td>\n      <td>39.572067</td>\n      <td>145.912949</td>\n      <td>0.366279</td>\n      <td>1.983642</td>\n      <td>38.630047</td>\n      <td>9.702694</td>\n      <td>5.669499</td>\n      <td>1.268337</td>\n      <td>4.3</td>\n      <td>2.316358</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01-Jan-2018 06:00:01</td>\n      <td>6</td>\n      <td>NEGEV_JUNCTION</td>\n      <td>NGV</td>\n      <td>Gener</td>\n      <td>183567</td>\n      <td>551454</td>\n      <td>34.828516</td>\n      <td>31.053768</td>\n      <td>382.0</td>\n      <td>...</td>\n      <td>39.788353</td>\n      <td>148.871063</td>\n      <td>0.036141</td>\n      <td>0.778355</td>\n      <td>38.563164</td>\n      <td>10.352297</td>\n      <td>5.708193</td>\n      <td>0.745236</td>\n      <td>3.7</td>\n      <td>2.921645</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>01-Jan-2018 06:00:01</td>\n      <td>6</td>\n      <td>RAMAT_HOVAV_SOUTH</td>\n      <td>NGV</td>\n      <td>NaN</td>\n      <td>180247</td>\n      <td>559163</td>\n      <td>34.793436</td>\n      <td>31.123191</td>\n      <td>331.0</td>\n      <td>...</td>\n      <td>39.851860</td>\n      <td>150.837677</td>\n      <td>0.299864</td>\n      <td>1.928364</td>\n      <td>36.792328</td>\n      <td>11.010432</td>\n      <td>6.628351</td>\n      <td>4.130837</td>\n      <td>2.4</td>\n      <td>0.471636</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39143</th>\n      <td>23-Dec-2020 13:00:01</td>\n      <td>13</td>\n      <td>IRONID</td>\n      <td>GUD</td>\n      <td>Trans</td>\n      <td>180414</td>\n      <td>666788</td>\n      <td>34.790918</td>\n      <td>32.093806</td>\n      <td>12.0</td>\n      <td>...</td>\n      <td>40.060757</td>\n      <td>142.744675</td>\n      <td>3.144137</td>\n      <td>19.214502</td>\n      <td>24.201452</td>\n      <td>7.450971</td>\n      <td>5.922740</td>\n      <td>3.608224</td>\n      <td>14.8</td>\n      <td>-4.414502</td>\n    </tr>\n    <tr>\n      <th>39144</th>\n      <td>23-Dec-2020 13:00:01</td>\n      <td>13</td>\n      <td>EHAD_HAAM</td>\n      <td>GUD</td>\n      <td>Trans</td>\n      <td>187998</td>\n      <td>667486</td>\n      <td>34.871234</td>\n      <td>32.100337</td>\n      <td>28.0</td>\n      <td>...</td>\n      <td>40.240028</td>\n      <td>139.635483</td>\n      <td>1.977291</td>\n      <td>12.908053</td>\n      <td>26.793800</td>\n      <td>7.404082</td>\n      <td>6.374056</td>\n      <td>1.958934</td>\n      <td>25.5</td>\n      <td>12.591947</td>\n    </tr>\n    <tr>\n      <th>39145</th>\n      <td>23-Dec-2020 13:00:01</td>\n      <td>13</td>\n      <td>ARIEL</td>\n      <td>SHM</td>\n      <td>Gener</td>\n      <td>216136</td>\n      <td>667823</td>\n      <td>35.169332</td>\n      <td>32.103810</td>\n      <td>546.0</td>\n      <td>...</td>\n      <td>39.514626</td>\n      <td>121.745392</td>\n      <td>0.305540</td>\n      <td>2.528420</td>\n      <td>37.275600</td>\n      <td>2.038297</td>\n      <td>1.546840</td>\n      <td>0.403958</td>\n      <td>6.4</td>\n      <td>3.871580</td>\n    </tr>\n    <tr>\n      <th>39146</th>\n      <td>23-Dec-2020 13:00:01</td>\n      <td>13</td>\n      <td>YAD_AVNER</td>\n      <td>GUD</td>\n      <td>Gener</td>\n      <td>181757</td>\n      <td>669624</td>\n      <td>34.805033</td>\n      <td>32.119426</td>\n      <td>39.0</td>\n      <td>...</td>\n      <td>39.958965</td>\n      <td>146.506393</td>\n      <td>4.201608</td>\n      <td>22.097006</td>\n      <td>20.609669</td>\n      <td>7.887576</td>\n      <td>6.340599</td>\n      <td>4.807429</td>\n      <td>4.9</td>\n      <td>-17.197006</td>\n    </tr>\n    <tr>\n      <th>39147</th>\n      <td>23-Dec-2020 13:00:01</td>\n      <td>13</td>\n      <td>RAANANA</td>\n      <td>SHC</td>\n      <td>Trans</td>\n      <td>189030</td>\n      <td>676220</td>\n      <td>34.881890</td>\n      <td>32.179126</td>\n      <td>54.0</td>\n      <td>...</td>\n      <td>40.201035</td>\n      <td>134.747223</td>\n      <td>1.475349</td>\n      <td>10.152210</td>\n      <td>30.175163</td>\n      <td>7.810445</td>\n      <td>6.313896</td>\n      <td>1.758563</td>\n      <td>12.6</td>\n      <td>2.447790</td>\n    </tr>\n  </tbody>\n</table>\n<p>39148 rows Ã— 46 columns</p>\n</div>"
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unmark the desired pollutant (NO2/PM2.5)\n",
    "pollutant = 'NO2'\n",
    "# pollutant = 'PM2.5'\n",
    "\n",
    "# to load the data, make sure the path for the .csv file is correct\n",
    "if pollutant == 'NO2':\n",
    "    data = pd.DataFrame(pd.read_csv(\"NO2COmpletTbl.csv\", parse_dates=True)) # reading data to pandas dataframe\n",
    "else: #PM2.5\n",
    "    data = pd.DataFrame(pd.read_csv(\"PM25CompletTbl.csv\", parse_dates=True)) # reading data to pandas dataframe\n",
    "\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mThere are no missing values in the dataset\n"
     ]
    }
   ],
   "source": [
    "data_for_corr = data.drop(['time', 'station', 'region', 'type'], axis=1) # dropping columns that are not to be used\n",
    "if data_for_corr.isnull().values.any(): # checking for missing values\n",
    "    print(Fore.RED + \"There are missing values in the dataset\")\n",
    "else:\n",
    "    print(Fore.GREEN + \"There are no missing values in the dataset\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   variable  correlation (absolute)  negative  positive\n0       NO2                0.911383       NaN  0.911383\n1    modSO2                0.283561 -0.283561       NaN\n2    modNO2                0.196556 -0.196556       NaN\n3      soim                0.193011       NaN  0.193011\n4      w10s                0.151239       NaN  0.151239\n5      w10m                0.151210       NaN  0.151210\n6      kzzz                0.142955       NaN  0.142955\n7      hght                0.125088       NaN  0.125088\n8      winz                0.123950       NaN  0.123950\n9      wsta                0.115323       NaN  0.115323\n10     usta                0.110555       NaN  0.110555\n11    modCO                0.103706       NaN  0.103706\n12     hlay                0.097607       NaN  0.097607\n13       ZH                0.095736       NaN  0.095736\n14        z                0.095736       NaN  0.095736\n15     alti                0.095736       NaN  0.095736\n16    modO3                0.095353       NaN  0.095353\n17     sphu                0.089058 -0.089058       NaN\n18     airm                0.084623       NaN  0.084623\n19    modNO                0.083112 -0.083112       NaN\n20     relh                0.081975 -0.081975       NaN\n21     sreh                0.081284 -0.081284       NaN\n22     atte                0.068596 -0.068596       NaN\n23     long                0.067022 -0.067022       NaN\n24        X                0.066505 -0.066505       NaN\n25     HASL                0.066448 -0.066448       NaN\n26  modPM25                0.065411 -0.065411       NaN\n27     HAGL                0.054796 -0.054796       NaN\n28     topo                0.052028 -0.052028       NaN\n29      HGT                0.052028 -0.052028       NaN\n30     swrd                0.051398       NaN  0.051398\n31     temp                0.048351 -0.048351       NaN\n32     tem2                0.044015 -0.044015       NaN\n33     clwc                0.042026 -0.042026       NaN\n34      lat                0.037828 -0.037828       NaN\n35        Y                0.037814 -0.037814       NaN\n36     hour                0.031242       NaN  0.031242\n37     winm                0.018873       NaN  0.018873\n38  modPM10                0.002783 -0.002783       NaN\n39     topc                0.001924       NaN  0.001924\n40     obuk                0.001209 -0.001209       NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>variable</th>\n      <th>correlation (absolute)</th>\n      <th>negative</th>\n      <th>positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NO2</td>\n      <td>0.911383</td>\n      <td>NaN</td>\n      <td>0.911383</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>modSO2</td>\n      <td>0.283561</td>\n      <td>-0.283561</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>modNO2</td>\n      <td>0.196556</td>\n      <td>-0.196556</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>soim</td>\n      <td>0.193011</td>\n      <td>NaN</td>\n      <td>0.193011</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>w10s</td>\n      <td>0.151239</td>\n      <td>NaN</td>\n      <td>0.151239</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>w10m</td>\n      <td>0.151210</td>\n      <td>NaN</td>\n      <td>0.151210</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>kzzz</td>\n      <td>0.142955</td>\n      <td>NaN</td>\n      <td>0.142955</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>hght</td>\n      <td>0.125088</td>\n      <td>NaN</td>\n      <td>0.125088</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>winz</td>\n      <td>0.123950</td>\n      <td>NaN</td>\n      <td>0.123950</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>wsta</td>\n      <td>0.115323</td>\n      <td>NaN</td>\n      <td>0.115323</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>usta</td>\n      <td>0.110555</td>\n      <td>NaN</td>\n      <td>0.110555</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>modCO</td>\n      <td>0.103706</td>\n      <td>NaN</td>\n      <td>0.103706</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>hlay</td>\n      <td>0.097607</td>\n      <td>NaN</td>\n      <td>0.097607</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ZH</td>\n      <td>0.095736</td>\n      <td>NaN</td>\n      <td>0.095736</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>z</td>\n      <td>0.095736</td>\n      <td>NaN</td>\n      <td>0.095736</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>alti</td>\n      <td>0.095736</td>\n      <td>NaN</td>\n      <td>0.095736</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>modO3</td>\n      <td>0.095353</td>\n      <td>NaN</td>\n      <td>0.095353</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>sphu</td>\n      <td>0.089058</td>\n      <td>-0.089058</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>airm</td>\n      <td>0.084623</td>\n      <td>NaN</td>\n      <td>0.084623</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>modNO</td>\n      <td>0.083112</td>\n      <td>-0.083112</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>relh</td>\n      <td>0.081975</td>\n      <td>-0.081975</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>sreh</td>\n      <td>0.081284</td>\n      <td>-0.081284</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>atte</td>\n      <td>0.068596</td>\n      <td>-0.068596</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>long</td>\n      <td>0.067022</td>\n      <td>-0.067022</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>X</td>\n      <td>0.066505</td>\n      <td>-0.066505</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>HASL</td>\n      <td>0.066448</td>\n      <td>-0.066448</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>modPM25</td>\n      <td>0.065411</td>\n      <td>-0.065411</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>HAGL</td>\n      <td>0.054796</td>\n      <td>-0.054796</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>topo</td>\n      <td>0.052028</td>\n      <td>-0.052028</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>HGT</td>\n      <td>0.052028</td>\n      <td>-0.052028</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>swrd</td>\n      <td>0.051398</td>\n      <td>NaN</td>\n      <td>0.051398</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>temp</td>\n      <td>0.048351</td>\n      <td>-0.048351</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>tem2</td>\n      <td>0.044015</td>\n      <td>-0.044015</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>clwc</td>\n      <td>0.042026</td>\n      <td>-0.042026</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>lat</td>\n      <td>0.037828</td>\n      <td>-0.037828</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Y</td>\n      <td>0.037814</td>\n      <td>-0.037814</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>hour</td>\n      <td>0.031242</td>\n      <td>NaN</td>\n      <td>0.031242</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>winm</td>\n      <td>0.018873</td>\n      <td>NaN</td>\n      <td>0.018873</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>modPM10</td>\n      <td>0.002783</td>\n      <td>-0.002783</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>topc</td>\n      <td>0.001924</td>\n      <td>NaN</td>\n      <td>0.001924</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>obuk</td>\n      <td>0.001209</td>\n      <td>-0.001209</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = data_for_corr.corr().loc['Err'].sort_values(ascending=False)[1:] # saving correlation only of \"Err\" with all variables except itself in a table in descending order\n",
    "\n",
    "corr = corr.rename_axis('variable').reset_index(name='correlation') # first column \"variable\" and \"correlation\" second column\n",
    "corr_absolute = corr.copy() # copying the table to a new table which will rank the correlation in absolute values\n",
    "corr_absolute['negative'] = corr_absolute['correlation'].apply(lambda x: x if x < 0 else np.nan) # a column for features with negative correlation\n",
    "corr_absolute['positive'] = corr_absolute['correlation'].apply(lambda x: x if x > 0 else np.nan) # a column for features with positive correlation\n",
    "corr_absolute[\"correlation (absolute)\"] = corr_absolute[\"correlation\"].abs() # a column for absolute values of correlation\n",
    "corr_absolute = corr_absolute.sort_values('correlation (absolute)', ascending=False) # sorting by the absolute correlation values, in descending order\n",
    "corr_absolute = corr_absolute.reset_index(drop=True) # resetting index\n",
    "\n",
    "# plotting the correlation table\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(corr[\"variable\"], corr[\"correlation\"])\n",
    "plt.title(\"Correlation between Estimation Error and other variables\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.show()\n",
    "\n",
    "corr_absolute[[\"variable\", \"correlation (absolute)\", \"negative\", \"positive\"]] # displaying the correlation table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "outputs": [],
   "source": [
    "correlation_fs_names = list(corr_absolute[\"variable\"][1:11]) # saving the names of the top 10 features with highest absolute correlation (Excluding NO2 measurements)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [],
   "source": [
    "y = data['Err'] # target variable\n",
    "X = data.drop(['time', 'station', 'region', 'type', 'Err'], axis=1) # features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Elastic Net Feature Selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet # for more information, see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "if pollutant == 'NO2':\n",
    "    X_scaled = scaler.fit_transform(X.drop(['NO2'], axis=1, inplace=False)) # standardizing the features, and dropping NO2 measurements\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.drop(['NO2'], axis=1, inplace=False).columns) # allocating the standardized features the original names\n",
    "    sfm = SelectFromModel(ElasticNet(alpha=0.2, l1_ratio=0.5, random_state=0)) # creating the model instance. These hyperparameters are chosen after several trials, it ends up with 10 features (excluding actual measurements)\n",
    "else: #PM2.5\n",
    "    X_scaled = scaler.fit_transform(X.drop(['PM25'], axis=1, inplace=False))\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.drop(['PM25'], axis=1, inplace=False).columns)\n",
    "    sfm = SelectFromModel(ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=0))\n",
    "\n",
    "sfm.fit(X_scaled, y) # fitting the model\n",
    "feature_idx = sfm.get_support()\n",
    "feature_name = X_scaled.columns[feature_idx]  # saving the names of the selected features\n",
    "feature_coefficient = sfm.estimator_.coef_[feature_idx] # saving the coefficients of the selected features\n",
    "feature_importance = pd.DataFrame(list(zip(feature_name, feature_coefficient)), columns=['feature', 'coefficient']) # creating a dataframe of the selected features and their coefficients\n",
    "feature_importance['negative'] = feature_importance['coefficient'].apply(lambda x: x if x < 0 else np.nan) # a column for features with negative coefficients\n",
    "feature_importance['positive'] = feature_importance['coefficient'].apply(lambda x: x if x > 0 else np.nan) # a column for features with positive coefficients\n",
    "feature_importance[\"coefficient (absolute)\"] = feature_importance[\"coefficient\"].abs() # a column for absolute values of coefficients\n",
    "feature_importance = feature_importance.sort_values('coefficient (absolute)', ascending=False) # sorting by the absolute correlation values, in descending order\n",
    "feature_importance = feature_importance.reset_index(drop=True) # resetting index\n",
    "\n",
    "elastic_fs_names = feature_importance['feature'].tolist() # saving the names of the selected features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "outputs": [
    {
     "data": {
      "text/plain": "  feature  coefficient  negative  positive  coefficient (absolute)\n0  modSO2    -2.284999 -2.284999       NaN                2.284999\n1    soim     1.752239       NaN  1.752239                1.752239\n2  modNO2    -1.353569 -1.353569       NaN                1.353569\n3   modCO     1.237850       NaN  1.237850                1.237850\n4    HASL    -0.789783 -0.789783       NaN                0.789783\n5    hour    -0.687519 -0.687519       NaN                0.687519\n6    HAGL    -0.644111 -0.644111       NaN                0.644111\n7    kzzz     0.616078       NaN  0.616078                0.616078\n8    wsta     0.506825       NaN  0.506825                0.506825\n9    airm    -0.399635 -0.399635       NaN                0.399635",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>coefficient</th>\n      <th>negative</th>\n      <th>positive</th>\n      <th>coefficient (absolute)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>modSO2</td>\n      <td>-2.284999</td>\n      <td>-2.284999</td>\n      <td>NaN</td>\n      <td>2.284999</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>soim</td>\n      <td>1.752239</td>\n      <td>NaN</td>\n      <td>1.752239</td>\n      <td>1.752239</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>modNO2</td>\n      <td>-1.353569</td>\n      <td>-1.353569</td>\n      <td>NaN</td>\n      <td>1.353569</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>modCO</td>\n      <td>1.237850</td>\n      <td>NaN</td>\n      <td>1.237850</td>\n      <td>1.237850</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HASL</td>\n      <td>-0.789783</td>\n      <td>-0.789783</td>\n      <td>NaN</td>\n      <td>0.789783</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>hour</td>\n      <td>-0.687519</td>\n      <td>-0.687519</td>\n      <td>NaN</td>\n      <td>0.687519</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>HAGL</td>\n      <td>-0.644111</td>\n      <td>-0.644111</td>\n      <td>NaN</td>\n      <td>0.644111</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>kzzz</td>\n      <td>0.616078</td>\n      <td>NaN</td>\n      <td>0.616078</td>\n      <td>0.616078</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>wsta</td>\n      <td>0.506825</td>\n      <td>NaN</td>\n      <td>0.506825</td>\n      <td>0.506825</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>airm</td>\n      <td>-0.399635</td>\n      <td>-0.399635</td>\n      <td>NaN</td>\n      <td>0.399635</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (29361, 40)\n",
      "X_test shape:  (9787, 40)\n",
      "y_train shape:  (29361,)\n",
      "y_test shape:  (9787,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10) # split the data into train and test sets\n",
    "\n",
    "if pollutant == 'NO2':\n",
    "    test_pollutant = X_test[\"NO2\"] # for later prediction use\n",
    "    X_train = X_train.drop(['NO2'], axis=1) # dropping measurements because it can't be part of the prediction task\n",
    "    X_test = X_test.drop(['NO2'], axis=1)\n",
    "else:\n",
    "    test_pollutant = X_test[\"PM25\"]\n",
    "    X_train = X_train.drop(['PM25'], axis=1)\n",
    "    X_test = X_test.drop(['PM25'], axis=1)\n",
    "\n",
    "X_names = X_train.columns # saving the names of the features\n",
    "\n",
    "X_test_unscaled = X_test.copy()\n",
    "X_train = scaler.fit_transform(X_train) # standardizing the train set, per feature\n",
    "X_test = scaler.fit_transform(X_test) # standardizing the test set, per feature\n",
    "X_train_copy = pd.DataFrame(X_train) # scaled\n",
    "X_test_copy = pd.DataFrame(X_test) # scaled\n",
    "\n",
    "# assign X_train_copy columns the names of X_names\n",
    "X_train_copy.columns = X_names\n",
    "X_test_copy.columns = X_names\n",
    "\n",
    "# converting y_train and y_test to numpy arrays\n",
    "y_test_index = y_test.index # saving the indices of y_test\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# printing the shapes of the train and test sets\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# # Implementing grid search on random forest, to find the best hyperparameters\n",
    "#\n",
    "# import datetime\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "#\n",
    "# # defining the parameter values that should be searched\n",
    "# param_grid = {'max_depth': ['None', 50, 100],\n",
    "#               'max_features': ['auto', 'sqrt', 0.2, 0.4, 0.5, 0.6, 0.8],\n",
    "#               'n_estimators': [150, 200, 250],\n",
    "#               'min_samples_split': [2, 5, 10],\n",
    "#               'min_samples_leaf': [1, 2, 4],\n",
    "#               'criterion': ['squared_error'],} # grid search will try all possible combinations of these values\n",
    "#\n",
    "# # instantiating the grid\n",
    "# grid = GridSearchCV(RandomForestRegressor(), param_grid, refit=True, cv=4) # 4-fold cross validation\n",
    "#\n",
    "# print(Fore.CYAN + \"Started at: \", datetime.datetime.now()) # printing the time the grid search started\n",
    "#\n",
    "# grid.fit(X_train, y_train) # fitting the grid\n",
    "#\n",
    "# pd.DataFrame(grid.cv_results_).sort_values(\"rank_test_score\").to_csv(\"NO2_RF_grid_search_results.csv\") # saving the results of the grid search\n",
    "# Although the first result is allegedly the best, it is not necessarily the case, because other metrics are not taken into account.\n",
    "#\n",
    "# print(Fore.CYAN + \"Ended at: \", datetime.datetime.now()) # printing the time the grid search ended"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestRegressor(max_depth=50, min_samples_leaf=4, n_estimators=150,\n                      random_state=0)",
      "text/html": "<style>#sk-container-id-22 {color: black;background-color: white;}#sk-container-id-22 pre{padding: 0;}#sk-container-id-22 div.sk-toggleable {background-color: white;}#sk-container-id-22 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-22 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-22 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-22 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-22 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-22 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-22 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-22 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-22 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-22 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-22 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-22 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-22 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-22 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-22 div.sk-item {position: relative;z-index: 1;}#sk-container-id-22 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-22 div.sk-item::before, #sk-container-id-22 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-22 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-22 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-22 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-22 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-22 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-22 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-22 div.sk-label-container {text-align: center;}#sk-container-id-22 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-22 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-22\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=50, min_samples_leaf=4, n_estimators=150,\n                      random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=50, min_samples_leaf=4, n_estimators=150,\n                      random_state=0)</pre></div></div></div></div></div>"
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "if pollutant == 'NO2':\n",
    "    rf = RandomForestRegressor(max_depth = 50, max_features = 1.0, min_samples_leaf = 4, min_samples_split = 2, n_estimators = 150, random_state = 0)\n",
    "else: # pollutant == 'PM2.5'\n",
    "    rf = RandomForestRegressor(max_depth = 50, max_features = 'sqrt', min_samples_split = 5, n_estimators = 150, random_state = 0)\n",
    "\n",
    "rf.fit(X_train, y_train) # fitting the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Implementing grid search on xgboost, to find the best hyperparameters\n",
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import datetime\n",
    "#\n",
    "# # define the parameter values that should be searched\n",
    "# param_grid = {'max_depth': [6, 8, 10],\n",
    "#               'eta': [0.0, 0.1, 0.2, 0.3],\n",
    "#               'subsample': [0.5, 0.7],\n",
    "#               'colsample_bytree': [1, 0.75],\n",
    "#               'gamma': [0, 5, 10],\n",
    "#               'reg_alpha': [0, 0.5],\n",
    "#               'reg_lambda': [1, 2, 3],\n",
    "#               'n_estimators': [100, 150, 200]},\n",
    "#               'num_parallel_tree': [1, 2, 3],\n",
    "#               'booster': ['gbtree', 'dart']}\n",
    "#\n",
    "# # instantiate the grid\n",
    "# grid = GridSearchCV(XGBRegressor(), param_grid, refit=True, cv=3)\n",
    "#\n",
    "# print(Fore.CYAN + \"Started at: \", datetime.datetime.now()) # printing the time the grid search started\n",
    "#\n",
    "# # fit the grid with data\n",
    "# grid.fit(X_train, y_train)\n",
    "#\n",
    "# # rank them by score and then save a csv file with the results of the grid search\n",
    "# pd.DataFrame(grid.cv_results_).sort_values(\"rank_test_score\").to_csv(\"NO2_XGB_grid_search_results.csv\")\n",
    "# # Although the first result is allegedly the best, it is not necessarily the case, because other metrics are not taken into account.\n",
    "#\n",
    "# print(Fore.CYAN + \"Ended at: \", datetime.datetime.now()) # printing the time the grid search ended"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "outputs": [
    {
     "data": {
      "text/plain": "XGBRegressor(booster='dart', eta=0.0, gamma=5, max_depth=8, min_child_weight=2,\n             num_parallel_tree=3, subsample=0.7)",
      "text/html": "<style>#sk-container-id-23 {color: black;background-color: white;}#sk-container-id-23 pre{padding: 0;}#sk-container-id-23 div.sk-toggleable {background-color: white;}#sk-container-id-23 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-23 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-23 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-23 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-23 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-23 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-23 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-23 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-23 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-23 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-23 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-23 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-23 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-23 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-23 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-23 div.sk-item {position: relative;z-index: 1;}#sk-container-id-23 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-23 div.sk-item::before, #sk-container-id-23 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-23 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-23 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-23 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-23 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-23 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-23 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-23 div.sk-label-container {text-align: center;}#sk-container-id-23 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-23 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-23\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(booster=&#x27;dart&#x27;, eta=0.0, gamma=5, max_depth=8, min_child_weight=2,\n             num_parallel_tree=3, subsample=0.7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" checked><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(booster=&#x27;dart&#x27;, eta=0.0, gamma=5, max_depth=8, min_child_weight=2,\n             num_parallel_tree=3, subsample=0.7)</pre></div></div></div></div></div>"
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "if pollutant == 'NO2':\n",
    "    xgb = XGBRegressor(booster = 'dart', eta = 0.0, gamma = 5, max_depth = 8, min_child_weight = 2, num_parallel_tree = 3, reg_lambda = 1, reg_alpha = 0, subsample = 0.7, colsample_bytree = 1, n_estimators = 100, random_state = 0)\n",
    "else: # pollutant == 'PM2.5'\n",
    "    xgb = XGBRegressor(booster = 'dart', gamma = 0, max_depth = 8, min_child_weight = 1, num_parallel_tree = 1, reg_lambda = 1, reg_alpha = 0.5, subsample = 1.0, colsample_bytree = 0.75, n_estimators = 200, random_state = 0)\n",
    "\n",
    "\n",
    "xgb.fit(X_train, y_train) # fitting the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 5, 'importance_type': 'gain', 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 8, 'min_child_weight': 2, 'missing': None, 'n_estimators': 100, 'n_jobs': 1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': None, 'silent': None, 'subsample': 0.7, 'verbosity': 1, 'eta': 0.0, 'num_parallel_tree': 3}\n"
     ]
    }
   ],
   "source": [
    "print(xgb.get_params()) # printing the parameters used for the xgb model\n",
    "# n_estimators is 100 by default"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "outputs": [
    {
     "data": {
      "text/plain": "  Feature Name  Portion of MSE Reduction  Feature\n0         HASL                  0.177508        5\n1         soim                  0.162491       17\n2         hour                  0.076172        0\n3       modSO2                  0.074377       39\n4       modNO2                  0.066994       35\n5            Y                  0.038099        2\n6        modNO                  0.036498       34\n7        modCO                  0.030346       33\n8         winz                  0.025121       29\n9            X                  0.024318        1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature Name</th>\n      <th>Portion of MSE Reduction</th>\n      <th>Feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HASL</td>\n      <td>0.177508</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>soim</td>\n      <td>0.162491</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hour</td>\n      <td>0.076172</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>modSO2</td>\n      <td>0.074377</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>modNO2</td>\n      <td>0.066994</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Y</td>\n      <td>0.038099</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>modNO</td>\n      <td>0.036498</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>modCO</td>\n      <td>0.030346</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>winz</td>\n      <td>0.025121</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>X</td>\n      <td>0.024318</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gain_score = xgb.get_booster().get_score(importance_type='total_gain') # extracting the feature importance scores\n",
    "xgb_gain_sum = sum(xgb_gain_score.values()) # summing the total_gain values of all features\n",
    "gain_normalized = {key: value / xgb_gain_sum for key, value in xgb_gain_score.items()} # calculating the relative gain of each feature\n",
    "sorted_gain = dict(sorted(gain_normalized.items(), key=lambda x: x[1], reverse=True)) # sorting the relative gain scores\n",
    "\n",
    "k = 10\n",
    "selected_indices = [int(key[1:]) for key in list(sorted_gain.keys())[:k]] # extracting the indices of the k top features\n",
    "selected_features_names = X_test_copy.columns[selected_indices] # extracting the names of the k top features\n",
    "\n",
    "# extract the values from sorted_importance\n",
    "sorted_gain_numpy = np.array(list(sorted_gain.values())) # transforming the values of the dictionary into a numpy array\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature Name': X_test_unscaled.columns[selected_indices], 'Portion of MSE Reduction': sorted_gain_numpy[:10], 'Feature': selected_indices})\n",
    "xgb_fs_names = feature_importance['Feature Name'].tolist() # extracting the names of the k top features\n",
    "feature_importance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PERMUTATION IMPORTANCE:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(xgb, X_test, y_test, n_repeats=10, random_state=0) # calculating the permutation importance scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "outputs": [
    {
     "data": {
      "text/plain": "   Feature Name  Permutation Importance\n39         HASL                0.259950\n38         soim                0.239658\n37       modNO2                0.169318\n36         hour                0.069247\n35        modNO                0.066592\n34       modSO2                0.044400\n33            Y                0.039737\n32         HAGL                0.034625\n31         winz                0.026904\n30        modCO                0.026219",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature Name</th>\n      <th>Permutation Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>39</th>\n      <td>HASL</td>\n      <td>0.259950</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>soim</td>\n      <td>0.239658</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>modNO2</td>\n      <td>0.169318</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>hour</td>\n      <td>0.069247</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>modNO</td>\n      <td>0.066592</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>modSO2</td>\n      <td>0.044400</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Y</td>\n      <td>0.039737</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>HAGL</td>\n      <td>0.034625</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>winz</td>\n      <td>0.026904</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>modCO</td>\n      <td>0.026219</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "xgb_permutation_importance = pd.DataFrame({'Feature Name': X_names[sorted_idx], 'Permutation Importance': perm_importance.importances_mean[sorted_idx]}).sort_values(by = 'Permutation Importance', ascending = False).head(10)\n",
    "# save the top 10 feature names to a list\n",
    "xgb_permutation_names = xgb_permutation_importance['Feature Name'].tolist()\n",
    "xgb_permutation_importance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NEURAL NETWORK:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter # Used to log the training and test loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 40])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# converting the train and test sets to tensors if they are numpy arrays\n",
    "def numpy_to_tensor(x):\n",
    "    if type(x) == np.ndarray:\n",
    "        return torch.from_numpy(x).float()\n",
    "    return x\n",
    "\n",
    "X_train, X_test, y_train, y_test = numpy_to_tensor(X_train), numpy_to_tensor(X_test), \\\n",
    "                                   numpy_to_tensor(y_train), numpy_to_tensor(y_test)\n",
    "\n",
    "# creating a TensorDataset with train and test sets\n",
    "train = TensorDataset(X_train, y_train)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "\n",
    "# creating a DataLoader with train and test sets\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# printing a batch sample from train_loader\n",
    "dataiter = iter(train_loader)\n",
    "samples, targets = dataiter.next()\n",
    "targets = targets.view(targets.shape[0], 1)\n",
    "print(samples.shape)\n",
    "print(targets.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# connect to Jupyter GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "outputs": [],
   "source": [
    "def train_regular_NN(net, train_loader, test_loader, criterion, optimizer, num_epochs, writer):\n",
    "    test_loss_min = np.Inf # to later on track change in test loss\n",
    "    count_no_improvement = 0 # to later on track the number of epochs with no improvement in test loss\n",
    "    min_loss_epoch = 0 # to later on track the epoch with the minimum test loss\n",
    "\n",
    "    start_time = time.time() # used to track the time it takes to train the model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0 # initializing the running loss\n",
    "        for samples, targets in train_loader:\n",
    "            samples, targets = samples.to(device), targets.to(device) # sending the data to the device (GPU if available)\n",
    "            targets = targets.view(targets.shape[0], 1) # reshaping targets to (batch_size, 1)\n",
    "            optimizer.zero_grad() # clear the gradients, because gradients are accumulated\n",
    "            output = net(samples) # forward pass\n",
    "            loss = criterion(output, targets) # calculating the loss\n",
    "            loss.backward() # backward pass, calculating the gradients\n",
    "            optimizer.step() # updating the weights\n",
    "            running_loss += loss.item() # adding the loss of the current batch to the running loss\n",
    "\n",
    "        should_stop, test_loss_min, min_loss_epoch, count_no_improvement = evaluation(\n",
    "            net, criterion, train_loader, test_loader, writer, epoch, num_epochs,\n",
    "            running_loss, test_loss_min, min_loss_epoch, count_no_improvement, 'regular_net.pt') # evaluating the model on train and test sets\n",
    "        if should_stop: # if the test loss did not improve for 40 epochs, stop training\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(Fore.MAGENTA + \"Time to train the model: \", end_time - start_time, \"seconds\")\n",
    "\n",
    "    return net, test_loss_min, min_loss_epoch\n",
    "\n",
    "# _______________________________________________________________________________________________________________\n",
    "\n",
    "def evaluation(net, criterion, train_loader, test_loader, writer, epoch, num_epochs, running_loss, test_loss_min, min_loss_epoch, count_no_improvement, model_name):\n",
    "    print(Fore.CYAN + \"Epoch: {}/{}.. \".format(epoch+1, num_epochs),\n",
    "          \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader))) # printing the training loss for the current epoch\n",
    "    writer.add_scalar('Training loss', running_loss/len(train_loader), epoch) # writing the training loss to tensorboard\n",
    "\n",
    "    test_loss = 0 # initializing the test loss\n",
    "    with torch.no_grad(): # we do not need to calculate the gradients, because we are not training\n",
    "        net.eval() # putting the model in evaluation mode\n",
    "        for samples, targets in test_loader:\n",
    "            samples, targets = samples.to(device), targets.to(device)\n",
    "            targets = targets.view(targets.shape[0], 1)\n",
    "            output = net(samples)\n",
    "            test_loss += criterion(output, targets) # calculating the test loss\n",
    "\n",
    "    print(Fore.LIGHTYELLOW_EX + \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader))) # printing the test loss for the current epoch\n",
    "    writer.add_scalar('Test loss', test_loss/len(test_loader), epoch) # writing the test loss to tensorboard\n",
    "\n",
    "    if test_loss/len(test_loader) < test_loss_min: # if the test loss improved, save the model\n",
    "        print(Fore.GREEN + \"Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(test_loss_min, test_loss/len(test_loader)))\n",
    "        test_loss_min = test_loss/len(test_loader) # updating the minimum test loss\n",
    "        min_loss_epoch = epoch # updating the epoch with the minimum test loss\n",
    "        torch.save(net.state_dict(), model_name) # saving the model\n",
    "        count_no_improvement = 0 # resetting the number of epochs with no improvement in test loss\n",
    "    else:\n",
    "        count_no_improvement += 1 # if the test loss did not improve, increment the counter\n",
    "\n",
    "        if count_no_improvement == 40: # if the test loss did not improve for 40 epochs, stop training\n",
    "            print(Fore.RED + \"Test loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\")\n",
    "            return True, test_loss_min, min_loss_epoch, count_no_improvement\n",
    "\n",
    "    net.train() # putting the model back in training mode\n",
    "    return False, test_loss_min, min_loss_epoch, count_no_improvement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "outputs": [],
   "source": [
    "def train_elastic_net_NN(net, reg_parameter, alpha, train_loader, test_loader, criterion, optimizer, num_epochs, writer):\n",
    "    test_loss_min = np.Inf\n",
    "    count_no_improvement = 0\n",
    "    min_loss_epoch = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for samples, targets in train_loader:\n",
    "            samples, targets = samples.to(device), targets.to(device)\n",
    "            targets = targets.view(targets.shape[0], 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = reg_net(samples)\n",
    "            main_loss = criterion(output, targets) # calculating the main loss\n",
    "            l1_loss = alpha * torch.abs(reg_net.fc1.weight) # calculating the L1 loss for the weights\n",
    "            l2_loss = (1 - alpha) * reg_net.fc1.weight ** 2 # calculating the L2 loss for the weights\n",
    "            elastic_net_loss = l1_loss + l2_loss # the elastic net loss\n",
    "            loss = (1 - reg_parameter) * main_loss + reg_parameter * elastic_net_loss.sum() # the total loss, combining the main loss and the elastic net loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        should_stop, test_loss_min, min_loss_epoch, count_no_improvement = evaluation(\n",
    "            net, criterion, train_loader, test_loader, writer, epoch, num_epochs,\n",
    "            running_loss, test_loss_min, min_loss_epoch, count_no_improvement, 'elastic_net.pt')\n",
    "        if should_stop:\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(Fore.MAGENTA + \"Time to train the model: \", end_time - start_time, \"seconds\")\n",
    "\n",
    "    return net, test_loss_min, min_loss_epoch\n",
    "\n",
    "# _______________________________________________________________________________________________________________\n",
    "\n",
    "def train_gradient_feature_selection_NN(net, n_top_features, train_loader, test_loader, criterion, optimizer, num_epochs, writer):\n",
    "    test_loss_min = np.Inf\n",
    "    count_no_improvement = 0\n",
    "    min_loss_epoch = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for samples, targets in train_loader:\n",
    "            samples, targets = samples.to(device), targets.to(device)\n",
    "            targets = targets.view(targets.shape[0], 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(samples)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            gradients = torch.sum(net.fc1.weight.grad.abs(), dim=0) # summing the absolute value of the gradients for each feature across all neurons in the first layer\n",
    "            _, top_n_features = torch.topk(gradients, k = n_top_features) # getting the indices of the top k features (the ones with the highest sum of the absolute value of the gradients)\n",
    "            non_top_features = np.delete(np.arange(0, input_dim), top_n_features) # indices of the features that are outside the top k features\n",
    "            net.fc1.weight.grad[:, non_top_features] = 0 # zeroing out the gradients for the non-top features\n",
    "\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        should_stop, test_loss_min, min_loss_epoch, count_no_improvement = evaluation(\n",
    "            net, criterion, train_loader, test_loader, writer, epoch, num_epochs,\n",
    "            running_loss, test_loss_min, min_loss_epoch, count_no_improvement, 'gradient_net.pt')\n",
    "        if should_stop:\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(Fore.MAGENTA + \"Time to train the model: \", end_time - start_time, \"seconds\")\n",
    "\n",
    "    return net, test_loss_min, min_loss_epoch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1] # number of features\n",
    "hidden_dim = 200\n",
    "num_epochs = 200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m Net(\n",
      "  (fc1): Linear(in_features=40, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc4): Linear(in_features=200, out_features=1, bias=True)\n",
      ")\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 98.288.. \n",
      "\u001B[93mTest Loss: 103.356.. \n",
      "\u001B[32mTest loss decreased (inf --> 103.355659).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 85.749.. \n",
      "\u001B[93mTest Loss: 99.914.. \n",
      "\u001B[32mTest loss decreased (103.355659 --> 99.913559).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 82.769.. \n",
      "\u001B[93mTest Loss: 102.054.. \n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 80.257.. \n",
      "\u001B[93mTest Loss: 98.215.. \n",
      "\u001B[32mTest loss decreased (99.913559 --> 98.214912).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 78.123.. \n",
      "\u001B[93mTest Loss: 94.402.. \n",
      "\u001B[32mTest loss decreased (98.214912 --> 94.401917).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 77.309.. \n",
      "\u001B[93mTest Loss: 96.329.. \n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 75.584.. \n",
      "\u001B[93mTest Loss: 94.682.. \n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 74.473.. \n",
      "\u001B[93mTest Loss: 95.466.. \n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 74.181.. \n",
      "\u001B[93mTest Loss: 94.726.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 71.396.. \n",
      "\u001B[93mTest Loss: 91.557.. \n",
      "\u001B[32mTest loss decreased (94.401917 --> 91.556946).  Saving model ...\n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 70.461.. \n",
      "\u001B[93mTest Loss: 97.233.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 70.000.. \n",
      "\u001B[93mTest Loss: 92.644.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 68.132.. \n",
      "\u001B[93mTest Loss: 93.599.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 67.037.. \n",
      "\u001B[93mTest Loss: 90.926.. \n",
      "\u001B[32mTest loss decreased (91.556946 --> 90.925888).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 66.704.. \n",
      "\u001B[93mTest Loss: 95.894.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 66.062.. \n",
      "\u001B[93mTest Loss: 91.898.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 63.075.. \n",
      "\u001B[93mTest Loss: 102.207.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 61.276.. \n",
      "\u001B[93mTest Loss: 92.973.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 61.059.. \n",
      "\u001B[93mTest Loss: 90.766.. \n",
      "\u001B[32mTest loss decreased (90.925888 --> 90.766136).  Saving model ...\n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 59.891.. \n",
      "\u001B[93mTest Loss: 94.201.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 58.625.. \n",
      "\u001B[93mTest Loss: 90.676.. \n",
      "\u001B[32mTest loss decreased (90.766136 --> 90.676292).  Saving model ...\n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 57.148.. \n",
      "\u001B[93mTest Loss: 91.119.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 54.828.. \n",
      "\u001B[93mTest Loss: 91.227.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 54.583.. \n",
      "\u001B[93mTest Loss: 88.367.. \n",
      "\u001B[32mTest loss decreased (90.676292 --> 88.366623).  Saving model ...\n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 52.002.. \n",
      "\u001B[93mTest Loss: 89.308.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 51.101.. \n",
      "\u001B[93mTest Loss: 90.560.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 50.468.. \n",
      "\u001B[93mTest Loss: 88.891.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 48.056.. \n",
      "\u001B[93mTest Loss: 85.507.. \n",
      "\u001B[32mTest loss decreased (88.366623 --> 85.507011).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 46.566.. \n",
      "\u001B[93mTest Loss: 90.874.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 46.201.. \n",
      "\u001B[93mTest Loss: 87.029.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 42.055.. \n",
      "\u001B[93mTest Loss: 86.133.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 41.006.. \n",
      "\u001B[93mTest Loss: 83.251.. \n",
      "\u001B[32mTest loss decreased (85.507011 --> 83.250603).  Saving model ...\n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 40.287.. \n",
      "\u001B[93mTest Loss: 78.972.. \n",
      "\u001B[32mTest loss decreased (83.250603 --> 78.972244).  Saving model ...\n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 38.659.. \n",
      "\u001B[93mTest Loss: 78.757.. \n",
      "\u001B[32mTest loss decreased (78.972244 --> 78.757133).  Saving model ...\n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 37.513.. \n",
      "\u001B[93mTest Loss: 82.264.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 35.790.. \n",
      "\u001B[93mTest Loss: 76.454.. \n",
      "\u001B[32mTest loss decreased (78.757133 --> 76.453903).  Saving model ...\n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 34.195.. \n",
      "\u001B[93mTest Loss: 75.461.. \n",
      "\u001B[32mTest loss decreased (76.453903 --> 75.460548).  Saving model ...\n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 34.125.. \n",
      "\u001B[93mTest Loss: 75.537.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 33.092.. \n",
      "\u001B[93mTest Loss: 76.721.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 33.555.. \n",
      "\u001B[93mTest Loss: 85.443.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 31.076.. \n",
      "\u001B[93mTest Loss: 76.062.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 31.598.. \n",
      "\u001B[93mTest Loss: 77.999.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 28.523.. \n",
      "\u001B[93mTest Loss: 76.446.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 29.595.. \n",
      "\u001B[93mTest Loss: 75.583.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 27.730.. \n",
      "\u001B[93mTest Loss: 74.659.. \n",
      "\u001B[32mTest loss decreased (75.460548 --> 74.659416).  Saving model ...\n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 29.816.. \n",
      "\u001B[93mTest Loss: 75.809.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 26.542.. \n",
      "\u001B[93mTest Loss: 75.928.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 25.600.. \n",
      "\u001B[93mTest Loss: 77.384.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 24.301.. \n",
      "\u001B[93mTest Loss: 71.995.. \n",
      "\u001B[32mTest loss decreased (74.659416 --> 71.995148).  Saving model ...\n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 29.266.. \n",
      "\u001B[93mTest Loss: 72.755.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 24.033.. \n",
      "\u001B[93mTest Loss: 76.949.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 24.760.. \n",
      "\u001B[93mTest Loss: 71.748.. \n",
      "\u001B[32mTest loss decreased (71.995148 --> 71.748421).  Saving model ...\n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 24.225.. \n",
      "\u001B[93mTest Loss: 74.536.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 22.938.. \n",
      "\u001B[93mTest Loss: 73.987.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 22.895.. \n",
      "\u001B[93mTest Loss: 76.505.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 23.931.. \n",
      "\u001B[93mTest Loss: 83.697.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 23.648.. \n",
      "\u001B[93mTest Loss: 76.323.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 21.643.. \n",
      "\u001B[93mTest Loss: 75.771.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 21.893.. \n",
      "\u001B[93mTest Loss: 72.007.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 20.108.. \n",
      "\u001B[93mTest Loss: 71.024.. \n",
      "\u001B[32mTest loss decreased (71.748421 --> 71.023613).  Saving model ...\n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 20.048.. \n",
      "\u001B[93mTest Loss: 72.858.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 19.890.. \n",
      "\u001B[93mTest Loss: 75.931.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 23.481.. \n",
      "\u001B[93mTest Loss: 75.536.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 19.251.. \n",
      "\u001B[93mTest Loss: 73.890.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 18.082.. \n",
      "\u001B[93mTest Loss: 74.330.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 18.554.. \n",
      "\u001B[93mTest Loss: 73.174.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 18.132.. \n",
      "\u001B[93mTest Loss: 72.138.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 17.552.. \n",
      "\u001B[93mTest Loss: 71.859.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 17.200.. \n",
      "\u001B[93mTest Loss: 75.258.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 18.241.. \n",
      "\u001B[93mTest Loss: 72.003.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 17.662.. \n",
      "\u001B[93mTest Loss: 74.311.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 16.935.. \n",
      "\u001B[93mTest Loss: 72.846.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 18.390.. \n",
      "\u001B[93mTest Loss: 70.107.. \n",
      "\u001B[32mTest loss decreased (71.023613 --> 70.107010).  Saving model ...\n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 23.574.. \n",
      "\u001B[93mTest Loss: 75.921.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 18.995.. \n",
      "\u001B[93mTest Loss: 71.422.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 16.258.. \n",
      "\u001B[93mTest Loss: 75.318.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 16.085.. \n",
      "\u001B[93mTest Loss: 72.134.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 14.571.. \n",
      "\u001B[93mTest Loss: 73.227.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 13.318.. \n",
      "\u001B[93mTest Loss: 70.922.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 15.837.. \n",
      "\u001B[93mTest Loss: 72.261.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 15.950.. \n",
      "\u001B[93mTest Loss: 71.410.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 17.729.. \n",
      "\u001B[93mTest Loss: 72.468.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 15.569.. \n",
      "\u001B[93mTest Loss: 73.134.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 16.576.. \n",
      "\u001B[93mTest Loss: 72.423.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 14.609.. \n",
      "\u001B[93mTest Loss: 71.411.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 13.352.. \n",
      "\u001B[93mTest Loss: 72.268.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 12.935.. \n",
      "\u001B[93mTest Loss: 71.591.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 13.479.. \n",
      "\u001B[93mTest Loss: 70.983.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 14.165.. \n",
      "\u001B[93mTest Loss: 72.431.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 13.160.. \n",
      "\u001B[93mTest Loss: 71.757.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 13.163.. \n",
      "\u001B[93mTest Loss: 74.535.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 14.678.. \n",
      "\u001B[93mTest Loss: 71.775.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 11.682.. \n",
      "\u001B[93mTest Loss: 74.137.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 11.445.. \n",
      "\u001B[93mTest Loss: 72.848.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 11.968.. \n",
      "\u001B[93mTest Loss: 70.658.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 13.830.. \n",
      "\u001B[93mTest Loss: 76.361.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 13.018.. \n",
      "\u001B[93mTest Loss: 73.496.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 14.142.. \n",
      "\u001B[93mTest Loss: 72.843.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 11.688.. \n",
      "\u001B[93mTest Loss: 70.107.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 10.960.. \n",
      "\u001B[93mTest Loss: 70.691.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 10.804.. \n",
      "\u001B[93mTest Loss: 70.173.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 10.844.. \n",
      "\u001B[93mTest Loss: 74.196.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 13.630.. \n",
      "\u001B[93mTest Loss: 71.237.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 13.119.. \n",
      "\u001B[93mTest Loss: 75.425.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 11.043.. \n",
      "\u001B[93mTest Loss: 69.758.. \n",
      "\u001B[32mTest loss decreased (70.107010 --> 69.758072).  Saving model ...\n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 10.049.. \n",
      "\u001B[93mTest Loss: 72.456.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 9.961.. \n",
      "\u001B[93mTest Loss: 69.384.. \n",
      "\u001B[32mTest loss decreased (69.758072 --> 69.383759).  Saving model ...\n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 9.754.. \n",
      "\u001B[93mTest Loss: 71.156.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 9.664.. \n",
      "\u001B[93mTest Loss: 72.825.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 17.392.. \n",
      "\u001B[93mTest Loss: 75.304.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 12.229.. \n",
      "\u001B[93mTest Loss: 71.705.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 9.074.. \n",
      "\u001B[93mTest Loss: 70.011.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 11.419.. \n",
      "\u001B[93mTest Loss: 70.964.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 9.977.. \n",
      "\u001B[93mTest Loss: 70.361.. \n",
      "\u001B[36mEpoch: 115/200..  Training Loss: 11.169.. \n",
      "\u001B[93mTest Loss: 71.553.. \n",
      "\u001B[36mEpoch: 116/200..  Training Loss: 9.331.. \n",
      "\u001B[93mTest Loss: 70.276.. \n",
      "\u001B[36mEpoch: 117/200..  Training Loss: 8.544.. \n",
      "\u001B[93mTest Loss: 70.939.. \n",
      "\u001B[36mEpoch: 118/200..  Training Loss: 9.123.. \n",
      "\u001B[93mTest Loss: 74.182.. \n",
      "\u001B[36mEpoch: 119/200..  Training Loss: 9.809.. \n",
      "\u001B[93mTest Loss: 69.991.. \n",
      "\u001B[36mEpoch: 120/200..  Training Loss: 9.153.. \n",
      "\u001B[93mTest Loss: 75.456.. \n",
      "\u001B[36mEpoch: 121/200..  Training Loss: 12.984.. \n",
      "\u001B[93mTest Loss: 73.437.. \n",
      "\u001B[36mEpoch: 122/200..  Training Loss: 10.569.. \n",
      "\u001B[93mTest Loss: 71.001.. \n",
      "\u001B[36mEpoch: 123/200..  Training Loss: 8.449.. \n",
      "\u001B[93mTest Loss: 71.249.. \n",
      "\u001B[36mEpoch: 124/200..  Training Loss: 7.970.. \n",
      "\u001B[93mTest Loss: 69.476.. \n",
      "\u001B[36mEpoch: 125/200..  Training Loss: 8.083.. \n",
      "\u001B[93mTest Loss: 69.621.. \n",
      "\u001B[36mEpoch: 126/200..  Training Loss: 11.801.. \n",
      "\u001B[93mTest Loss: 73.453.. \n",
      "\u001B[36mEpoch: 127/200..  Training Loss: 10.813.. \n",
      "\u001B[93mTest Loss: 72.576.. \n",
      "\u001B[36mEpoch: 128/200..  Training Loss: 8.218.. \n",
      "\u001B[93mTest Loss: 69.654.. \n",
      "\u001B[36mEpoch: 129/200..  Training Loss: 11.088.. \n",
      "\u001B[93mTest Loss: 70.342.. \n",
      "\u001B[36mEpoch: 130/200..  Training Loss: 11.351.. \n",
      "\u001B[93mTest Loss: 71.687.. \n",
      "\u001B[36mEpoch: 131/200..  Training Loss: 7.579.. \n",
      "\u001B[93mTest Loss: 71.512.. \n",
      "\u001B[36mEpoch: 132/200..  Training Loss: 7.268.. \n",
      "\u001B[93mTest Loss: 70.776.. \n",
      "\u001B[36mEpoch: 133/200..  Training Loss: 7.423.. \n",
      "\u001B[93mTest Loss: 73.337.. \n",
      "\u001B[36mEpoch: 134/200..  Training Loss: 7.935.. \n",
      "\u001B[93mTest Loss: 70.399.. \n",
      "\u001B[36mEpoch: 135/200..  Training Loss: 7.629.. \n",
      "\u001B[93mTest Loss: 71.980.. \n",
      "\u001B[36mEpoch: 136/200..  Training Loss: 7.762.. \n",
      "\u001B[93mTest Loss: 71.732.. \n",
      "\u001B[36mEpoch: 137/200..  Training Loss: 8.531.. \n",
      "\u001B[93mTest Loss: 75.141.. \n",
      "\u001B[36mEpoch: 138/200..  Training Loss: 10.572.. \n",
      "\u001B[93mTest Loss: 69.916.. \n",
      "\u001B[36mEpoch: 139/200..  Training Loss: 12.552.. \n",
      "\u001B[93mTest Loss: 73.472.. \n",
      "\u001B[36mEpoch: 140/200..  Training Loss: 8.444.. \n",
      "\u001B[93mTest Loss: 75.873.. \n",
      "\u001B[36mEpoch: 141/200..  Training Loss: 7.416.. \n",
      "\u001B[93mTest Loss: 71.091.. \n",
      "\u001B[36mEpoch: 142/200..  Training Loss: 6.321.. \n",
      "\u001B[93mTest Loss: 71.184.. \n",
      "\u001B[36mEpoch: 143/200..  Training Loss: 6.358.. \n",
      "\u001B[93mTest Loss: 71.284.. \n",
      "\u001B[36mEpoch: 144/200..  Training Loss: 7.085.. \n",
      "\u001B[93mTest Loss: 71.259.. \n",
      "\u001B[36mEpoch: 145/200..  Training Loss: 7.386.. \n",
      "\u001B[93mTest Loss: 70.368.. \n",
      "\u001B[36mEpoch: 146/200..  Training Loss: 14.987.. \n",
      "\u001B[93mTest Loss: 74.530.. \n",
      "\u001B[36mEpoch: 147/200..  Training Loss: 9.427.. \n",
      "\u001B[93mTest Loss: 70.702.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  341.1076419353485 seconds\n",
      "\u001B[32mLowest Test loss achieved: 69.383759\n",
      "\u001B[32mAt Epoch #106 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module): # defining the neural network architecture\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "net = Net().to(device) # creating an instance of neural network\n",
    "print(Fore.CYAN + \"\", net) # printing the neural network architecture\n",
    "\n",
    "criterion = nn.MSELoss() # mean squared error loss function\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) # Adam optimizer\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Train the regular neural network\n",
    "net, test_loss_min, min_loss_epoch = train_regular_NN(net, train_loader, test_loader, criterion, optimizer, num_epochs, writer)\n",
    "\n",
    "# load the model with the lowest validation loss\n",
    "print(Fore.GREEN + \"Lowest Test loss achieved: {:.6f}\".format(test_loss_min))\n",
    "print(Fore.GREEN + \"At Epoch #{}\".format(min_loss_epoch), \"\\n\")\n",
    "net.load_state_dict(torch.load('regular_net.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ELASTIC NET REGULARIZED NEURAL NETWORK FOR FEATURE SELECTION:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "outputs": [],
   "source": [
    "# definining the Elastic-Net Regularized Neural Network architecture\n",
    "class RegularizedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegularizedNet, self).__init__()\n",
    "        # input layer matching the number of features in X_train\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
    "        self.L1_loss = nn.L1Loss(size_average=False)\n",
    "        self.L2_loss = nn.MSELoss(size_average=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "WEIGHTS-BASED FEATURE SELECTION (Standard NN, Elastic-Net Regularized NN, Gradient Modified NN):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m Net(\n",
      "  (fc1): Linear(in_features=40, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc4): Linear(in_features=200, out_features=1, bias=True)\n",
      ")\n",
      "\u001B[36m RegularizedNet(\n",
      "  (fc1): Linear(in_features=40, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc4): Linear(in_features=200, out_features=1, bias=True)\n",
      "  (L1_loss): L1Loss()\n",
      "  (L2_loss): MSELoss()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avitay\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36mEpoch: 1/200..  Training Loss: 100.483.. \n",
      "\u001B[93mTest Loss: 103.960.. \n",
      "\u001B[32mTest loss decreased (inf --> 103.960175).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 86.176.. \n",
      "\u001B[93mTest Loss: 100.108.. \n",
      "\u001B[32mTest loss decreased (103.960175 --> 100.107681).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 82.223.. \n",
      "\u001B[93mTest Loss: 101.469.. \n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 80.503.. \n",
      "\u001B[93mTest Loss: 97.738.. \n",
      "\u001B[32mTest loss decreased (100.107681 --> 97.738220).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 78.749.. \n",
      "\u001B[93mTest Loss: 94.202.. \n",
      "\u001B[32mTest loss decreased (97.738220 --> 94.202499).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 77.825.. \n",
      "\u001B[93mTest Loss: 94.169.. \n",
      "\u001B[32mTest loss decreased (94.202499 --> 94.169357).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 76.318.. \n",
      "\u001B[93mTest Loss: 94.885.. \n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 75.166.. \n",
      "\u001B[93mTest Loss: 94.846.. \n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 73.709.. \n",
      "\u001B[93mTest Loss: 96.410.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 71.879.. \n",
      "\u001B[93mTest Loss: 97.962.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 70.296.. \n",
      "\u001B[93mTest Loss: 91.865.. \n",
      "\u001B[32mTest loss decreased (94.169357 --> 91.864929).  Saving model ...\n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 69.759.. \n",
      "\u001B[93mTest Loss: 92.727.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 68.530.. \n",
      "\u001B[93mTest Loss: 94.489.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 66.756.. \n",
      "\u001B[93mTest Loss: 95.021.. \n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 65.031.. \n",
      "\u001B[93mTest Loss: 93.524.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 64.451.. \n",
      "\u001B[93mTest Loss: 92.379.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 63.843.. \n",
      "\u001B[93mTest Loss: 95.736.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 62.464.. \n",
      "\u001B[93mTest Loss: 93.049.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 60.712.. \n",
      "\u001B[93mTest Loss: 91.662.. \n",
      "\u001B[32mTest loss decreased (91.864929 --> 91.661575).  Saving model ...\n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 59.518.. \n",
      "\u001B[93mTest Loss: 97.044.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 59.204.. \n",
      "\u001B[93mTest Loss: 91.296.. \n",
      "\u001B[32mTest loss decreased (91.661575 --> 91.296036).  Saving model ...\n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 56.107.. \n",
      "\u001B[93mTest Loss: 96.222.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 56.084.. \n",
      "\u001B[93mTest Loss: 92.019.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 55.442.. \n",
      "\u001B[93mTest Loss: 91.577.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 52.097.. \n",
      "\u001B[93mTest Loss: 108.283.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 52.644.. \n",
      "\u001B[93mTest Loss: 88.609.. \n",
      "\u001B[32mTest loss decreased (91.296036 --> 88.609322).  Saving model ...\n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 49.880.. \n",
      "\u001B[93mTest Loss: 88.406.. \n",
      "\u001B[32mTest loss decreased (88.609322 --> 88.405960).  Saving model ...\n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 48.066.. \n",
      "\u001B[93mTest Loss: 90.589.. \n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 46.737.. \n",
      "\u001B[93mTest Loss: 84.560.. \n",
      "\u001B[32mTest loss decreased (88.405960 --> 84.559853).  Saving model ...\n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 44.964.. \n",
      "\u001B[93mTest Loss: 86.144.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 43.883.. \n",
      "\u001B[93mTest Loss: 82.700.. \n",
      "\u001B[32mTest loss decreased (84.559853 --> 82.699745).  Saving model ...\n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 42.327.. \n",
      "\u001B[93mTest Loss: 81.408.. \n",
      "\u001B[32mTest loss decreased (82.699745 --> 81.407639).  Saving model ...\n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 40.806.. \n",
      "\u001B[93mTest Loss: 88.667.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 41.024.. \n",
      "\u001B[93mTest Loss: 83.949.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 40.105.. \n",
      "\u001B[93mTest Loss: 79.905.. \n",
      "\u001B[32mTest loss decreased (81.407639 --> 79.905281).  Saving model ...\n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 35.755.. \n",
      "\u001B[93mTest Loss: 80.951.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 35.053.. \n",
      "\u001B[93mTest Loss: 79.997.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 35.810.. \n",
      "\u001B[93mTest Loss: 79.559.. \n",
      "\u001B[32mTest loss decreased (79.905281 --> 79.558670).  Saving model ...\n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 33.286.. \n",
      "\u001B[93mTest Loss: 80.584.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 33.821.. \n",
      "\u001B[93mTest Loss: 78.514.. \n",
      "\u001B[32mTest loss decreased (79.558670 --> 78.513985).  Saving model ...\n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 31.442.. \n",
      "\u001B[93mTest Loss: 75.723.. \n",
      "\u001B[32mTest loss decreased (78.513985 --> 75.722595).  Saving model ...\n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 32.162.. \n",
      "\u001B[93mTest Loss: 76.681.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 30.173.. \n",
      "\u001B[93mTest Loss: 75.544.. \n",
      "\u001B[32mTest loss decreased (75.722595 --> 75.544212).  Saving model ...\n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 29.146.. \n",
      "\u001B[93mTest Loss: 74.674.. \n",
      "\u001B[32mTest loss decreased (75.544212 --> 74.673508).  Saving model ...\n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 29.054.. \n",
      "\u001B[93mTest Loss: 81.194.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 27.981.. \n",
      "\u001B[93mTest Loss: 76.360.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 28.036.. \n",
      "\u001B[93mTest Loss: 75.435.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 28.692.. \n",
      "\u001B[93mTest Loss: 74.336.. \n",
      "\u001B[32mTest loss decreased (74.673508 --> 74.336075).  Saving model ...\n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 27.137.. \n",
      "\u001B[93mTest Loss: 74.773.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 26.297.. \n",
      "\u001B[93mTest Loss: 76.314.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 24.990.. \n",
      "\u001B[93mTest Loss: 74.820.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 26.600.. \n",
      "\u001B[93mTest Loss: 78.107.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 24.558.. \n",
      "\u001B[93mTest Loss: 72.196.. \n",
      "\u001B[32mTest loss decreased (74.336075 --> 72.195793).  Saving model ...\n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 24.154.. \n",
      "\u001B[93mTest Loss: 72.520.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 23.168.. \n",
      "\u001B[93mTest Loss: 71.812.. \n",
      "\u001B[32mTest loss decreased (72.195793 --> 71.811813).  Saving model ...\n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 23.207.. \n",
      "\u001B[93mTest Loss: 76.846.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 24.527.. \n",
      "\u001B[93mTest Loss: 74.725.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 22.082.. \n",
      "\u001B[93mTest Loss: 76.102.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 23.314.. \n",
      "\u001B[93mTest Loss: 79.418.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 22.696.. \n",
      "\u001B[93mTest Loss: 74.737.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 21.078.. \n",
      "\u001B[93mTest Loss: 72.732.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 21.642.. \n",
      "\u001B[93mTest Loss: 73.478.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 21.056.. \n",
      "\u001B[93mTest Loss: 81.474.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 21.359.. \n",
      "\u001B[93mTest Loss: 73.357.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 18.363.. \n",
      "\u001B[93mTest Loss: 75.637.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 22.190.. \n",
      "\u001B[93mTest Loss: 77.400.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 18.654.. \n",
      "\u001B[93mTest Loss: 71.294.. \n",
      "\u001B[32mTest loss decreased (71.811813 --> 71.293541).  Saving model ...\n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 19.510.. \n",
      "\u001B[93mTest Loss: 72.262.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 18.339.. \n",
      "\u001B[93mTest Loss: 75.024.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 21.228.. \n",
      "\u001B[93mTest Loss: 73.880.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 17.962.. \n",
      "\u001B[93mTest Loss: 72.475.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 16.777.. \n",
      "\u001B[93mTest Loss: 74.474.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 18.033.. \n",
      "\u001B[93mTest Loss: 73.495.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 18.619.. \n",
      "\u001B[93mTest Loss: 73.417.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 19.095.. \n",
      "\u001B[93mTest Loss: 78.357.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 17.647.. \n",
      "\u001B[93mTest Loss: 76.170.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 18.587.. \n",
      "\u001B[93mTest Loss: 77.422.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 16.271.. \n",
      "\u001B[93mTest Loss: 71.676.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 15.874.. \n",
      "\u001B[93mTest Loss: 75.112.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 15.822.. \n",
      "\u001B[93mTest Loss: 74.131.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 15.929.. \n",
      "\u001B[93mTest Loss: 80.532.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 16.848.. \n",
      "\u001B[93mTest Loss: 74.608.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 14.468.. \n",
      "\u001B[93mTest Loss: 78.194.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 15.717.. \n",
      "\u001B[93mTest Loss: 72.771.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 14.512.. \n",
      "\u001B[93mTest Loss: 74.691.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 13.395.. \n",
      "\u001B[93mTest Loss: 72.366.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 15.479.. \n",
      "\u001B[93mTest Loss: 73.867.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 14.876.. \n",
      "\u001B[93mTest Loss: 74.593.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 17.094.. \n",
      "\u001B[93mTest Loss: 73.507.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 14.754.. \n",
      "\u001B[93mTest Loss: 77.595.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 14.875.. \n",
      "\u001B[93mTest Loss: 77.197.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 13.119.. \n",
      "\u001B[93mTest Loss: 72.306.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 13.758.. \n",
      "\u001B[93mTest Loss: 73.702.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 12.687.. \n",
      "\u001B[93mTest Loss: 72.975.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 12.839.. \n",
      "\u001B[93mTest Loss: 83.219.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 17.128.. \n",
      "\u001B[93mTest Loss: 71.283.. \n",
      "\u001B[32mTest loss decreased (71.293541 --> 71.283470).  Saving model ...\n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 12.276.. \n",
      "\u001B[93mTest Loss: 73.971.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 10.998.. \n",
      "\u001B[93mTest Loss: 75.240.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 14.342.. \n",
      "\u001B[93mTest Loss: 84.732.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 15.734.. \n",
      "\u001B[93mTest Loss: 73.758.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 12.338.. \n",
      "\u001B[93mTest Loss: 85.993.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 13.791.. \n",
      "\u001B[93mTest Loss: 76.475.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 11.059.. \n",
      "\u001B[93mTest Loss: 73.433.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 12.286.. \n",
      "\u001B[93mTest Loss: 74.105.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 16.333.. \n",
      "\u001B[93mTest Loss: 76.060.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 12.972.. \n",
      "\u001B[93mTest Loss: 72.365.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 10.487.. \n",
      "\u001B[93mTest Loss: 75.185.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 9.615.. \n",
      "\u001B[93mTest Loss: 73.760.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 9.277.. \n",
      "\u001B[93mTest Loss: 72.261.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 13.080.. \n",
      "\u001B[93mTest Loss: 76.558.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 13.216.. \n",
      "\u001B[93mTest Loss: 74.720.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 12.534.. \n",
      "\u001B[93mTest Loss: 72.433.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 10.215.. \n",
      "\u001B[93mTest Loss: 73.897.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 16.565.. \n",
      "\u001B[93mTest Loss: 74.911.. \n",
      "\u001B[36mEpoch: 115/200..  Training Loss: 11.191.. \n",
      "\u001B[93mTest Loss: 74.291.. \n",
      "\u001B[36mEpoch: 116/200..  Training Loss: 9.376.. \n",
      "\u001B[93mTest Loss: 73.112.. \n",
      "\u001B[36mEpoch: 117/200..  Training Loss: 8.695.. \n",
      "\u001B[93mTest Loss: 71.125.. \n",
      "\u001B[32mTest loss decreased (71.283470 --> 71.125160).  Saving model ...\n",
      "\u001B[36mEpoch: 118/200..  Training Loss: 14.071.. \n",
      "\u001B[93mTest Loss: 73.372.. \n",
      "\u001B[36mEpoch: 119/200..  Training Loss: 17.206.. \n",
      "\u001B[93mTest Loss: 73.017.. \n",
      "\u001B[36mEpoch: 120/200..  Training Loss: 10.303.. \n",
      "\u001B[93mTest Loss: 73.122.. \n",
      "\u001B[36mEpoch: 121/200..  Training Loss: 8.307.. \n",
      "\u001B[93mTest Loss: 73.392.. \n",
      "\u001B[36mEpoch: 122/200..  Training Loss: 12.003.. \n",
      "\u001B[93mTest Loss: 74.819.. \n",
      "\u001B[36mEpoch: 123/200..  Training Loss: 9.038.. \n",
      "\u001B[93mTest Loss: 73.309.. \n",
      "\u001B[36mEpoch: 124/200..  Training Loss: 9.824.. \n",
      "\u001B[93mTest Loss: 73.669.. \n",
      "\u001B[36mEpoch: 125/200..  Training Loss: 10.323.. \n",
      "\u001B[93mTest Loss: 72.945.. \n",
      "\u001B[36mEpoch: 126/200..  Training Loss: 8.254.. \n",
      "\u001B[93mTest Loss: 72.677.. \n",
      "\u001B[36mEpoch: 127/200..  Training Loss: 7.922.. \n",
      "\u001B[93mTest Loss: 74.044.. \n",
      "\u001B[36mEpoch: 128/200..  Training Loss: 8.155.. \n",
      "\u001B[93mTest Loss: 73.318.. \n",
      "\u001B[36mEpoch: 129/200..  Training Loss: 8.321.. \n",
      "\u001B[93mTest Loss: 74.413.. \n",
      "\u001B[36mEpoch: 130/200..  Training Loss: 11.449.. \n",
      "\u001B[93mTest Loss: 74.131.. \n",
      "\u001B[36mEpoch: 131/200..  Training Loss: 9.383.. \n",
      "\u001B[93mTest Loss: 73.169.. \n",
      "\u001B[36mEpoch: 132/200..  Training Loss: 9.314.. \n",
      "\u001B[93mTest Loss: 73.649.. \n",
      "\u001B[36mEpoch: 133/200..  Training Loss: 11.699.. \n",
      "\u001B[93mTest Loss: 81.583.. \n",
      "\u001B[36mEpoch: 134/200..  Training Loss: 11.428.. \n",
      "\u001B[93mTest Loss: 72.734.. \n",
      "\u001B[36mEpoch: 135/200..  Training Loss: 7.584.. \n",
      "\u001B[93mTest Loss: 72.788.. \n",
      "\u001B[36mEpoch: 136/200..  Training Loss: 7.076.. \n",
      "\u001B[93mTest Loss: 71.812.. \n",
      "\u001B[36mEpoch: 137/200..  Training Loss: 7.102.. \n",
      "\u001B[93mTest Loss: 73.673.. \n",
      "\u001B[36mEpoch: 138/200..  Training Loss: 7.380.. \n",
      "\u001B[93mTest Loss: 73.344.. \n",
      "\u001B[36mEpoch: 139/200..  Training Loss: 13.057.. \n",
      "\u001B[93mTest Loss: 76.319.. \n",
      "\u001B[36mEpoch: 140/200..  Training Loss: 14.159.. \n",
      "\u001B[93mTest Loss: 75.429.. \n",
      "\u001B[36mEpoch: 141/200..  Training Loss: 12.119.. \n",
      "\u001B[93mTest Loss: 75.774.. \n",
      "\u001B[36mEpoch: 142/200..  Training Loss: 9.468.. \n",
      "\u001B[93mTest Loss: 72.734.. \n",
      "\u001B[36mEpoch: 143/200..  Training Loss: 6.714.. \n",
      "\u001B[93mTest Loss: 72.890.. \n",
      "\u001B[36mEpoch: 144/200..  Training Loss: 6.248.. \n",
      "\u001B[93mTest Loss: 73.575.. \n",
      "\u001B[36mEpoch: 145/200..  Training Loss: 6.482.. \n",
      "\u001B[93mTest Loss: 73.402.. \n",
      "\u001B[36mEpoch: 146/200..  Training Loss: 7.226.. \n",
      "\u001B[93mTest Loss: 76.039.. \n",
      "\u001B[36mEpoch: 147/200..  Training Loss: 9.237.. \n",
      "\u001B[93mTest Loss: 90.816.. \n",
      "\u001B[36mEpoch: 148/200..  Training Loss: 19.989.. \n",
      "\u001B[93mTest Loss: 75.519.. \n",
      "\u001B[36mEpoch: 149/200..  Training Loss: 8.990.. \n",
      "\u001B[93mTest Loss: 73.195.. \n",
      "\u001B[36mEpoch: 150/200..  Training Loss: 6.535.. \n",
      "\u001B[93mTest Loss: 74.051.. \n",
      "\u001B[36mEpoch: 151/200..  Training Loss: 6.126.. \n",
      "\u001B[93mTest Loss: 73.270.. \n",
      "\u001B[36mEpoch: 152/200..  Training Loss: 6.155.. \n",
      "\u001B[93mTest Loss: 74.700.. \n",
      "\u001B[36mEpoch: 153/200..  Training Loss: 6.416.. \n",
      "\u001B[93mTest Loss: 73.604.. \n",
      "\u001B[36mEpoch: 154/200..  Training Loss: 11.073.. \n",
      "\u001B[93mTest Loss: 75.573.. \n",
      "\u001B[36mEpoch: 155/200..  Training Loss: 9.814.. \n",
      "\u001B[93mTest Loss: 75.703.. \n",
      "\u001B[36mEpoch: 156/200..  Training Loss: 6.926.. \n",
      "\u001B[93mTest Loss: 75.153.. \n",
      "\u001B[36mEpoch: 157/200..  Training Loss: 6.006.. \n",
      "\u001B[93mTest Loss: 73.567.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  362.70464301109314 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 102.774.. \n",
      "\u001B[93mTest Loss: 104.609.. \n",
      "\u001B[32mTest loss decreased (inf --> 104.609322).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 80.567.. \n",
      "\u001B[93mTest Loss: 99.771.. \n",
      "\u001B[32mTest loss decreased (104.609322 --> 99.770920).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 76.353.. \n",
      "\u001B[93mTest Loss: 102.082.. \n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 73.752.. \n",
      "\u001B[93mTest Loss: 97.635.. \n",
      "\u001B[32mTest loss decreased (99.770920 --> 97.634575).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 72.950.. \n",
      "\u001B[93mTest Loss: 98.852.. \n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 71.135.. \n",
      "\u001B[93mTest Loss: 94.243.. \n",
      "\u001B[32mTest loss decreased (97.634575 --> 94.242912).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 69.849.. \n",
      "\u001B[93mTest Loss: 97.590.. \n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 69.227.. \n",
      "\u001B[93mTest Loss: 95.856.. \n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 68.777.. \n",
      "\u001B[93mTest Loss: 99.321.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 67.722.. \n",
      "\u001B[93mTest Loss: 91.274.. \n",
      "\u001B[32mTest loss decreased (94.242912 --> 91.273819).  Saving model ...\n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 66.641.. \n",
      "\u001B[93mTest Loss: 93.022.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 65.648.. \n",
      "\u001B[93mTest Loss: 89.334.. \n",
      "\u001B[32mTest loss decreased (91.273819 --> 89.333710).  Saving model ...\n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 64.852.. \n",
      "\u001B[93mTest Loss: 91.372.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 63.803.. \n",
      "\u001B[93mTest Loss: 86.779.. \n",
      "\u001B[32mTest loss decreased (89.333710 --> 86.778816).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 63.339.. \n",
      "\u001B[93mTest Loss: 89.949.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 61.726.. \n",
      "\u001B[93mTest Loss: 91.716.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 60.347.. \n",
      "\u001B[93mTest Loss: 82.486.. \n",
      "\u001B[32mTest loss decreased (86.778816 --> 82.486259).  Saving model ...\n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 59.487.. \n",
      "\u001B[93mTest Loss: 83.109.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 59.434.. \n",
      "\u001B[93mTest Loss: 83.329.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 57.340.. \n",
      "\u001B[93mTest Loss: 80.487.. \n",
      "\u001B[32mTest loss decreased (82.486259 --> 80.486794).  Saving model ...\n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 57.079.. \n",
      "\u001B[93mTest Loss: 84.810.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 56.988.. \n",
      "\u001B[93mTest Loss: 80.654.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 55.036.. \n",
      "\u001B[93mTest Loss: 97.082.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 55.936.. \n",
      "\u001B[93mTest Loss: 83.400.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 55.408.. \n",
      "\u001B[93mTest Loss: 82.246.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 54.768.. \n",
      "\u001B[93mTest Loss: 79.738.. \n",
      "\u001B[32mTest loss decreased (80.486794 --> 79.738106).  Saving model ...\n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 53.011.. \n",
      "\u001B[93mTest Loss: 76.682.. \n",
      "\u001B[32mTest loss decreased (79.738106 --> 76.681915).  Saving model ...\n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 52.545.. \n",
      "\u001B[93mTest Loss: 80.211.. \n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 52.594.. \n",
      "\u001B[93mTest Loss: 76.114.. \n",
      "\u001B[32mTest loss decreased (76.681915 --> 76.113617).  Saving model ...\n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 53.023.. \n",
      "\u001B[93mTest Loss: 79.979.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 50.701.. \n",
      "\u001B[93mTest Loss: 75.149.. \n",
      "\u001B[32mTest loss decreased (76.113617 --> 75.149071).  Saving model ...\n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 51.414.. \n",
      "\u001B[93mTest Loss: 77.671.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 52.198.. \n",
      "\u001B[93mTest Loss: 75.962.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 51.486.. \n",
      "\u001B[93mTest Loss: 76.618.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 47.859.. \n",
      "\u001B[93mTest Loss: 79.649.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 49.299.. \n",
      "\u001B[93mTest Loss: 79.861.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 47.660.. \n",
      "\u001B[93mTest Loss: 75.073.. \n",
      "\u001B[32mTest loss decreased (75.149071 --> 75.073219).  Saving model ...\n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 47.336.. \n",
      "\u001B[93mTest Loss: 72.229.. \n",
      "\u001B[32mTest loss decreased (75.073219 --> 72.229477).  Saving model ...\n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 47.226.. \n",
      "\u001B[93mTest Loss: 73.981.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 47.464.. \n",
      "\u001B[93mTest Loss: 69.796.. \n",
      "\u001B[32mTest loss decreased (72.229477 --> 69.795631).  Saving model ...\n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 47.946.. \n",
      "\u001B[93mTest Loss: 69.316.. \n",
      "\u001B[32mTest loss decreased (69.795631 --> 69.315514).  Saving model ...\n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 46.101.. \n",
      "\u001B[93mTest Loss: 75.246.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 46.309.. \n",
      "\u001B[93mTest Loss: 74.594.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 45.532.. \n",
      "\u001B[93mTest Loss: 73.313.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 43.667.. \n",
      "\u001B[93mTest Loss: 70.449.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 44.424.. \n",
      "\u001B[93mTest Loss: 72.197.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 44.558.. \n",
      "\u001B[93mTest Loss: 69.105.. \n",
      "\u001B[32mTest loss decreased (69.315514 --> 69.104767).  Saving model ...\n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 47.232.. \n",
      "\u001B[93mTest Loss: 73.249.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 45.385.. \n",
      "\u001B[93mTest Loss: 75.950.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 44.060.. \n",
      "\u001B[93mTest Loss: 69.498.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 42.843.. \n",
      "\u001B[93mTest Loss: 69.339.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 43.243.. \n",
      "\u001B[93mTest Loss: 74.014.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 42.273.. \n",
      "\u001B[93mTest Loss: 69.479.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 42.523.. \n",
      "\u001B[93mTest Loss: 72.402.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 40.860.. \n",
      "\u001B[93mTest Loss: 72.504.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 44.512.. \n",
      "\u001B[93mTest Loss: 71.977.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 42.295.. \n",
      "\u001B[93mTest Loss: 70.421.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 42.923.. \n",
      "\u001B[93mTest Loss: 70.871.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 40.433.. \n",
      "\u001B[93mTest Loss: 81.850.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 42.505.. \n",
      "\u001B[93mTest Loss: 71.435.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 39.180.. \n",
      "\u001B[93mTest Loss: 71.145.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 41.543.. \n",
      "\u001B[93mTest Loss: 69.646.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 40.900.. \n",
      "\u001B[93mTest Loss: 73.594.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 41.727.. \n",
      "\u001B[93mTest Loss: 68.187.. \n",
      "\u001B[32mTest loss decreased (69.104767 --> 68.187401).  Saving model ...\n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 39.360.. \n",
      "\u001B[93mTest Loss: 71.548.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 41.115.. \n",
      "\u001B[93mTest Loss: 76.566.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 39.858.. \n",
      "\u001B[93mTest Loss: 73.304.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 38.536.. \n",
      "\u001B[93mTest Loss: 72.909.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 38.133.. \n",
      "\u001B[93mTest Loss: 71.427.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 37.878.. \n",
      "\u001B[93mTest Loss: 77.682.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 39.472.. \n",
      "\u001B[93mTest Loss: 70.139.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 41.179.. \n",
      "\u001B[93mTest Loss: 72.166.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 38.781.. \n",
      "\u001B[93mTest Loss: 72.341.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 39.294.. \n",
      "\u001B[93mTest Loss: 76.175.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 37.684.. \n",
      "\u001B[93mTest Loss: 73.445.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 41.209.. \n",
      "\u001B[93mTest Loss: 73.001.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 37.055.. \n",
      "\u001B[93mTest Loss: 72.707.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 39.944.. \n",
      "\u001B[93mTest Loss: 70.878.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 38.818.. \n",
      "\u001B[93mTest Loss: 69.447.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 36.835.. \n",
      "\u001B[93mTest Loss: 69.270.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 36.293.. \n",
      "\u001B[93mTest Loss: 71.835.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 35.349.. \n",
      "\u001B[93mTest Loss: 71.633.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 37.855.. \n",
      "\u001B[93mTest Loss: 71.198.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 37.376.. \n",
      "\u001B[93mTest Loss: 69.781.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 35.477.. \n",
      "\u001B[93mTest Loss: 78.617.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 39.852.. \n",
      "\u001B[93mTest Loss: 69.668.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 34.950.. \n",
      "\u001B[93mTest Loss: 69.248.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 34.685.. \n",
      "\u001B[93mTest Loss: 72.317.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 34.197.. \n",
      "\u001B[93mTest Loss: 70.657.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 37.236.. \n",
      "\u001B[93mTest Loss: 87.264.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 41.194.. \n",
      "\u001B[93mTest Loss: 77.418.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 36.989.. \n",
      "\u001B[93mTest Loss: 70.214.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 33.914.. \n",
      "\u001B[93mTest Loss: 71.040.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 32.521.. \n",
      "\u001B[93mTest Loss: 70.700.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 38.973.. \n",
      "\u001B[93mTest Loss: 77.981.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 36.220.. \n",
      "\u001B[93mTest Loss: 69.582.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 32.967.. \n",
      "\u001B[93mTest Loss: 71.165.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 35.395.. \n",
      "\u001B[93mTest Loss: 81.820.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 39.298.. \n",
      "\u001B[93mTest Loss: 78.058.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 34.035.. \n",
      "\u001B[93mTest Loss: 73.478.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 31.742.. \n",
      "\u001B[93mTest Loss: 70.596.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 33.364.. \n",
      "\u001B[93mTest Loss: 74.111.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 40.351.. \n",
      "\u001B[93mTest Loss: 75.456.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 33.992.. \n",
      "\u001B[93mTest Loss: 70.059.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  278.2388234138489 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 102.659.. \n",
      "\u001B[93mTest Loss: 105.795.. \n",
      "\u001B[32mTest loss decreased (inf --> 105.794632).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 89.132.. \n",
      "\u001B[93mTest Loss: 104.188.. \n",
      "\u001B[32mTest loss decreased (105.794632 --> 104.188286).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 85.815.. \n",
      "\u001B[93mTest Loss: 99.467.. \n",
      "\u001B[32mTest loss decreased (104.188286 --> 99.466888).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 83.634.. \n",
      "\u001B[93mTest Loss: 102.017.. \n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 80.974.. \n",
      "\u001B[93mTest Loss: 96.590.. \n",
      "\u001B[32mTest loss decreased (99.466888 --> 96.589813).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 80.507.. \n",
      "\u001B[93mTest Loss: 97.957.. \n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 78.719.. \n",
      "\u001B[93mTest Loss: 95.035.. \n",
      "\u001B[32mTest loss decreased (96.589813 --> 95.035385).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 78.118.. \n",
      "\u001B[93mTest Loss: 94.667.. \n",
      "\u001B[32mTest loss decreased (95.035385 --> 94.666565).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 77.223.. \n",
      "\u001B[93mTest Loss: 94.420.. \n",
      "\u001B[32mTest loss decreased (94.666565 --> 94.420395).  Saving model ...\n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 74.301.. \n",
      "\u001B[93mTest Loss: 97.992.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 73.914.. \n",
      "\u001B[93mTest Loss: 94.592.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 72.337.. \n",
      "\u001B[93mTest Loss: 93.519.. \n",
      "\u001B[32mTest loss decreased (94.420395 --> 93.518539).  Saving model ...\n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 70.592.. \n",
      "\u001B[93mTest Loss: 98.918.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 72.057.. \n",
      "\u001B[93mTest Loss: 92.871.. \n",
      "\u001B[32mTest loss decreased (93.518539 --> 92.871269).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 69.147.. \n",
      "\u001B[93mTest Loss: 95.097.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 68.315.. \n",
      "\u001B[93mTest Loss: 89.808.. \n",
      "\u001B[32mTest loss decreased (92.871269 --> 89.807961).  Saving model ...\n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 65.950.. \n",
      "\u001B[93mTest Loss: 91.738.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 65.634.. \n",
      "\u001B[93mTest Loss: 96.583.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 63.058.. \n",
      "\u001B[93mTest Loss: 93.761.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 64.043.. \n",
      "\u001B[93mTest Loss: 90.335.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 60.766.. \n",
      "\u001B[93mTest Loss: 91.535.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 60.006.. \n",
      "\u001B[93mTest Loss: 89.474.. \n",
      "\u001B[32mTest loss decreased (89.807961 --> 89.473923).  Saving model ...\n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 59.034.. \n",
      "\u001B[93mTest Loss: 90.771.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 56.817.. \n",
      "\u001B[93mTest Loss: 89.097.. \n",
      "\u001B[32mTest loss decreased (89.473923 --> 89.097389).  Saving model ...\n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 55.853.. \n",
      "\u001B[93mTest Loss: 89.488.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 53.641.. \n",
      "\u001B[93mTest Loss: 88.023.. \n",
      "\u001B[32mTest loss decreased (89.097389 --> 88.023262).  Saving model ...\n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 53.224.. \n",
      "\u001B[93mTest Loss: 83.978.. \n",
      "\u001B[32mTest loss decreased (88.023262 --> 83.978195).  Saving model ...\n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 50.557.. \n",
      "\u001B[93mTest Loss: 83.270.. \n",
      "\u001B[32mTest loss decreased (83.978195 --> 83.269669).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 49.393.. \n",
      "\u001B[93mTest Loss: 90.414.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 47.863.. \n",
      "\u001B[93mTest Loss: 84.310.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 48.310.. \n",
      "\u001B[93mTest Loss: 79.609.. \n",
      "\u001B[32mTest loss decreased (83.269669 --> 79.609421).  Saving model ...\n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 43.835.. \n",
      "\u001B[93mTest Loss: 85.629.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 44.509.. \n",
      "\u001B[93mTest Loss: 82.958.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 43.455.. \n",
      "\u001B[93mTest Loss: 83.597.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 41.333.. \n",
      "\u001B[93mTest Loss: 79.530.. \n",
      "\u001B[32mTest loss decreased (79.609421 --> 79.529823).  Saving model ...\n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 40.523.. \n",
      "\u001B[93mTest Loss: 77.426.. \n",
      "\u001B[32mTest loss decreased (79.529823 --> 77.426384).  Saving model ...\n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 38.525.. \n",
      "\u001B[93mTest Loss: 83.405.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 40.499.. \n",
      "\u001B[93mTest Loss: 81.430.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 37.985.. \n",
      "\u001B[93mTest Loss: 92.645.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 37.181.. \n",
      "\u001B[93mTest Loss: 85.395.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 38.658.. \n",
      "\u001B[93mTest Loss: 83.209.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 33.736.. \n",
      "\u001B[93mTest Loss: 75.214.. \n",
      "\u001B[32mTest loss decreased (77.426384 --> 75.213623).  Saving model ...\n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 35.117.. \n",
      "\u001B[93mTest Loss: 75.280.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 35.376.. \n",
      "\u001B[93mTest Loss: 79.292.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 34.521.. \n",
      "\u001B[93mTest Loss: 78.721.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 32.656.. \n",
      "\u001B[93mTest Loss: 77.393.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 32.869.. \n",
      "\u001B[93mTest Loss: 76.218.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 31.173.. \n",
      "\u001B[93mTest Loss: 82.333.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 34.139.. \n",
      "\u001B[93mTest Loss: 74.011.. \n",
      "\u001B[32mTest loss decreased (75.213623 --> 74.010765).  Saving model ...\n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 31.035.. \n",
      "\u001B[93mTest Loss: 76.347.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 29.788.. \n",
      "\u001B[93mTest Loss: 76.315.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 28.525.. \n",
      "\u001B[93mTest Loss: 73.202.. \n",
      "\u001B[32mTest loss decreased (74.010765 --> 73.202232).  Saving model ...\n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 29.597.. \n",
      "\u001B[93mTest Loss: 74.490.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 29.024.. \n",
      "\u001B[93mTest Loss: 74.145.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 31.578.. \n",
      "\u001B[93mTest Loss: 79.308.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 29.339.. \n",
      "\u001B[93mTest Loss: 75.992.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 28.272.. \n",
      "\u001B[93mTest Loss: 75.792.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 28.243.. \n",
      "\u001B[93mTest Loss: 89.852.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 27.437.. \n",
      "\u001B[93mTest Loss: 77.883.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 26.349.. \n",
      "\u001B[93mTest Loss: 79.655.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 24.658.. \n",
      "\u001B[93mTest Loss: 78.776.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 29.597.. \n",
      "\u001B[93mTest Loss: 77.533.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 27.518.. \n",
      "\u001B[93mTest Loss: 75.800.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 24.698.. \n",
      "\u001B[93mTest Loss: 78.366.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 25.111.. \n",
      "\u001B[93mTest Loss: 75.033.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 24.410.. \n",
      "\u001B[93mTest Loss: 78.671.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 27.983.. \n",
      "\u001B[93mTest Loss: 76.841.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 24.160.. \n",
      "\u001B[93mTest Loss: 79.612.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 22.923.. \n",
      "\u001B[93mTest Loss: 80.159.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 27.826.. \n",
      "\u001B[93mTest Loss: 76.978.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 22.820.. \n",
      "\u001B[93mTest Loss: 75.480.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 22.188.. \n",
      "\u001B[93mTest Loss: 76.267.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 25.735.. \n",
      "\u001B[93mTest Loss: 75.265.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 21.316.. \n",
      "\u001B[93mTest Loss: 74.664.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 20.221.. \n",
      "\u001B[93mTest Loss: 76.101.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 23.552.. \n",
      "\u001B[93mTest Loss: 77.782.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 21.620.. \n",
      "\u001B[93mTest Loss: 73.061.. \n",
      "\u001B[32mTest loss decreased (73.202232 --> 73.061470).  Saving model ...\n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 19.501.. \n",
      "\u001B[93mTest Loss: 76.380.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 20.335.. \n",
      "\u001B[93mTest Loss: 75.570.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 22.599.. \n",
      "\u001B[93mTest Loss: 74.758.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 20.944.. \n",
      "\u001B[93mTest Loss: 74.696.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 21.542.. \n",
      "\u001B[93mTest Loss: 74.252.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 18.630.. \n",
      "\u001B[93mTest Loss: 75.702.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 17.527.. \n",
      "\u001B[93mTest Loss: 75.316.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 21.935.. \n",
      "\u001B[93mTest Loss: 86.147.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 23.436.. \n",
      "\u001B[93mTest Loss: 76.811.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 18.559.. \n",
      "\u001B[93mTest Loss: 78.382.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 18.045.. \n",
      "\u001B[93mTest Loss: 79.584.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 19.911.. \n",
      "\u001B[93mTest Loss: 75.035.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 18.792.. \n",
      "\u001B[93mTest Loss: 78.372.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 16.279.. \n",
      "\u001B[93mTest Loss: 79.303.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 17.544.. \n",
      "\u001B[93mTest Loss: 75.623.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 19.300.. \n",
      "\u001B[93mTest Loss: 76.497.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 16.247.. \n",
      "\u001B[93mTest Loss: 78.416.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 17.376.. \n",
      "\u001B[93mTest Loss: 76.555.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 20.696.. \n",
      "\u001B[93mTest Loss: 78.463.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 17.914.. \n",
      "\u001B[93mTest Loss: 77.290.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 16.770.. \n",
      "\u001B[93mTest Loss: 78.678.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 16.273.. \n",
      "\u001B[93mTest Loss: 75.887.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 17.103.. \n",
      "\u001B[93mTest Loss: 78.403.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 18.965.. \n",
      "\u001B[93mTest Loss: 75.463.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 15.744.. \n",
      "\u001B[93mTest Loss: 75.297.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 16.095.. \n",
      "\u001B[93mTest Loss: 80.616.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 17.397.. \n",
      "\u001B[93mTest Loss: 78.184.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 19.789.. \n",
      "\u001B[93mTest Loss: 78.620.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 18.509.. \n",
      "\u001B[93mTest Loss: 77.605.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 16.420.. \n",
      "\u001B[93mTest Loss: 77.176.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 16.344.. \n",
      "\u001B[93mTest Loss: 74.972.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 13.928.. \n",
      "\u001B[93mTest Loss: 77.384.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 13.071.. \n",
      "\u001B[93mTest Loss: 74.284.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 12.787.. \n",
      "\u001B[93mTest Loss: 74.965.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 13.136.. \n",
      "\u001B[93mTest Loss: 74.835.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 13.488.. \n",
      "\u001B[93mTest Loss: 78.195.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 20.905.. \n",
      "\u001B[93mTest Loss: 77.577.. \n",
      "\u001B[36mEpoch: 115/200..  Training Loss: 20.147.. \n",
      "\u001B[93mTest Loss: 75.703.. \n",
      "\u001B[36mEpoch: 116/200..  Training Loss: 14.739.. \n",
      "\u001B[93mTest Loss: 75.204.. \n",
      "\u001B[36mEpoch: 117/200..  Training Loss: 15.984.. \n",
      "\u001B[93mTest Loss: 76.500.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  266.82100462913513 seconds\n",
      "\u001B[32mLowest Test loss achieved for Standard Net: 71.125160\n",
      "\u001B[32mAt Epoch #116 \n",
      "\n",
      "\u001B[32mLowest Elastic-Net loss achieved: 68.187401\n",
      "\u001B[32mAt Epoch #63 \n",
      "\n",
      "\u001B[32mLowest Test loss achieved for Gradient-Net: 73.061470\n",
      "\u001B[32mAt Epoch #76 \n",
      "\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 100.000.. \n",
      "\u001B[93mTest Loss: 104.182.. \n",
      "\u001B[32mTest loss decreased (inf --> 104.182091).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 87.078.. \n",
      "\u001B[93mTest Loss: 100.017.. \n",
      "\u001B[32mTest loss decreased (104.182091 --> 100.017395).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 82.109.. \n",
      "\u001B[93mTest Loss: 95.781.. \n",
      "\u001B[32mTest loss decreased (100.017395 --> 95.780739).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 80.461.. \n",
      "\u001B[93mTest Loss: 97.226.. \n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 78.516.. \n",
      "\u001B[93mTest Loss: 94.401.. \n",
      "\u001B[32mTest loss decreased (95.780739 --> 94.400993).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 77.267.. \n",
      "\u001B[93mTest Loss: 96.389.. \n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 75.975.. \n",
      "\u001B[93mTest Loss: 95.422.. \n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 74.476.. \n",
      "\u001B[93mTest Loss: 95.338.. \n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 73.090.. \n",
      "\u001B[93mTest Loss: 92.149.. \n",
      "\u001B[32mTest loss decreased (94.400993 --> 92.148926).  Saving model ...\n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 72.621.. \n",
      "\u001B[93mTest Loss: 92.890.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 70.669.. \n",
      "\u001B[93mTest Loss: 93.710.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 69.719.. \n",
      "\u001B[93mTest Loss: 92.124.. \n",
      "\u001B[32mTest loss decreased (92.148926 --> 92.124367).  Saving model ...\n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 67.114.. \n",
      "\u001B[93mTest Loss: 94.976.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 67.236.. \n",
      "\u001B[93mTest Loss: 93.663.. \n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 65.318.. \n",
      "\u001B[93mTest Loss: 90.849.. \n",
      "\u001B[32mTest loss decreased (92.124367 --> 90.849358).  Saving model ...\n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 63.970.. \n",
      "\u001B[93mTest Loss: 95.618.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 63.566.. \n",
      "\u001B[93mTest Loss: 92.095.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 61.899.. \n",
      "\u001B[93mTest Loss: 91.910.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 61.697.. \n",
      "\u001B[93mTest Loss: 87.114.. \n",
      "\u001B[32mTest loss decreased (90.849358 --> 87.114311).  Saving model ...\n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 59.605.. \n",
      "\u001B[93mTest Loss: 89.037.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 56.746.. \n",
      "\u001B[93mTest Loss: 89.520.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 55.963.. \n",
      "\u001B[93mTest Loss: 90.900.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 56.537.. \n",
      "\u001B[93mTest Loss: 89.574.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 53.214.. \n",
      "\u001B[93mTest Loss: 89.477.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 51.751.. \n",
      "\u001B[93mTest Loss: 88.812.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 51.233.. \n",
      "\u001B[93mTest Loss: 86.834.. \n",
      "\u001B[32mTest loss decreased (87.114311 --> 86.834435).  Saving model ...\n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 47.770.. \n",
      "\u001B[93mTest Loss: 90.794.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 47.488.. \n",
      "\u001B[93mTest Loss: 80.393.. \n",
      "\u001B[32mTest loss decreased (86.834435 --> 80.393303).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 46.098.. \n",
      "\u001B[93mTest Loss: 84.575.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 43.389.. \n",
      "\u001B[93mTest Loss: 83.169.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 42.359.. \n",
      "\u001B[93mTest Loss: 85.187.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 41.899.. \n",
      "\u001B[93mTest Loss: 81.277.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 40.206.. \n",
      "\u001B[93mTest Loss: 77.817.. \n",
      "\u001B[32mTest loss decreased (80.393303 --> 77.817062).  Saving model ...\n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 37.488.. \n",
      "\u001B[93mTest Loss: 77.274.. \n",
      "\u001B[32mTest loss decreased (77.817062 --> 77.274025).  Saving model ...\n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 38.201.. \n",
      "\u001B[93mTest Loss: 79.693.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 36.395.. \n",
      "\u001B[93mTest Loss: 79.525.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 34.749.. \n",
      "\u001B[93mTest Loss: 77.756.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 34.215.. \n",
      "\u001B[93mTest Loss: 77.234.. \n",
      "\u001B[32mTest loss decreased (77.274025 --> 77.234329).  Saving model ...\n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 33.684.. \n",
      "\u001B[93mTest Loss: 81.637.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 33.815.. \n",
      "\u001B[93mTest Loss: 77.345.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 31.794.. \n",
      "\u001B[93mTest Loss: 79.114.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 30.173.. \n",
      "\u001B[93mTest Loss: 77.053.. \n",
      "\u001B[32mTest loss decreased (77.234329 --> 77.053261).  Saving model ...\n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 30.291.. \n",
      "\u001B[93mTest Loss: 75.166.. \n",
      "\u001B[32mTest loss decreased (77.053261 --> 75.165924).  Saving model ...\n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 29.345.. \n",
      "\u001B[93mTest Loss: 75.830.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 30.206.. \n",
      "\u001B[93mTest Loss: 76.294.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 27.101.. \n",
      "\u001B[93mTest Loss: 73.896.. \n",
      "\u001B[32mTest loss decreased (75.165924 --> 73.895531).  Saving model ...\n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 27.154.. \n",
      "\u001B[93mTest Loss: 83.695.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 28.133.. \n",
      "\u001B[93mTest Loss: 74.349.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 25.677.. \n",
      "\u001B[93mTest Loss: 77.315.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 25.925.. \n",
      "\u001B[93mTest Loss: 73.939.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 27.749.. \n",
      "\u001B[93mTest Loss: 79.456.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 27.323.. \n",
      "\u001B[93mTest Loss: 74.709.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 23.645.. \n",
      "\u001B[93mTest Loss: 74.576.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 24.206.. \n",
      "\u001B[93mTest Loss: 74.795.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 24.258.. \n",
      "\u001B[93mTest Loss: 77.165.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 22.340.. \n",
      "\u001B[93mTest Loss: 70.105.. \n",
      "\u001B[32mTest loss decreased (73.895531 --> 70.104820).  Saving model ...\n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 23.946.. \n",
      "\u001B[93mTest Loss: 75.436.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 22.600.. \n",
      "\u001B[93mTest Loss: 72.751.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 22.443.. \n",
      "\u001B[93mTest Loss: 77.090.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 21.640.. \n",
      "\u001B[93mTest Loss: 72.023.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 19.964.. \n",
      "\u001B[93mTest Loss: 71.170.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 19.396.. \n",
      "\u001B[93mTest Loss: 73.087.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 20.026.. \n",
      "\u001B[93mTest Loss: 76.677.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 24.549.. \n",
      "\u001B[93mTest Loss: 76.971.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 19.443.. \n",
      "\u001B[93mTest Loss: 71.724.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 17.125.. \n",
      "\u001B[93mTest Loss: 71.878.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 18.318.. \n",
      "\u001B[93mTest Loss: 80.168.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 19.028.. \n",
      "\u001B[93mTest Loss: 77.405.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 21.780.. \n",
      "\u001B[93mTest Loss: 75.837.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 19.270.. \n",
      "\u001B[93mTest Loss: 72.552.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 19.412.. \n",
      "\u001B[93mTest Loss: 69.441.. \n",
      "\u001B[32mTest loss decreased (70.104820 --> 69.440842).  Saving model ...\n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 18.148.. \n",
      "\u001B[93mTest Loss: 71.357.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 15.496.. \n",
      "\u001B[93mTest Loss: 71.980.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 15.018.. \n",
      "\u001B[93mTest Loss: 70.763.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 23.023.. \n",
      "\u001B[93mTest Loss: 73.489.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 17.728.. \n",
      "\u001B[93mTest Loss: 73.789.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 15.279.. \n",
      "\u001B[93mTest Loss: 71.021.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 14.397.. \n",
      "\u001B[93mTest Loss: 71.802.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 15.557.. \n",
      "\u001B[93mTest Loss: 73.293.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 16.164.. \n",
      "\u001B[93mTest Loss: 78.352.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 17.647.. \n",
      "\u001B[93mTest Loss: 73.087.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 14.193.. \n",
      "\u001B[93mTest Loss: 69.848.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 14.613.. \n",
      "\u001B[93mTest Loss: 73.742.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 16.280.. \n",
      "\u001B[93mTest Loss: 80.410.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 18.392.. \n",
      "\u001B[93mTest Loss: 71.586.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 14.518.. \n",
      "\u001B[93mTest Loss: 69.852.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 12.370.. \n",
      "\u001B[93mTest Loss: 73.458.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 15.164.. \n",
      "\u001B[93mTest Loss: 70.808.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 12.543.. \n",
      "\u001B[93mTest Loss: 71.093.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 12.931.. \n",
      "\u001B[93mTest Loss: 70.584.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 12.538.. \n",
      "\u001B[93mTest Loss: 72.981.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 13.574.. \n",
      "\u001B[93mTest Loss: 72.246.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 17.576.. \n",
      "\u001B[93mTest Loss: 74.024.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 14.100.. \n",
      "\u001B[93mTest Loss: 70.454.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 11.698.. \n",
      "\u001B[93mTest Loss: 70.822.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 10.831.. \n",
      "\u001B[93mTest Loss: 70.746.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 11.008.. \n",
      "\u001B[93mTest Loss: 73.692.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 14.605.. \n",
      "\u001B[93mTest Loss: 74.279.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 15.262.. \n",
      "\u001B[93mTest Loss: 71.703.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 12.164.. \n",
      "\u001B[93mTest Loss: 73.264.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 11.424.. \n",
      "\u001B[93mTest Loss: 70.352.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 10.173.. \n",
      "\u001B[93mTest Loss: 71.298.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 10.616.. \n",
      "\u001B[93mTest Loss: 70.222.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 9.898.. \n",
      "\u001B[93mTest Loss: 71.230.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 10.200.. \n",
      "\u001B[93mTest Loss: 70.111.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 10.420.. \n",
      "\u001B[93mTest Loss: 71.895.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 11.134.. \n",
      "\u001B[93mTest Loss: 73.734.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 13.852.. \n",
      "\u001B[93mTest Loss: 71.098.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 13.847.. \n",
      "\u001B[93mTest Loss: 74.926.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 12.642.. \n",
      "\u001B[93mTest Loss: 72.063.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 11.712.. \n",
      "\u001B[93mTest Loss: 76.505.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  205.3106405735016 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 104.753.. \n",
      "\u001B[93mTest Loss: 106.143.. \n",
      "\u001B[32mTest loss decreased (inf --> 106.142685).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 81.059.. \n",
      "\u001B[93mTest Loss: 101.137.. \n",
      "\u001B[32mTest loss decreased (106.142685 --> 101.136749).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 76.621.. \n",
      "\u001B[93mTest Loss: 99.166.. \n",
      "\u001B[32mTest loss decreased (101.136749 --> 99.165909).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 74.859.. \n",
      "\u001B[93mTest Loss: 98.678.. \n",
      "\u001B[32mTest loss decreased (99.165909 --> 98.677879).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 72.391.. \n",
      "\u001B[93mTest Loss: 98.263.. \n",
      "\u001B[32mTest loss decreased (98.677879 --> 98.263382).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 71.571.. \n",
      "\u001B[93mTest Loss: 104.701.. \n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 70.887.. \n",
      "\u001B[93mTest Loss: 94.625.. \n",
      "\u001B[32mTest loss decreased (98.263382 --> 94.624626).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 69.398.. \n",
      "\u001B[93mTest Loss: 94.834.. \n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 68.796.. \n",
      "\u001B[93mTest Loss: 94.917.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 68.328.. \n",
      "\u001B[93mTest Loss: 91.866.. \n",
      "\u001B[32mTest loss decreased (94.624626 --> 91.865776).  Saving model ...\n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 67.626.. \n",
      "\u001B[93mTest Loss: 93.011.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 66.430.. \n",
      "\u001B[93mTest Loss: 94.357.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 66.624.. \n",
      "\u001B[93mTest Loss: 94.351.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 66.142.. \n",
      "\u001B[93mTest Loss: 90.409.. \n",
      "\u001B[32mTest loss decreased (91.865776 --> 90.408600).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 64.822.. \n",
      "\u001B[93mTest Loss: 91.432.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 64.088.. \n",
      "\u001B[93mTest Loss: 94.413.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 64.472.. \n",
      "\u001B[93mTest Loss: 90.937.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 63.508.. \n",
      "\u001B[93mTest Loss: 93.405.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 63.504.. \n",
      "\u001B[93mTest Loss: 89.312.. \n",
      "\u001B[32mTest loss decreased (90.408600 --> 89.312256).  Saving model ...\n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 63.198.. \n",
      "\u001B[93mTest Loss: 89.430.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 61.763.. \n",
      "\u001B[93mTest Loss: 89.353.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 62.295.. \n",
      "\u001B[93mTest Loss: 87.974.. \n",
      "\u001B[32mTest loss decreased (89.312256 --> 87.973892).  Saving model ...\n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 60.238.. \n",
      "\u001B[93mTest Loss: 87.387.. \n",
      "\u001B[32mTest loss decreased (87.973892 --> 87.386955).  Saving model ...\n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 61.012.. \n",
      "\u001B[93mTest Loss: 87.565.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 60.095.. \n",
      "\u001B[93mTest Loss: 89.851.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 59.362.. \n",
      "\u001B[93mTest Loss: 83.164.. \n",
      "\u001B[32mTest loss decreased (87.386955 --> 83.164444).  Saving model ...\n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 59.992.. \n",
      "\u001B[93mTest Loss: 86.347.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 58.697.. \n",
      "\u001B[93mTest Loss: 89.228.. \n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 58.160.. \n",
      "\u001B[93mTest Loss: 83.846.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 56.783.. \n",
      "\u001B[93mTest Loss: 79.865.. \n",
      "\u001B[32mTest loss decreased (83.164444 --> 79.865028).  Saving model ...\n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 56.844.. \n",
      "\u001B[93mTest Loss: 87.120.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 55.442.. \n",
      "\u001B[93mTest Loss: 82.027.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 55.893.. \n",
      "\u001B[93mTest Loss: 85.754.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 53.554.. \n",
      "\u001B[93mTest Loss: 83.525.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 53.934.. \n",
      "\u001B[93mTest Loss: 78.862.. \n",
      "\u001B[32mTest loss decreased (79.865028 --> 78.861511).  Saving model ...\n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 54.903.. \n",
      "\u001B[93mTest Loss: 99.616.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 53.792.. \n",
      "\u001B[93mTest Loss: 77.494.. \n",
      "\u001B[32mTest loss decreased (78.861511 --> 77.494041).  Saving model ...\n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 52.188.. \n",
      "\u001B[93mTest Loss: 79.070.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 50.824.. \n",
      "\u001B[93mTest Loss: 80.070.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 52.342.. \n",
      "\u001B[93mTest Loss: 81.412.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 53.221.. \n",
      "\u001B[93mTest Loss: 79.725.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 51.363.. \n",
      "\u001B[93mTest Loss: 76.423.. \n",
      "\u001B[32mTest loss decreased (77.494041 --> 76.422737).  Saving model ...\n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 50.237.. \n",
      "\u001B[93mTest Loss: 76.236.. \n",
      "\u001B[32mTest loss decreased (76.422737 --> 76.235878).  Saving model ...\n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 50.194.. \n",
      "\u001B[93mTest Loss: 78.813.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 52.757.. \n",
      "\u001B[93mTest Loss: 75.950.. \n",
      "\u001B[32mTest loss decreased (76.235878 --> 75.949547).  Saving model ...\n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 49.080.. \n",
      "\u001B[93mTest Loss: 75.983.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 48.678.. \n",
      "\u001B[93mTest Loss: 86.432.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 52.333.. \n",
      "\u001B[93mTest Loss: 73.240.. \n",
      "\u001B[32mTest loss decreased (75.949547 --> 73.240295).  Saving model ...\n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 48.871.. \n",
      "\u001B[93mTest Loss: 77.632.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 47.439.. \n",
      "\u001B[93mTest Loss: 79.828.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 45.723.. \n",
      "\u001B[93mTest Loss: 79.701.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 46.653.. \n",
      "\u001B[93mTest Loss: 74.655.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 48.554.. \n",
      "\u001B[93mTest Loss: 73.641.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 44.581.. \n",
      "\u001B[93mTest Loss: 71.803.. \n",
      "\u001B[32mTest loss decreased (73.240295 --> 71.802528).  Saving model ...\n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 43.650.. \n",
      "\u001B[93mTest Loss: 74.969.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 44.707.. \n",
      "\u001B[93mTest Loss: 76.918.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 43.968.. \n",
      "\u001B[93mTest Loss: 70.807.. \n",
      "\u001B[32mTest loss decreased (71.802528 --> 70.807243).  Saving model ...\n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 44.010.. \n",
      "\u001B[93mTest Loss: 71.078.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 42.914.. \n",
      "\u001B[93mTest Loss: 72.767.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 42.329.. \n",
      "\u001B[93mTest Loss: 70.010.. \n",
      "\u001B[32mTest loss decreased (70.807243 --> 70.010056).  Saving model ...\n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 43.986.. \n",
      "\u001B[93mTest Loss: 72.551.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 42.395.. \n",
      "\u001B[93mTest Loss: 74.387.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 41.805.. \n",
      "\u001B[93mTest Loss: 74.316.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 42.264.. \n",
      "\u001B[93mTest Loss: 69.870.. \n",
      "\u001B[32mTest loss decreased (70.010056 --> 69.870270).  Saving model ...\n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 40.218.. \n",
      "\u001B[93mTest Loss: 76.766.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 44.287.. \n",
      "\u001B[93mTest Loss: 71.940.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 41.202.. \n",
      "\u001B[93mTest Loss: 71.978.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 42.034.. \n",
      "\u001B[93mTest Loss: 71.990.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 39.882.. \n",
      "\u001B[93mTest Loss: 70.876.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 40.084.. \n",
      "\u001B[93mTest Loss: 72.708.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 42.689.. \n",
      "\u001B[93mTest Loss: 72.238.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 39.399.. \n",
      "\u001B[93mTest Loss: 71.354.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 41.781.. \n",
      "\u001B[93mTest Loss: 76.893.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 42.315.. \n",
      "\u001B[93mTest Loss: 68.213.. \n",
      "\u001B[32mTest loss decreased (69.870270 --> 68.213463).  Saving model ...\n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 40.881.. \n",
      "\u001B[93mTest Loss: 68.822.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 38.623.. \n",
      "\u001B[93mTest Loss: 69.598.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 39.588.. \n",
      "\u001B[93mTest Loss: 70.867.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 38.654.. \n",
      "\u001B[93mTest Loss: 69.854.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 39.636.. \n",
      "\u001B[93mTest Loss: 69.085.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 41.073.. \n",
      "\u001B[93mTest Loss: 74.653.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 39.021.. \n",
      "\u001B[93mTest Loss: 68.625.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 38.454.. \n",
      "\u001B[93mTest Loss: 72.534.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 36.955.. \n",
      "\u001B[93mTest Loss: 72.381.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 37.884.. \n",
      "\u001B[93mTest Loss: 73.674.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 39.986.. \n",
      "\u001B[93mTest Loss: 70.092.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 38.874.. \n",
      "\u001B[93mTest Loss: 71.979.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 37.068.. \n",
      "\u001B[93mTest Loss: 70.661.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 37.614.. \n",
      "\u001B[93mTest Loss: 71.965.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 36.461.. \n",
      "\u001B[93mTest Loss: 72.422.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 35.820.. \n",
      "\u001B[93mTest Loss: 71.091.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 36.666.. \n",
      "\u001B[93mTest Loss: 77.093.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 36.521.. \n",
      "\u001B[93mTest Loss: 88.216.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 39.627.. \n",
      "\u001B[93mTest Loss: 69.065.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 35.032.. \n",
      "\u001B[93mTest Loss: 71.757.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 35.769.. \n",
      "\u001B[93mTest Loss: 73.978.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 37.478.. \n",
      "\u001B[93mTest Loss: 85.304.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 37.308.. \n",
      "\u001B[93mTest Loss: 69.860.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 34.035.. \n",
      "\u001B[93mTest Loss: 69.654.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 34.614.. \n",
      "\u001B[93mTest Loss: 69.217.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 33.586.. \n",
      "\u001B[93mTest Loss: 67.403.. \n",
      "\u001B[32mTest loss decreased (68.213463 --> 67.403008).  Saving model ...\n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 36.890.. \n",
      "\u001B[93mTest Loss: 74.382.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 35.690.. \n",
      "\u001B[93mTest Loss: 71.148.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 37.397.. \n",
      "\u001B[93mTest Loss: 69.370.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 34.072.. \n",
      "\u001B[93mTest Loss: 71.548.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 33.594.. \n",
      "\u001B[93mTest Loss: 68.739.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 35.373.. \n",
      "\u001B[93mTest Loss: 91.270.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 41.101.. \n",
      "\u001B[93mTest Loss: 70.698.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 34.771.. \n",
      "\u001B[93mTest Loss: 73.410.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 32.710.. \n",
      "\u001B[93mTest Loss: 72.626.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 35.149.. \n",
      "\u001B[93mTest Loss: 69.588.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 32.991.. \n",
      "\u001B[93mTest Loss: 71.572.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 34.521.. \n",
      "\u001B[93mTest Loss: 70.852.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 34.385.. \n",
      "\u001B[93mTest Loss: 71.218.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 32.671.. \n",
      "\u001B[93mTest Loss: 73.684.. \n",
      "\u001B[36mEpoch: 115/200..  Training Loss: 35.975.. \n",
      "\u001B[93mTest Loss: 74.429.. \n",
      "\u001B[36mEpoch: 116/200..  Training Loss: 37.680.. \n",
      "\u001B[93mTest Loss: 77.163.. \n",
      "\u001B[36mEpoch: 117/200..  Training Loss: 37.650.. \n",
      "\u001B[93mTest Loss: 70.722.. \n",
      "\u001B[36mEpoch: 118/200..  Training Loss: 31.229.. \n",
      "\u001B[93mTest Loss: 69.764.. \n",
      "\u001B[36mEpoch: 119/200..  Training Loss: 32.398.. \n",
      "\u001B[93mTest Loss: 68.024.. \n",
      "\u001B[36mEpoch: 120/200..  Training Loss: 31.162.. \n",
      "\u001B[93mTest Loss: 70.210.. \n",
      "\u001B[36mEpoch: 121/200..  Training Loss: 32.182.. \n",
      "\u001B[93mTest Loss: 74.167.. \n",
      "\u001B[36mEpoch: 122/200..  Training Loss: 32.619.. \n",
      "\u001B[93mTest Loss: 69.818.. \n",
      "\u001B[36mEpoch: 123/200..  Training Loss: 34.894.. \n",
      "\u001B[93mTest Loss: 73.732.. \n",
      "\u001B[36mEpoch: 124/200..  Training Loss: 35.317.. \n",
      "\u001B[93mTest Loss: 70.985.. \n",
      "\u001B[36mEpoch: 125/200..  Training Loss: 35.611.. \n",
      "\u001B[93mTest Loss: 73.752.. \n",
      "\u001B[36mEpoch: 126/200..  Training Loss: 32.428.. \n",
      "\u001B[93mTest Loss: 73.608.. \n",
      "\u001B[36mEpoch: 127/200..  Training Loss: 31.115.. \n",
      "\u001B[93mTest Loss: 83.275.. \n",
      "\u001B[36mEpoch: 128/200..  Training Loss: 32.976.. \n",
      "\u001B[93mTest Loss: 72.146.. \n",
      "\u001B[36mEpoch: 129/200..  Training Loss: 33.477.. \n",
      "\u001B[93mTest Loss: 73.748.. \n",
      "\u001B[36mEpoch: 130/200..  Training Loss: 31.292.. \n",
      "\u001B[93mTest Loss: 72.928.. \n",
      "\u001B[36mEpoch: 131/200..  Training Loss: 30.246.. \n",
      "\u001B[93mTest Loss: 70.713.. \n",
      "\u001B[36mEpoch: 132/200..  Training Loss: 38.820.. \n",
      "\u001B[93mTest Loss: 77.752.. \n",
      "\u001B[36mEpoch: 133/200..  Training Loss: 34.680.. \n",
      "\u001B[93mTest Loss: 74.108.. \n",
      "\u001B[36mEpoch: 134/200..  Training Loss: 30.388.. \n",
      "\u001B[93mTest Loss: 68.625.. \n",
      "\u001B[36mEpoch: 135/200..  Training Loss: 30.202.. \n",
      "\u001B[93mTest Loss: 71.531.. \n",
      "\u001B[36mEpoch: 136/200..  Training Loss: 29.936.. \n",
      "\u001B[93mTest Loss: 71.771.. \n",
      "\u001B[36mEpoch: 137/200..  Training Loss: 40.083.. \n",
      "\u001B[93mTest Loss: 72.689.. \n",
      "\u001B[36mEpoch: 138/200..  Training Loss: 31.114.. \n",
      "\u001B[93mTest Loss: 71.353.. \n",
      "\u001B[36mEpoch: 139/200..  Training Loss: 28.561.. \n",
      "\u001B[93mTest Loss: 71.586.. \n",
      "\u001B[36mEpoch: 140/200..  Training Loss: 28.348.. \n",
      "\u001B[93mTest Loss: 71.464.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  239.1946244239807 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 101.677.. \n",
      "\u001B[93mTest Loss: 107.192.. \n",
      "\u001B[32mTest loss decreased (inf --> 107.192123).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 89.851.. \n",
      "\u001B[93mTest Loss: 104.982.. \n",
      "\u001B[32mTest loss decreased (107.192123 --> 104.982079).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 86.432.. \n",
      "\u001B[93mTest Loss: 98.764.. \n",
      "\u001B[32mTest loss decreased (104.982079 --> 98.764214).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 83.467.. \n",
      "\u001B[93mTest Loss: 99.526.. \n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 81.988.. \n",
      "\u001B[93mTest Loss: 97.164.. \n",
      "\u001B[32mTest loss decreased (98.764214 --> 97.163605).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 80.543.. \n",
      "\u001B[93mTest Loss: 99.033.. \n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 79.937.. \n",
      "\u001B[93mTest Loss: 100.002.. \n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 78.240.. \n",
      "\u001B[93mTest Loss: 96.214.. \n",
      "\u001B[32mTest loss decreased (97.163605 --> 96.213654).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 76.348.. \n",
      "\u001B[93mTest Loss: 100.260.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 75.629.. \n",
      "\u001B[93mTest Loss: 92.995.. \n",
      "\u001B[32mTest loss decreased (96.213654 --> 92.994667).  Saving model ...\n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 74.297.. \n",
      "\u001B[93mTest Loss: 94.586.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 72.727.. \n",
      "\u001B[93mTest Loss: 96.730.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 72.573.. \n",
      "\u001B[93mTest Loss: 92.987.. \n",
      "\u001B[32mTest loss decreased (92.994667 --> 92.987389).  Saving model ...\n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 70.992.. \n",
      "\u001B[93mTest Loss: 92.664.. \n",
      "\u001B[32mTest loss decreased (92.987389 --> 92.663559).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 68.589.. \n",
      "\u001B[93mTest Loss: 92.405.. \n",
      "\u001B[32mTest loss decreased (92.663559 --> 92.404625).  Saving model ...\n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 67.970.. \n",
      "\u001B[93mTest Loss: 91.543.. \n",
      "\u001B[32mTest loss decreased (92.404625 --> 91.542511).  Saving model ...\n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 66.283.. \n",
      "\u001B[93mTest Loss: 94.696.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 64.909.. \n",
      "\u001B[93mTest Loss: 92.058.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 62.922.. \n",
      "\u001B[93mTest Loss: 90.727.. \n",
      "\u001B[32mTest loss decreased (91.542511 --> 90.726593).  Saving model ...\n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 62.161.. \n",
      "\u001B[93mTest Loss: 92.082.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 61.444.. \n",
      "\u001B[93mTest Loss: 90.757.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 59.934.. \n",
      "\u001B[93mTest Loss: 90.102.. \n",
      "\u001B[32mTest loss decreased (90.726593 --> 90.101555).  Saving model ...\n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 57.710.. \n",
      "\u001B[93mTest Loss: 90.366.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 55.928.. \n",
      "\u001B[93mTest Loss: 90.630.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 56.518.. \n",
      "\u001B[93mTest Loss: 86.477.. \n",
      "\u001B[32mTest loss decreased (90.101555 --> 86.477173).  Saving model ...\n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 53.397.. \n",
      "\u001B[93mTest Loss: 88.702.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 54.444.. \n",
      "\u001B[93mTest Loss: 88.364.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 49.371.. \n",
      "\u001B[93mTest Loss: 82.771.. \n",
      "\u001B[32mTest loss decreased (86.477173 --> 82.770615).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 48.102.. \n",
      "\u001B[93mTest Loss: 83.700.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 48.738.. \n",
      "\u001B[93mTest Loss: 81.790.. \n",
      "\u001B[32mTest loss decreased (82.770615 --> 81.789993).  Saving model ...\n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 46.619.. \n",
      "\u001B[93mTest Loss: 81.870.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 44.208.. \n",
      "\u001B[93mTest Loss: 82.228.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 41.629.. \n",
      "\u001B[93mTest Loss: 88.000.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 42.311.. \n",
      "\u001B[93mTest Loss: 79.999.. \n",
      "\u001B[32mTest loss decreased (81.789993 --> 79.999466).  Saving model ...\n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 41.503.. \n",
      "\u001B[93mTest Loss: 76.990.. \n",
      "\u001B[32mTest loss decreased (79.999466 --> 76.989998).  Saving model ...\n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 41.457.. \n",
      "\u001B[93mTest Loss: 79.485.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 37.704.. \n",
      "\u001B[93mTest Loss: 80.052.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 36.731.. \n",
      "\u001B[93mTest Loss: 78.167.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 36.404.. \n",
      "\u001B[93mTest Loss: 78.588.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 38.101.. \n",
      "\u001B[93mTest Loss: 81.053.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 35.459.. \n",
      "\u001B[93mTest Loss: 75.711.. \n",
      "\u001B[32mTest loss decreased (76.989998 --> 75.710564).  Saving model ...\n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 33.250.. \n",
      "\u001B[93mTest Loss: 78.057.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 34.447.. \n",
      "\u001B[93mTest Loss: 77.590.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 31.949.. \n",
      "\u001B[93mTest Loss: 76.511.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 34.235.. \n",
      "\u001B[93mTest Loss: 79.867.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 30.865.. \n",
      "\u001B[93mTest Loss: 77.893.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 31.954.. \n",
      "\u001B[93mTest Loss: 73.569.. \n",
      "\u001B[32mTest loss decreased (75.710564 --> 73.569435).  Saving model ...\n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 30.239.. \n",
      "\u001B[93mTest Loss: 77.909.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 32.761.. \n",
      "\u001B[93mTest Loss: 77.350.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 29.295.. \n",
      "\u001B[93mTest Loss: 76.834.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 29.079.. \n",
      "\u001B[93mTest Loss: 86.638.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 32.538.. \n",
      "\u001B[93mTest Loss: 75.334.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 27.594.. \n",
      "\u001B[93mTest Loss: 75.542.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 27.514.. \n",
      "\u001B[93mTest Loss: 75.542.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 28.149.. \n",
      "\u001B[93mTest Loss: 74.559.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 31.013.. \n",
      "\u001B[93mTest Loss: 79.966.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 26.884.. \n",
      "\u001B[93mTest Loss: 79.114.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 28.726.. \n",
      "\u001B[93mTest Loss: 74.566.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 25.255.. \n",
      "\u001B[93mTest Loss: 74.101.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 25.498.. \n",
      "\u001B[93mTest Loss: 73.715.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 26.321.. \n",
      "\u001B[93mTest Loss: 75.389.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 26.367.. \n",
      "\u001B[93mTest Loss: 74.135.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 24.262.. \n",
      "\u001B[93mTest Loss: 78.088.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 27.408.. \n",
      "\u001B[93mTest Loss: 78.691.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 22.903.. \n",
      "\u001B[93mTest Loss: 75.046.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 24.573.. \n",
      "\u001B[93mTest Loss: 73.523.. \n",
      "\u001B[32mTest loss decreased (73.569435 --> 73.522789).  Saving model ...\n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 25.411.. \n",
      "\u001B[93mTest Loss: 74.385.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 22.122.. \n",
      "\u001B[93mTest Loss: 74.201.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 25.211.. \n",
      "\u001B[93mTest Loss: 73.127.. \n",
      "\u001B[32mTest loss decreased (73.522789 --> 73.126526).  Saving model ...\n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 24.294.. \n",
      "\u001B[93mTest Loss: 76.969.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 21.678.. \n",
      "\u001B[93mTest Loss: 73.502.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 21.883.. \n",
      "\u001B[93mTest Loss: 74.505.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 21.477.. \n",
      "\u001B[93mTest Loss: 74.620.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 23.231.. \n",
      "\u001B[93mTest Loss: 74.446.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 20.230.. \n",
      "\u001B[93mTest Loss: 73.556.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 25.583.. \n",
      "\u001B[93mTest Loss: 75.194.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 20.076.. \n",
      "\u001B[93mTest Loss: 74.158.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 19.734.. \n",
      "\u001B[93mTest Loss: 72.840.. \n",
      "\u001B[32mTest loss decreased (73.126526 --> 72.839851).  Saving model ...\n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 22.880.. \n",
      "\u001B[93mTest Loss: 76.178.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 20.239.. \n",
      "\u001B[93mTest Loss: 75.919.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 20.473.. \n",
      "\u001B[93mTest Loss: 74.173.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 18.766.. \n",
      "\u001B[93mTest Loss: 76.217.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 21.510.. \n",
      "\u001B[93mTest Loss: 74.868.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 19.165.. \n",
      "\u001B[93mTest Loss: 77.736.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 16.911.. \n",
      "\u001B[93mTest Loss: 73.694.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 20.391.. \n",
      "\u001B[93mTest Loss: 80.825.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 18.672.. \n",
      "\u001B[93mTest Loss: 77.426.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 16.908.. \n",
      "\u001B[93mTest Loss: 73.632.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 15.884.. \n",
      "\u001B[93mTest Loss: 74.754.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 21.011.. \n",
      "\u001B[93mTest Loss: 74.978.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 19.375.. \n",
      "\u001B[93mTest Loss: 75.947.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 18.636.. \n",
      "\u001B[93mTest Loss: 75.793.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 20.158.. \n",
      "\u001B[93mTest Loss: 73.847.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 17.357.. \n",
      "\u001B[93mTest Loss: 74.590.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 15.542.. \n",
      "\u001B[93mTest Loss: 74.948.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 18.736.. \n",
      "\u001B[93mTest Loss: 76.610.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 15.983.. \n",
      "\u001B[93mTest Loss: 74.061.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 14.766.. \n",
      "\u001B[93mTest Loss: 72.973.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 14.745.. \n",
      "\u001B[93mTest Loss: 77.187.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 16.008.. \n",
      "\u001B[93mTest Loss: 79.268.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 17.107.. \n",
      "\u001B[93mTest Loss: 74.542.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 15.188.. \n",
      "\u001B[93mTest Loss: 75.174.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 16.735.. \n",
      "\u001B[93mTest Loss: 92.013.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 20.141.. \n",
      "\u001B[93mTest Loss: 78.967.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 16.524.. \n",
      "\u001B[93mTest Loss: 74.315.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 14.062.. \n",
      "\u001B[93mTest Loss: 72.940.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 14.711.. \n",
      "\u001B[93mTest Loss: 76.689.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 15.214.. \n",
      "\u001B[93mTest Loss: 75.659.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 13.159.. \n",
      "\u001B[93mTest Loss: 75.575.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 13.686.. \n",
      "\u001B[93mTest Loss: 74.263.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 12.804.. \n",
      "\u001B[93mTest Loss: 73.456.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 13.080.. \n",
      "\u001B[93mTest Loss: 78.247.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 18.241.. \n",
      "\u001B[93mTest Loss: 74.097.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 13.329.. \n",
      "\u001B[93mTest Loss: 71.580.. \n",
      "\u001B[32mTest loss decreased (72.839851 --> 71.579926).  Saving model ...\n",
      "\u001B[36mEpoch: 115/200..  Training Loss: 15.199.. \n",
      "\u001B[93mTest Loss: 77.105.. \n",
      "\u001B[36mEpoch: 116/200..  Training Loss: 17.443.. \n",
      "\u001B[93mTest Loss: 71.801.. \n",
      "\u001B[36mEpoch: 117/200..  Training Loss: 14.025.. \n",
      "\u001B[93mTest Loss: 71.871.. \n",
      "\u001B[36mEpoch: 118/200..  Training Loss: 13.598.. \n",
      "\u001B[93mTest Loss: 75.551.. \n",
      "\u001B[36mEpoch: 119/200..  Training Loss: 12.162.. \n",
      "\u001B[93mTest Loss: 74.610.. \n",
      "\u001B[36mEpoch: 120/200..  Training Loss: 11.702.. \n",
      "\u001B[93mTest Loss: 73.715.. \n",
      "\u001B[36mEpoch: 121/200..  Training Loss: 11.868.. \n",
      "\u001B[93mTest Loss: 75.605.. \n",
      "\u001B[36mEpoch: 122/200..  Training Loss: 12.091.. \n",
      "\u001B[93mTest Loss: 83.152.. \n",
      "\u001B[36mEpoch: 123/200..  Training Loss: 18.548.. \n",
      "\u001B[93mTest Loss: 75.589.. \n",
      "\u001B[36mEpoch: 124/200..  Training Loss: 19.521.. \n",
      "\u001B[93mTest Loss: 75.356.. \n",
      "\u001B[36mEpoch: 125/200..  Training Loss: 12.788.. \n",
      "\u001B[93mTest Loss: 74.575.. \n",
      "\u001B[36mEpoch: 126/200..  Training Loss: 12.048.. \n",
      "\u001B[93mTest Loss: 73.195.. \n",
      "\u001B[36mEpoch: 127/200..  Training Loss: 18.167.. \n",
      "\u001B[93mTest Loss: 85.552.. \n",
      "\u001B[36mEpoch: 128/200..  Training Loss: 14.816.. \n",
      "\u001B[93mTest Loss: 77.145.. \n",
      "\u001B[36mEpoch: 129/200..  Training Loss: 12.315.. \n",
      "\u001B[93mTest Loss: 74.813.. \n",
      "\u001B[36mEpoch: 130/200..  Training Loss: 16.220.. \n",
      "\u001B[93mTest Loss: 80.536.. \n",
      "\u001B[36mEpoch: 131/200..  Training Loss: 17.484.. \n",
      "\u001B[93mTest Loss: 72.801.. \n",
      "\u001B[36mEpoch: 132/200..  Training Loss: 12.636.. \n",
      "\u001B[93mTest Loss: 72.606.. \n",
      "\u001B[36mEpoch: 133/200..  Training Loss: 11.545.. \n",
      "\u001B[93mTest Loss: 73.696.. \n",
      "\u001B[36mEpoch: 134/200..  Training Loss: 10.334.. \n",
      "\u001B[93mTest Loss: 74.456.. \n",
      "\u001B[36mEpoch: 135/200..  Training Loss: 10.613.. \n",
      "\u001B[93mTest Loss: 74.514.. \n",
      "\u001B[36mEpoch: 136/200..  Training Loss: 10.045.. \n",
      "\u001B[93mTest Loss: 75.209.. \n",
      "\u001B[36mEpoch: 137/200..  Training Loss: 9.950.. \n",
      "\u001B[93mTest Loss: 73.796.. \n",
      "\u001B[36mEpoch: 138/200..  Training Loss: 10.404.. \n",
      "\u001B[93mTest Loss: 74.581.. \n",
      "\u001B[36mEpoch: 139/200..  Training Loss: 18.531.. \n",
      "\u001B[93mTest Loss: 78.186.. \n",
      "\u001B[36mEpoch: 140/200..  Training Loss: 14.933.. \n",
      "\u001B[93mTest Loss: 75.322.. \n",
      "\u001B[36mEpoch: 141/200..  Training Loss: 10.709.. \n",
      "\u001B[93mTest Loss: 74.811.. \n",
      "\u001B[36mEpoch: 142/200..  Training Loss: 9.896.. \n",
      "\u001B[93mTest Loss: 74.249.. \n",
      "\u001B[36mEpoch: 143/200..  Training Loss: 9.763.. \n",
      "\u001B[93mTest Loss: 79.283.. \n",
      "\u001B[36mEpoch: 144/200..  Training Loss: 12.649.. \n",
      "\u001B[93mTest Loss: 75.308.. \n",
      "\u001B[36mEpoch: 145/200..  Training Loss: 11.372.. \n",
      "\u001B[93mTest Loss: 75.237.. \n",
      "\u001B[36mEpoch: 146/200..  Training Loss: 12.856.. \n",
      "\u001B[93mTest Loss: 78.799.. \n",
      "\u001B[36mEpoch: 147/200..  Training Loss: 15.889.. \n",
      "\u001B[93mTest Loss: 76.997.. \n",
      "\u001B[36mEpoch: 148/200..  Training Loss: 12.221.. \n",
      "\u001B[93mTest Loss: 79.165.. \n",
      "\u001B[36mEpoch: 149/200..  Training Loss: 11.237.. \n",
      "\u001B[93mTest Loss: 74.505.. \n",
      "\u001B[36mEpoch: 150/200..  Training Loss: 10.035.. \n",
      "\u001B[93mTest Loss: 74.575.. \n",
      "\u001B[36mEpoch: 151/200..  Training Loss: 8.635.. \n",
      "\u001B[93mTest Loss: 78.161.. \n",
      "\u001B[36mEpoch: 152/200..  Training Loss: 9.813.. \n",
      "\u001B[93mTest Loss: 88.390.. \n",
      "\u001B[36mEpoch: 153/200..  Training Loss: 13.972.. \n",
      "\u001B[93mTest Loss: 75.928.. \n",
      "\u001B[36mEpoch: 154/200..  Training Loss: 10.868.. \n",
      "\u001B[93mTest Loss: 75.438.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  244.32819533348083 seconds\n",
      "\u001B[32mLowest Test loss achieved for Standard Net: 69.440842\n",
      "\u001B[32mAt Epoch #70 \n",
      "\n",
      "\u001B[32mLowest Elastic-Net loss achieved: 67.403008\n",
      "\u001B[32mAt Epoch #99 \n",
      "\n",
      "\u001B[32mLowest Test loss achieved for Gradient-Net: 71.579926\n",
      "\u001B[32mAt Epoch #113 \n",
      "\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 100.114.. \n",
      "\u001B[93mTest Loss: 107.470.. \n",
      "\u001B[32mTest loss decreased (inf --> 107.469826).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 85.560.. \n",
      "\u001B[93mTest Loss: 100.613.. \n",
      "\u001B[32mTest loss decreased (107.469826 --> 100.613029).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 82.975.. \n",
      "\u001B[93mTest Loss: 100.921.. \n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 81.095.. \n",
      "\u001B[93mTest Loss: 99.792.. \n",
      "\u001B[32mTest loss decreased (100.613029 --> 99.791634).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 79.039.. \n",
      "\u001B[93mTest Loss: 95.124.. \n",
      "\u001B[32mTest loss decreased (99.791634 --> 95.124367).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 76.872.. \n",
      "\u001B[93mTest Loss: 94.327.. \n",
      "\u001B[32mTest loss decreased (95.124367 --> 94.326614).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 76.101.. \n",
      "\u001B[93mTest Loss: 94.157.. \n",
      "\u001B[32mTest loss decreased (94.326614 --> 94.157288).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 73.725.. \n",
      "\u001B[93mTest Loss: 94.010.. \n",
      "\u001B[32mTest loss decreased (94.157288 --> 94.009705).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 73.277.. \n",
      "\u001B[93mTest Loss: 91.857.. \n",
      "\u001B[32mTest loss decreased (94.009705 --> 91.857048).  Saving model ...\n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 72.096.. \n",
      "\u001B[93mTest Loss: 94.524.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 71.060.. \n",
      "\u001B[93mTest Loss: 91.662.. \n",
      "\u001B[32mTest loss decreased (91.857048 --> 91.661789).  Saving model ...\n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 70.178.. \n",
      "\u001B[93mTest Loss: 94.937.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 68.389.. \n",
      "\u001B[93mTest Loss: 91.290.. \n",
      "\u001B[32mTest loss decreased (91.661789 --> 91.290291).  Saving model ...\n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 67.592.. \n",
      "\u001B[93mTest Loss: 92.429.. \n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 66.144.. \n",
      "\u001B[93mTest Loss: 89.666.. \n",
      "\u001B[32mTest loss decreased (91.290291 --> 89.665833).  Saving model ...\n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 63.707.. \n",
      "\u001B[93mTest Loss: 92.504.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 62.541.. \n",
      "\u001B[93mTest Loss: 94.426.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 60.985.. \n",
      "\u001B[93mTest Loss: 92.113.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 59.409.. \n",
      "\u001B[93mTest Loss: 90.970.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 58.267.. \n",
      "\u001B[93mTest Loss: 93.631.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 57.750.. \n",
      "\u001B[93mTest Loss: 90.701.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 56.832.. \n",
      "\u001B[93mTest Loss: 86.559.. \n",
      "\u001B[32mTest loss decreased (89.665833 --> 86.558975).  Saving model ...\n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 53.635.. \n",
      "\u001B[93mTest Loss: 93.133.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 52.585.. \n",
      "\u001B[93mTest Loss: 85.843.. \n",
      "\u001B[32mTest loss decreased (86.558975 --> 85.843201).  Saving model ...\n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 49.631.. \n",
      "\u001B[93mTest Loss: 85.582.. \n",
      "\u001B[32mTest loss decreased (85.843201 --> 85.582207).  Saving model ...\n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 49.063.. \n",
      "\u001B[93mTest Loss: 86.147.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 47.325.. \n",
      "\u001B[93mTest Loss: 81.233.. \n",
      "\u001B[32mTest loss decreased (85.582207 --> 81.232742).  Saving model ...\n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 45.355.. \n",
      "\u001B[93mTest Loss: 80.457.. \n",
      "\u001B[32mTest loss decreased (81.232742 --> 80.456886).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 44.080.. \n",
      "\u001B[93mTest Loss: 80.629.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 41.718.. \n",
      "\u001B[93mTest Loss: 84.836.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 42.176.. \n",
      "\u001B[93mTest Loss: 82.301.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 39.028.. \n",
      "\u001B[93mTest Loss: 78.925.. \n",
      "\u001B[32mTest loss decreased (80.456886 --> 78.925499).  Saving model ...\n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 41.036.. \n",
      "\u001B[93mTest Loss: 80.868.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 36.222.. \n",
      "\u001B[93mTest Loss: 76.141.. \n",
      "\u001B[32mTest loss decreased (78.925499 --> 76.140732).  Saving model ...\n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 36.061.. \n",
      "\u001B[93mTest Loss: 75.899.. \n",
      "\u001B[32mTest loss decreased (76.140732 --> 75.899376).  Saving model ...\n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 34.360.. \n",
      "\u001B[93mTest Loss: 76.404.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 32.074.. \n",
      "\u001B[93mTest Loss: 74.708.. \n",
      "\u001B[32mTest loss decreased (75.899376 --> 74.707909).  Saving model ...\n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 32.184.. \n",
      "\u001B[93mTest Loss: 75.869.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 34.239.. \n",
      "\u001B[93mTest Loss: 81.850.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 31.830.. \n",
      "\u001B[93mTest Loss: 75.927.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 29.651.. \n",
      "\u001B[93mTest Loss: 71.164.. \n",
      "\u001B[32mTest loss decreased (74.707909 --> 71.163841).  Saving model ...\n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 29.406.. \n",
      "\u001B[93mTest Loss: 74.205.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 31.386.. \n",
      "\u001B[93mTest Loss: 75.382.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 29.139.. \n",
      "\u001B[93mTest Loss: 74.286.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 29.106.. \n",
      "\u001B[93mTest Loss: 76.296.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 27.268.. \n",
      "\u001B[93mTest Loss: 75.107.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 26.749.. \n",
      "\u001B[93mTest Loss: 71.650.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 26.487.. \n",
      "\u001B[93mTest Loss: 73.883.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 26.571.. \n",
      "\u001B[93mTest Loss: 72.974.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 23.601.. \n",
      "\u001B[93mTest Loss: 70.471.. \n",
      "\u001B[32mTest loss decreased (71.163841 --> 70.471146).  Saving model ...\n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 24.373.. \n",
      "\u001B[93mTest Loss: 75.342.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 24.773.. \n",
      "\u001B[93mTest Loss: 72.489.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 26.822.. \n",
      "\u001B[93mTest Loss: 75.875.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 24.193.. \n",
      "\u001B[93mTest Loss: 69.909.. \n",
      "\u001B[32mTest loss decreased (70.471146 --> 69.909386).  Saving model ...\n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 23.462.. \n",
      "\u001B[93mTest Loss: 72.197.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 23.314.. \n",
      "\u001B[93mTest Loss: 70.980.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 20.751.. \n",
      "\u001B[93mTest Loss: 70.928.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 26.258.. \n",
      "\u001B[93mTest Loss: 74.717.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 21.849.. \n",
      "\u001B[93mTest Loss: 71.745.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 20.662.. \n",
      "\u001B[93mTest Loss: 72.006.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 20.032.. \n",
      "\u001B[93mTest Loss: 71.143.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 18.814.. \n",
      "\u001B[93mTest Loss: 70.975.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 19.647.. \n",
      "\u001B[93mTest Loss: 85.035.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 22.140.. \n",
      "\u001B[93mTest Loss: 71.562.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 20.847.. \n",
      "\u001B[93mTest Loss: 76.650.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 19.368.. \n",
      "\u001B[93mTest Loss: 70.523.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 18.886.. \n",
      "\u001B[93mTest Loss: 75.099.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 18.512.. \n",
      "\u001B[93mTest Loss: 70.348.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 17.600.. \n",
      "\u001B[93mTest Loss: 70.327.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 17.701.. \n",
      "\u001B[93mTest Loss: 70.064.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 18.442.. \n",
      "\u001B[93mTest Loss: 70.164.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 23.864.. \n",
      "\u001B[93mTest Loss: 71.775.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 17.916.. \n",
      "\u001B[93mTest Loss: 69.008.. \n",
      "\u001B[32mTest loss decreased (69.909386 --> 69.007858).  Saving model ...\n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 15.051.. \n",
      "\u001B[93mTest Loss: 70.142.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 15.661.. \n",
      "\u001B[93mTest Loss: 68.761.. \n",
      "\u001B[32mTest loss decreased (69.007858 --> 68.761391).  Saving model ...\n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 21.563.. \n",
      "\u001B[93mTest Loss: 74.976.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 17.228.. \n",
      "\u001B[93mTest Loss: 68.742.. \n",
      "\u001B[32mTest loss decreased (68.761391 --> 68.742073).  Saving model ...\n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 15.594.. \n",
      "\u001B[93mTest Loss: 69.576.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 15.786.. \n",
      "\u001B[93mTest Loss: 73.562.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 17.989.. \n",
      "\u001B[93mTest Loss: 71.217.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 14.760.. \n",
      "\u001B[93mTest Loss: 68.627.. \n",
      "\u001B[32mTest loss decreased (68.742073 --> 68.626518).  Saving model ...\n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 13.997.. \n",
      "\u001B[93mTest Loss: 70.608.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 13.936.. \n",
      "\u001B[93mTest Loss: 68.014.. \n",
      "\u001B[32mTest loss decreased (68.626518 --> 68.013680).  Saving model ...\n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 13.618.. \n",
      "\u001B[93mTest Loss: 73.692.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 15.035.. \n",
      "\u001B[93mTest Loss: 69.472.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 15.805.. \n",
      "\u001B[93mTest Loss: 70.173.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 13.527.. \n",
      "\u001B[93mTest Loss: 69.248.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 13.718.. \n",
      "\u001B[93mTest Loss: 69.935.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 14.996.. \n",
      "\u001B[93mTest Loss: 71.983.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 14.735.. \n",
      "\u001B[93mTest Loss: 69.179.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 13.189.. \n",
      "\u001B[93mTest Loss: 70.374.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 14.977.. \n",
      "\u001B[93mTest Loss: 70.609.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 12.998.. \n",
      "\u001B[93mTest Loss: 72.578.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 11.967.. \n",
      "\u001B[93mTest Loss: 79.634.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 13.438.. \n",
      "\u001B[93mTest Loss: 69.722.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 12.717.. \n",
      "\u001B[93mTest Loss: 69.237.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 18.969.. \n",
      "\u001B[93mTest Loss: 81.619.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 16.819.. \n",
      "\u001B[93mTest Loss: 70.966.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 11.359.. \n",
      "\u001B[93mTest Loss: 69.273.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 10.372.. \n",
      "\u001B[93mTest Loss: 69.821.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 10.454.. \n",
      "\u001B[93mTest Loss: 70.260.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 15.823.. \n",
      "\u001B[93mTest Loss: 70.421.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 12.302.. \n",
      "\u001B[93mTest Loss: 68.862.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 10.407.. \n",
      "\u001B[93mTest Loss: 70.096.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 10.340.. \n",
      "\u001B[93mTest Loss: 71.936.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 11.184.. \n",
      "\u001B[93mTest Loss: 71.080.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 11.701.. \n",
      "\u001B[93mTest Loss: 68.744.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 13.762.. \n",
      "\u001B[93mTest Loss: 71.191.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 11.164.. \n",
      "\u001B[93mTest Loss: 71.140.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 9.805.. \n",
      "\u001B[93mTest Loss: 69.646.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 9.840.. \n",
      "\u001B[93mTest Loss: 70.940.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 10.442.. \n",
      "\u001B[93mTest Loss: 71.692.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 11.659.. \n",
      "\u001B[93mTest Loss: 72.622.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 11.735.. \n",
      "\u001B[93mTest Loss: 71.623.. \n",
      "\u001B[36mEpoch: 115/200..  Training Loss: 12.260.. \n",
      "\u001B[93mTest Loss: 71.209.. \n",
      "\u001B[36mEpoch: 116/200..  Training Loss: 10.687.. \n",
      "\u001B[93mTest Loss: 70.592.. \n",
      "\u001B[36mEpoch: 117/200..  Training Loss: 10.555.. \n",
      "\u001B[93mTest Loss: 69.877.. \n",
      "\u001B[36mEpoch: 118/200..  Training Loss: 8.997.. \n",
      "\u001B[93mTest Loss: 71.272.. \n",
      "\u001B[36mEpoch: 119/200..  Training Loss: 8.876.. \n",
      "\u001B[93mTest Loss: 69.734.. \n",
      "\u001B[36mEpoch: 120/200..  Training Loss: 10.157.. \n",
      "\u001B[93mTest Loss: 71.421.. \n",
      "\u001B[36mEpoch: 121/200..  Training Loss: 9.367.. \n",
      "\u001B[93mTest Loss: 70.833.. \n",
      "\u001B[36mEpoch: 122/200..  Training Loss: 10.572.. \n",
      "\u001B[93mTest Loss: 69.309.. \n",
      "\u001B[36mEpoch: 123/200..  Training Loss: 15.584.. \n",
      "\u001B[93mTest Loss: 72.524.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  180.13678359985352 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 103.574.. \n",
      "\u001B[93mTest Loss: 109.824.. \n",
      "\u001B[32mTest loss decreased (inf --> 109.824448).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 81.202.. \n",
      "\u001B[93mTest Loss: 101.380.. \n",
      "\u001B[32mTest loss decreased (109.824448 --> 101.379578).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 75.706.. \n",
      "\u001B[93mTest Loss: 99.582.. \n",
      "\u001B[32mTest loss decreased (101.379578 --> 99.581589).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 73.206.. \n",
      "\u001B[93mTest Loss: 99.519.. \n",
      "\u001B[32mTest loss decreased (99.581589 --> 99.518898).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 71.998.. \n",
      "\u001B[93mTest Loss: 97.589.. \n",
      "\u001B[32mTest loss decreased (99.518898 --> 97.589211).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 70.743.. \n",
      "\u001B[93mTest Loss: 96.811.. \n",
      "\u001B[32mTest loss decreased (97.589211 --> 96.810913).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 70.339.. \n",
      "\u001B[93mTest Loss: 94.315.. \n",
      "\u001B[32mTest loss decreased (96.810913 --> 94.314857).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 69.910.. \n",
      "\u001B[93mTest Loss: 93.945.. \n",
      "\u001B[32mTest loss decreased (94.314857 --> 93.945068).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 68.388.. \n",
      "\u001B[93mTest Loss: 96.094.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 68.583.. \n",
      "\u001B[93mTest Loss: 94.984.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 67.615.. \n",
      "\u001B[93mTest Loss: 93.584.. \n",
      "\u001B[32mTest loss decreased (93.945068 --> 93.584160).  Saving model ...\n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 67.209.. \n",
      "\u001B[93mTest Loss: 92.745.. \n",
      "\u001B[32mTest loss decreased (93.584160 --> 92.745293).  Saving model ...\n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 66.213.. \n",
      "\u001B[93mTest Loss: 94.472.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 67.286.. \n",
      "\u001B[93mTest Loss: 95.715.. \n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 66.127.. \n",
      "\u001B[93mTest Loss: 93.425.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 65.326.. \n",
      "\u001B[93mTest Loss: 91.549.. \n",
      "\u001B[32mTest loss decreased (92.745293 --> 91.548843).  Saving model ...\n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 64.921.. \n",
      "\u001B[93mTest Loss: 90.077.. \n",
      "\u001B[32mTest loss decreased (91.548843 --> 90.077370).  Saving model ...\n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 64.413.. \n",
      "\u001B[93mTest Loss: 91.119.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 64.346.. \n",
      "\u001B[93mTest Loss: 94.396.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 63.144.. \n",
      "\u001B[93mTest Loss: 89.872.. \n",
      "\u001B[32mTest loss decreased (90.077370 --> 89.872215).  Saving model ...\n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 63.151.. \n",
      "\u001B[93mTest Loss: 86.889.. \n",
      "\u001B[32mTest loss decreased (89.872215 --> 86.889030).  Saving model ...\n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 61.757.. \n",
      "\u001B[93mTest Loss: 88.611.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 61.847.. \n",
      "\u001B[93mTest Loss: 90.140.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 61.006.. \n",
      "\u001B[93mTest Loss: 89.024.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 60.808.. \n",
      "\u001B[93mTest Loss: 84.519.. \n",
      "\u001B[32mTest loss decreased (86.889030 --> 84.518906).  Saving model ...\n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 60.452.. \n",
      "\u001B[93mTest Loss: 89.545.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 59.925.. \n",
      "\u001B[93mTest Loss: 84.060.. \n",
      "\u001B[32mTest loss decreased (84.518906 --> 84.059547).  Saving model ...\n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 59.376.. \n",
      "\u001B[93mTest Loss: 83.970.. \n",
      "\u001B[32mTest loss decreased (84.059547 --> 83.969589).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 58.543.. \n",
      "\u001B[93mTest Loss: 85.317.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 57.118.. \n",
      "\u001B[93mTest Loss: 90.599.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 56.912.. \n",
      "\u001B[93mTest Loss: 83.512.. \n",
      "\u001B[32mTest loss decreased (83.969589 --> 83.512207).  Saving model ...\n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 57.399.. \n",
      "\u001B[93mTest Loss: 80.632.. \n",
      "\u001B[32mTest loss decreased (83.512207 --> 80.632439).  Saving model ...\n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 55.226.. \n",
      "\u001B[93mTest Loss: 90.292.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 55.265.. \n",
      "\u001B[93mTest Loss: 76.793.. \n",
      "\u001B[32mTest loss decreased (80.632439 --> 76.792694).  Saving model ...\n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 54.050.. \n",
      "\u001B[93mTest Loss: 83.593.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 55.293.. \n",
      "\u001B[93mTest Loss: 81.751.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 53.154.. \n",
      "\u001B[93mTest Loss: 80.699.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 52.615.. \n",
      "\u001B[93mTest Loss: 85.730.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 53.520.. \n",
      "\u001B[93mTest Loss: 80.546.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 53.456.. \n",
      "\u001B[93mTest Loss: 81.104.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 52.479.. \n",
      "\u001B[93mTest Loss: 78.878.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 52.694.. \n",
      "\u001B[93mTest Loss: 93.010.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 53.010.. \n",
      "\u001B[93mTest Loss: 82.492.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 51.135.. \n",
      "\u001B[93mTest Loss: 78.714.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 52.345.. \n",
      "\u001B[93mTest Loss: 77.152.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 51.550.. \n",
      "\u001B[93mTest Loss: 77.658.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 50.932.. \n",
      "\u001B[93mTest Loss: 78.795.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 51.784.. \n",
      "\u001B[93mTest Loss: 79.242.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 50.135.. \n",
      "\u001B[93mTest Loss: 76.197.. \n",
      "\u001B[32mTest loss decreased (76.792694 --> 76.197372).  Saving model ...\n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 51.584.. \n",
      "\u001B[93mTest Loss: 81.755.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 50.703.. \n",
      "\u001B[93mTest Loss: 85.775.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 48.176.. \n",
      "\u001B[93mTest Loss: 75.348.. \n",
      "\u001B[32mTest loss decreased (76.197372 --> 75.348145).  Saving model ...\n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 49.524.. \n",
      "\u001B[93mTest Loss: 74.845.. \n",
      "\u001B[32mTest loss decreased (75.348145 --> 74.844795).  Saving model ...\n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 47.546.. \n",
      "\u001B[93mTest Loss: 74.822.. \n",
      "\u001B[32mTest loss decreased (74.844795 --> 74.821747).  Saving model ...\n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 49.328.. \n",
      "\u001B[93mTest Loss: 83.080.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 49.366.. \n",
      "\u001B[93mTest Loss: 75.912.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 46.496.. \n",
      "\u001B[93mTest Loss: 82.612.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 45.970.. \n",
      "\u001B[93mTest Loss: 83.277.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 47.883.. \n",
      "\u001B[93mTest Loss: 76.141.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 46.074.. \n",
      "\u001B[93mTest Loss: 78.344.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 47.149.. \n",
      "\u001B[93mTest Loss: 75.092.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 44.036.. \n",
      "\u001B[93mTest Loss: 76.224.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 45.218.. \n",
      "\u001B[93mTest Loss: 73.794.. \n",
      "\u001B[32mTest loss decreased (74.821747 --> 73.794090).  Saving model ...\n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 47.523.. \n",
      "\u001B[93mTest Loss: 76.269.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 44.721.. \n",
      "\u001B[93mTest Loss: 73.021.. \n",
      "\u001B[32mTest loss decreased (73.794090 --> 73.020500).  Saving model ...\n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 46.209.. \n",
      "\u001B[93mTest Loss: 74.426.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 42.800.. \n",
      "\u001B[93mTest Loss: 81.269.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 44.112.. \n",
      "\u001B[93mTest Loss: 71.020.. \n",
      "\u001B[32mTest loss decreased (73.020500 --> 71.020416).  Saving model ...\n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 41.294.. \n",
      "\u001B[93mTest Loss: 73.883.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 44.863.. \n",
      "\u001B[93mTest Loss: 87.316.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 50.138.. \n",
      "\u001B[93mTest Loss: 75.845.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 41.367.. \n",
      "\u001B[93mTest Loss: 72.343.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 43.873.. \n",
      "\u001B[93mTest Loss: 75.621.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 44.882.. \n",
      "\u001B[93mTest Loss: 75.751.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 43.103.. \n",
      "\u001B[93mTest Loss: 76.810.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 42.330.. \n",
      "\u001B[93mTest Loss: 82.212.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 46.202.. \n",
      "\u001B[93mTest Loss: 77.512.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 42.255.. \n",
      "\u001B[93mTest Loss: 71.594.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 41.239.. \n",
      "\u001B[93mTest Loss: 77.547.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 42.592.. \n",
      "\u001B[93mTest Loss: 86.363.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 39.723.. \n",
      "\u001B[93mTest Loss: 74.590.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 41.905.. \n",
      "\u001B[93mTest Loss: 73.461.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 40.237.. \n",
      "\u001B[93mTest Loss: 73.339.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 39.356.. \n",
      "\u001B[93mTest Loss: 74.362.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 38.266.. \n",
      "\u001B[93mTest Loss: 74.628.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 38.704.. \n",
      "\u001B[93mTest Loss: 82.906.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 39.310.. \n",
      "\u001B[93mTest Loss: 77.719.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 41.133.. \n",
      "\u001B[93mTest Loss: 74.382.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 36.967.. \n",
      "\u001B[93mTest Loss: 71.870.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 41.050.. \n",
      "\u001B[93mTest Loss: 77.110.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 43.433.. \n",
      "\u001B[93mTest Loss: 76.400.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 38.887.. \n",
      "\u001B[93mTest Loss: 72.452.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 39.771.. \n",
      "\u001B[93mTest Loss: 72.243.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 39.698.. \n",
      "\u001B[93mTest Loss: 73.461.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 37.025.. \n",
      "\u001B[93mTest Loss: 72.086.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 36.664.. \n",
      "\u001B[93mTest Loss: 74.088.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 39.797.. \n",
      "\u001B[93mTest Loss: 74.430.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 35.294.. \n",
      "\u001B[93mTest Loss: 73.232.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 42.873.. \n",
      "\u001B[93mTest Loss: 82.358.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 39.743.. \n",
      "\u001B[93mTest Loss: 75.567.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 39.021.. \n",
      "\u001B[93mTest Loss: 92.135.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 36.953.. \n",
      "\u001B[93mTest Loss: 76.860.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 37.000.. \n",
      "\u001B[93mTest Loss: 74.667.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 41.038.. \n",
      "\u001B[93mTest Loss: 72.949.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 34.352.. \n",
      "\u001B[93mTest Loss: 72.213.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 36.634.. \n",
      "\u001B[93mTest Loss: 73.027.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 34.922.. \n",
      "\u001B[93mTest Loss: 74.634.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 36.820.. \n",
      "\u001B[93mTest Loss: 75.205.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  192.95483684539795 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 100.565.. \n",
      "\u001B[93mTest Loss: 105.126.. \n",
      "\u001B[32mTest loss decreased (inf --> 105.126366).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 88.247.. \n",
      "\u001B[93mTest Loss: 102.943.. \n",
      "\u001B[32mTest loss decreased (105.126366 --> 102.942795).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 86.171.. \n",
      "\u001B[93mTest Loss: 100.433.. \n",
      "\u001B[32mTest loss decreased (102.942795 --> 100.433495).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 84.396.. \n",
      "\u001B[93mTest Loss: 98.896.. \n",
      "\u001B[32mTest loss decreased (100.433495 --> 98.896149).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 82.118.. \n",
      "\u001B[93mTest Loss: 97.740.. \n",
      "\u001B[32mTest loss decreased (98.896149 --> 97.740486).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 79.589.. \n",
      "\u001B[93mTest Loss: 97.473.. \n",
      "\u001B[32mTest loss decreased (97.740486 --> 97.473007).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 78.924.. \n",
      "\u001B[93mTest Loss: 96.810.. \n",
      "\u001B[32mTest loss decreased (97.473007 --> 96.810120).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 77.052.. \n",
      "\u001B[93mTest Loss: 98.819.. \n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 76.902.. \n",
      "\u001B[93mTest Loss: 96.241.. \n",
      "\u001B[32mTest loss decreased (96.810120 --> 96.240578).  Saving model ...\n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 75.531.. \n",
      "\u001B[93mTest Loss: 93.350.. \n",
      "\u001B[32mTest loss decreased (96.240578 --> 93.349930).  Saving model ...\n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 73.458.. \n",
      "\u001B[93mTest Loss: 96.301.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 72.787.. \n",
      "\u001B[93mTest Loss: 93.821.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 72.081.. \n",
      "\u001B[93mTest Loss: 91.817.. \n",
      "\u001B[32mTest loss decreased (93.349930 --> 91.817207).  Saving model ...\n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 70.095.. \n",
      "\u001B[93mTest Loss: 94.749.. \n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 69.537.. \n",
      "\u001B[93mTest Loss: 92.531.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 68.719.. \n",
      "\u001B[93mTest Loss: 94.438.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 65.727.. \n",
      "\u001B[93mTest Loss: 92.274.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 64.924.. \n",
      "\u001B[93mTest Loss: 88.611.. \n",
      "\u001B[32mTest loss decreased (91.817207 --> 88.610550).  Saving model ...\n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 63.808.. \n",
      "\u001B[93mTest Loss: 91.305.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 62.047.. \n",
      "\u001B[93mTest Loss: 87.696.. \n",
      "\u001B[32mTest loss decreased (88.610550 --> 87.695625).  Saving model ...\n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 59.545.. \n",
      "\u001B[93mTest Loss: 88.287.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 59.376.. \n",
      "\u001B[93mTest Loss: 85.010.. \n",
      "\u001B[32mTest loss decreased (87.695625 --> 85.009689).  Saving model ...\n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 56.662.. \n",
      "\u001B[93mTest Loss: 86.678.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 56.979.. \n",
      "\u001B[93mTest Loss: 86.914.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 53.859.. \n",
      "\u001B[93mTest Loss: 90.030.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 52.958.. \n",
      "\u001B[93mTest Loss: 86.991.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 51.850.. \n",
      "\u001B[93mTest Loss: 83.856.. \n",
      "\u001B[32mTest loss decreased (85.009689 --> 83.855537).  Saving model ...\n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 49.858.. \n",
      "\u001B[93mTest Loss: 82.072.. \n",
      "\u001B[32mTest loss decreased (83.855537 --> 82.071510).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 47.976.. \n",
      "\u001B[93mTest Loss: 84.608.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 46.598.. \n",
      "\u001B[93mTest Loss: 78.537.. \n",
      "\u001B[32mTest loss decreased (82.071510 --> 78.537064).  Saving model ...\n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 43.713.. \n",
      "\u001B[93mTest Loss: 88.289.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 41.992.. \n",
      "\u001B[93mTest Loss: 76.736.. \n",
      "\u001B[32mTest loss decreased (78.537064 --> 76.736275).  Saving model ...\n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 41.615.. \n",
      "\u001B[93mTest Loss: 81.527.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 41.385.. \n",
      "\u001B[93mTest Loss: 75.719.. \n",
      "\u001B[32mTest loss decreased (76.736275 --> 75.719162).  Saving model ...\n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 38.830.. \n",
      "\u001B[93mTest Loss: 84.459.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 39.023.. \n",
      "\u001B[93mTest Loss: 81.274.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 38.444.. \n",
      "\u001B[93mTest Loss: 80.371.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 38.922.. \n",
      "\u001B[93mTest Loss: 75.859.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 34.681.. \n",
      "\u001B[93mTest Loss: 74.914.. \n",
      "\u001B[32mTest loss decreased (75.719162 --> 74.913795).  Saving model ...\n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 34.894.. \n",
      "\u001B[93mTest Loss: 74.347.. \n",
      "\u001B[32mTest loss decreased (74.913795 --> 74.347321).  Saving model ...\n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 33.558.. \n",
      "\u001B[93mTest Loss: 74.225.. \n",
      "\u001B[32mTest loss decreased (74.347321 --> 74.224640).  Saving model ...\n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 32.650.. \n",
      "\u001B[93mTest Loss: 75.769.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 34.222.. \n",
      "\u001B[93mTest Loss: 76.368.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 31.370.. \n",
      "\u001B[93mTest Loss: 75.084.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 31.707.. \n",
      "\u001B[93mTest Loss: 75.050.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 31.829.. \n",
      "\u001B[93mTest Loss: 76.454.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 31.777.. \n",
      "\u001B[93mTest Loss: 75.268.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 29.048.. \n",
      "\u001B[93mTest Loss: 76.609.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 30.934.. \n",
      "\u001B[93mTest Loss: 77.124.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 29.403.. \n",
      "\u001B[93mTest Loss: 75.579.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 26.714.. \n",
      "\u001B[93mTest Loss: 75.513.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 27.548.. \n",
      "\u001B[93mTest Loss: 77.493.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 28.812.. \n",
      "\u001B[93mTest Loss: 80.023.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 29.382.. \n",
      "\u001B[93mTest Loss: 77.492.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 27.778.. \n",
      "\u001B[93mTest Loss: 73.095.. \n",
      "\u001B[32mTest loss decreased (74.224640 --> 73.095108).  Saving model ...\n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 25.400.. \n",
      "\u001B[93mTest Loss: 76.153.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 25.482.. \n",
      "\u001B[93mTest Loss: 74.873.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 27.189.. \n",
      "\u001B[93mTest Loss: 75.409.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 24.181.. \n",
      "\u001B[93mTest Loss: 78.420.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 23.990.. \n",
      "\u001B[93mTest Loss: 73.237.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 24.537.. \n",
      "\u001B[93mTest Loss: 72.606.. \n",
      "\u001B[32mTest loss decreased (73.095108 --> 72.605751).  Saving model ...\n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 26.111.. \n",
      "\u001B[93mTest Loss: 71.258.. \n",
      "\u001B[32mTest loss decreased (72.605751 --> 71.258415).  Saving model ...\n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 24.830.. \n",
      "\u001B[93mTest Loss: 75.269.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 24.513.. \n",
      "\u001B[93mTest Loss: 72.091.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 24.080.. \n",
      "\u001B[93mTest Loss: 74.120.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 24.375.. \n",
      "\u001B[93mTest Loss: 74.375.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 22.838.. \n",
      "\u001B[93mTest Loss: 73.421.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 21.249.. \n",
      "\u001B[93mTest Loss: 72.229.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 23.686.. \n",
      "\u001B[93mTest Loss: 76.471.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 22.097.. \n",
      "\u001B[93mTest Loss: 75.023.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 22.707.. \n",
      "\u001B[93mTest Loss: 76.491.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 20.452.. \n",
      "\u001B[93mTest Loss: 74.080.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 20.439.. \n",
      "\u001B[93mTest Loss: 72.648.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 20.258.. \n",
      "\u001B[93mTest Loss: 77.591.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 22.404.. \n",
      "\u001B[93mTest Loss: 77.499.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 22.766.. \n",
      "\u001B[93mTest Loss: 74.566.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 19.109.. \n",
      "\u001B[93mTest Loss: 74.172.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 19.557.. \n",
      "\u001B[93mTest Loss: 72.352.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 17.946.. \n",
      "\u001B[93mTest Loss: 70.931.. \n",
      "\u001B[32mTest loss decreased (71.258415 --> 70.931465).  Saving model ...\n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 17.395.. \n",
      "\u001B[93mTest Loss: 72.780.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 22.428.. \n",
      "\u001B[93mTest Loss: 89.269.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 21.422.. \n",
      "\u001B[93mTest Loss: 73.340.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 20.213.. \n",
      "\u001B[93mTest Loss: 79.177.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 17.520.. \n",
      "\u001B[93mTest Loss: 75.522.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 18.592.. \n",
      "\u001B[93mTest Loss: 73.700.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 17.678.. \n",
      "\u001B[93mTest Loss: 75.971.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 18.605.. \n",
      "\u001B[93mTest Loss: 72.870.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 15.850.. \n",
      "\u001B[93mTest Loss: 71.072.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 17.829.. \n",
      "\u001B[93mTest Loss: 73.962.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 17.978.. \n",
      "\u001B[93mTest Loss: 76.168.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 17.459.. \n",
      "\u001B[93mTest Loss: 76.667.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 21.988.. \n",
      "\u001B[93mTest Loss: 72.098.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 20.894.. \n",
      "\u001B[93mTest Loss: 75.045.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 18.110.. \n",
      "\u001B[93mTest Loss: 77.148.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 16.489.. \n",
      "\u001B[93mTest Loss: 73.051.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 16.111.. \n",
      "\u001B[93mTest Loss: 75.936.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 18.425.. \n",
      "\u001B[93mTest Loss: 74.206.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 15.720.. \n",
      "\u001B[93mTest Loss: 70.271.. \n",
      "\u001B[32mTest loss decreased (70.931465 --> 70.270630).  Saving model ...\n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 15.102.. \n",
      "\u001B[93mTest Loss: 79.670.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 18.566.. \n",
      "\u001B[93mTest Loss: 78.526.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 15.300.. \n",
      "\u001B[93mTest Loss: 70.634.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 13.366.. \n",
      "\u001B[93mTest Loss: 70.628.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 13.694.. \n",
      "\u001B[93mTest Loss: 74.752.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 17.068.. \n",
      "\u001B[93mTest Loss: 75.765.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 15.092.. \n",
      "\u001B[93mTest Loss: 75.887.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 13.267.. \n",
      "\u001B[93mTest Loss: 71.098.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 13.191.. \n",
      "\u001B[93mTest Loss: 72.177.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 18.810.. \n",
      "\u001B[93mTest Loss: 76.624.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 17.146.. \n",
      "\u001B[93mTest Loss: 72.946.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 12.930.. \n",
      "\u001B[93mTest Loss: 71.502.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 14.275.. \n",
      "\u001B[93mTest Loss: 71.888.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 13.154.. \n",
      "\u001B[93mTest Loss: 74.249.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 14.093.. \n",
      "\u001B[93mTest Loss: 74.041.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 12.632.. \n",
      "\u001B[93mTest Loss: 71.482.. \n",
      "\u001B[36mEpoch: 115/200..  Training Loss: 14.716.. \n",
      "\u001B[93mTest Loss: 73.881.. \n",
      "\u001B[36mEpoch: 116/200..  Training Loss: 12.180.. \n",
      "\u001B[93mTest Loss: 74.161.. \n",
      "\u001B[36mEpoch: 117/200..  Training Loss: 14.166.. \n",
      "\u001B[93mTest Loss: 73.109.. \n",
      "\u001B[36mEpoch: 118/200..  Training Loss: 16.343.. \n",
      "\u001B[93mTest Loss: 72.522.. \n",
      "\u001B[36mEpoch: 119/200..  Training Loss: 13.702.. \n",
      "\u001B[93mTest Loss: 73.605.. \n",
      "\u001B[36mEpoch: 120/200..  Training Loss: 12.472.. \n",
      "\u001B[93mTest Loss: 72.495.. \n",
      "\u001B[36mEpoch: 121/200..  Training Loss: 14.421.. \n",
      "\u001B[93mTest Loss: 73.680.. \n",
      "\u001B[36mEpoch: 122/200..  Training Loss: 13.753.. \n",
      "\u001B[93mTest Loss: 73.059.. \n",
      "\u001B[36mEpoch: 123/200..  Training Loss: 11.392.. \n",
      "\u001B[93mTest Loss: 71.162.. \n",
      "\u001B[36mEpoch: 124/200..  Training Loss: 10.724.. \n",
      "\u001B[93mTest Loss: 75.346.. \n",
      "\u001B[36mEpoch: 125/200..  Training Loss: 12.127.. \n",
      "\u001B[93mTest Loss: 77.285.. \n",
      "\u001B[36mEpoch: 126/200..  Training Loss: 19.581.. \n",
      "\u001B[93mTest Loss: 71.723.. \n",
      "\u001B[36mEpoch: 127/200..  Training Loss: 13.170.. \n",
      "\u001B[93mTest Loss: 70.341.. \n",
      "\u001B[36mEpoch: 128/200..  Training Loss: 11.450.. \n",
      "\u001B[93mTest Loss: 71.214.. \n",
      "\u001B[36mEpoch: 129/200..  Training Loss: 14.744.. \n",
      "\u001B[93mTest Loss: 72.998.. \n",
      "\u001B[36mEpoch: 130/200..  Training Loss: 11.361.. \n",
      "\u001B[93mTest Loss: 70.501.. \n",
      "\u001B[36mEpoch: 131/200..  Training Loss: 9.941.. \n",
      "\u001B[93mTest Loss: 71.185.. \n",
      "\u001B[36mEpoch: 132/200..  Training Loss: 9.902.. \n",
      "\u001B[93mTest Loss: 73.206.. \n",
      "\u001B[36mEpoch: 133/200..  Training Loss: 10.260.. \n",
      "\u001B[93mTest Loss: 72.218.. \n",
      "\u001B[36mEpoch: 134/200..  Training Loss: 16.596.. \n",
      "\u001B[93mTest Loss: 76.541.. \n",
      "\u001B[36mEpoch: 135/200..  Training Loss: 15.146.. \n",
      "\u001B[93mTest Loss: 78.824.. \n",
      "\u001B[36mEpoch: 136/200..  Training Loss: 13.242.. \n",
      "\u001B[93mTest Loss: 75.359.. \n",
      "\u001B[36mEpoch: 137/200..  Training Loss: 11.949.. \n",
      "\u001B[93mTest Loss: 72.657.. \n",
      "\u001B[36mEpoch: 138/200..  Training Loss: 10.034.. \n",
      "\u001B[93mTest Loss: 74.535.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  222.7722203731537 seconds\n",
      "\u001B[32mLowest Test loss achieved for Standard Net: 68.013680\n",
      "\u001B[32mAt Epoch #82 \n",
      "\n",
      "\u001B[32mLowest Elastic-Net loss achieved: 71.020416\n",
      "\u001B[32mAt Epoch #67 \n",
      "\n",
      "\u001B[32mLowest Test loss achieved for Gradient-Net: 70.270630\n",
      "\u001B[32mAt Epoch #97 \n",
      "\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 99.050.. \n",
      "\u001B[93mTest Loss: 106.327.. \n",
      "\u001B[32mTest loss decreased (inf --> 106.326698).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 85.710.. \n",
      "\u001B[93mTest Loss: 98.534.. \n",
      "\u001B[32mTest loss decreased (106.326698 --> 98.533798).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 81.898.. \n",
      "\u001B[93mTest Loss: 97.590.. \n",
      "\u001B[32mTest loss decreased (98.533798 --> 97.590218).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 80.882.. \n",
      "\u001B[93mTest Loss: 96.632.. \n",
      "\u001B[32mTest loss decreased (97.590218 --> 96.631538).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 79.043.. \n",
      "\u001B[93mTest Loss: 96.580.. \n",
      "\u001B[32mTest loss decreased (96.631538 --> 96.580139).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 76.799.. \n",
      "\u001B[93mTest Loss: 99.675.. \n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 75.722.. \n",
      "\u001B[93mTest Loss: 99.299.. \n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 73.363.. \n",
      "\u001B[93mTest Loss: 92.414.. \n",
      "\u001B[32mTest loss decreased (96.580139 --> 92.414116).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 73.573.. \n",
      "\u001B[93mTest Loss: 96.091.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 72.338.. \n",
      "\u001B[93mTest Loss: 92.798.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 70.655.. \n",
      "\u001B[93mTest Loss: 94.532.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 69.216.. \n",
      "\u001B[93mTest Loss: 92.495.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 69.415.. \n",
      "\u001B[93mTest Loss: 94.228.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 66.909.. \n",
      "\u001B[93mTest Loss: 96.287.. \n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 66.667.. \n",
      "\u001B[93mTest Loss: 94.508.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 65.026.. \n",
      "\u001B[93mTest Loss: 92.669.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 63.766.. \n",
      "\u001B[93mTest Loss: 94.597.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 63.591.. \n",
      "\u001B[93mTest Loss: 90.005.. \n",
      "\u001B[32mTest loss decreased (92.414116 --> 90.005409).  Saving model ...\n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 60.146.. \n",
      "\u001B[93mTest Loss: 94.639.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 59.239.. \n",
      "\u001B[93mTest Loss: 89.465.. \n",
      "\u001B[32mTest loss decreased (90.005409 --> 89.464996).  Saving model ...\n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 57.632.. \n",
      "\u001B[93mTest Loss: 86.578.. \n",
      "\u001B[32mTest loss decreased (89.464996 --> 86.577759).  Saving model ...\n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 58.029.. \n",
      "\u001B[93mTest Loss: 89.644.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 55.261.. \n",
      "\u001B[93mTest Loss: 87.439.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 54.457.. \n",
      "\u001B[93mTest Loss: 87.205.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 52.151.. \n",
      "\u001B[93mTest Loss: 85.576.. \n",
      "\u001B[32mTest loss decreased (86.577759 --> 85.575920).  Saving model ...\n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 50.604.. \n",
      "\u001B[93mTest Loss: 84.860.. \n",
      "\u001B[32mTest loss decreased (85.575920 --> 84.860252).  Saving model ...\n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 48.166.. \n",
      "\u001B[93mTest Loss: 84.008.. \n",
      "\u001B[32mTest loss decreased (84.860252 --> 84.008209).  Saving model ...\n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 47.842.. \n",
      "\u001B[93mTest Loss: 83.863.. \n",
      "\u001B[32mTest loss decreased (84.008209 --> 83.863213).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 44.933.. \n",
      "\u001B[93mTest Loss: 81.123.. \n",
      "\u001B[32mTest loss decreased (83.863213 --> 81.122696).  Saving model ...\n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 43.363.. \n",
      "\u001B[93mTest Loss: 84.275.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 42.273.. \n",
      "\u001B[93mTest Loss: 88.912.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 41.521.. \n",
      "\u001B[93mTest Loss: 77.908.. \n",
      "\u001B[32mTest loss decreased (81.122696 --> 77.907944).  Saving model ...\n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 39.410.. \n",
      "\u001B[93mTest Loss: 76.594.. \n",
      "\u001B[32mTest loss decreased (77.907944 --> 76.593918).  Saving model ...\n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 37.874.. \n",
      "\u001B[93mTest Loss: 75.904.. \n",
      "\u001B[32mTest loss decreased (76.593918 --> 75.904045).  Saving model ...\n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 37.527.. \n",
      "\u001B[93mTest Loss: 82.448.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 36.182.. \n",
      "\u001B[93mTest Loss: 79.630.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 35.885.. \n",
      "\u001B[93mTest Loss: 75.669.. \n",
      "\u001B[32mTest loss decreased (75.904045 --> 75.669106).  Saving model ...\n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 34.219.. \n",
      "\u001B[93mTest Loss: 82.739.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 33.838.. \n",
      "\u001B[93mTest Loss: 78.783.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 33.579.. \n",
      "\u001B[93mTest Loss: 74.552.. \n",
      "\u001B[32mTest loss decreased (75.669106 --> 74.552444).  Saving model ...\n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 32.341.. \n",
      "\u001B[93mTest Loss: 74.601.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 29.760.. \n",
      "\u001B[93mTest Loss: 73.689.. \n",
      "\u001B[32mTest loss decreased (74.552444 --> 73.689079).  Saving model ...\n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 30.292.. \n",
      "\u001B[93mTest Loss: 73.190.. \n",
      "\u001B[32mTest loss decreased (73.689079 --> 73.190269).  Saving model ...\n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 28.904.. \n",
      "\u001B[93mTest Loss: 70.938.. \n",
      "\u001B[32mTest loss decreased (73.190269 --> 70.938492).  Saving model ...\n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 29.487.. \n",
      "\u001B[93mTest Loss: 73.160.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 28.498.. \n",
      "\u001B[93mTest Loss: 76.880.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 28.305.. \n",
      "\u001B[93mTest Loss: 72.414.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 26.366.. \n",
      "\u001B[93mTest Loss: 72.204.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 26.971.. \n",
      "\u001B[93mTest Loss: 71.798.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 27.672.. \n",
      "\u001B[93mTest Loss: 72.579.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 25.461.. \n",
      "\u001B[93mTest Loss: 74.619.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 25.342.. \n",
      "\u001B[93mTest Loss: 70.224.. \n",
      "\u001B[32mTest loss decreased (70.938492 --> 70.223579).  Saving model ...\n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 24.328.. \n",
      "\u001B[93mTest Loss: 73.571.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 25.957.. \n",
      "\u001B[93mTest Loss: 77.531.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 24.774.. \n",
      "\u001B[93mTest Loss: 69.756.. \n",
      "\u001B[32mTest loss decreased (70.223579 --> 69.756432).  Saving model ...\n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 23.504.. \n",
      "\u001B[93mTest Loss: 74.521.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 24.097.. \n",
      "\u001B[93mTest Loss: 73.587.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 21.808.. \n",
      "\u001B[93mTest Loss: 72.996.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 22.529.. \n",
      "\u001B[93mTest Loss: 77.352.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 24.292.. \n",
      "\u001B[93mTest Loss: 71.264.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 20.419.. \n",
      "\u001B[93mTest Loss: 71.761.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 19.786.. \n",
      "\u001B[93mTest Loss: 71.949.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 19.246.. \n",
      "\u001B[93mTest Loss: 72.578.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 19.475.. \n",
      "\u001B[93mTest Loss: 72.337.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 20.863.. \n",
      "\u001B[93mTest Loss: 71.291.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 18.335.. \n",
      "\u001B[93mTest Loss: 72.476.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 19.998.. \n",
      "\u001B[93mTest Loss: 72.975.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 20.219.. \n",
      "\u001B[93mTest Loss: 76.049.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 19.658.. \n",
      "\u001B[93mTest Loss: 69.759.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 23.551.. \n",
      "\u001B[93mTest Loss: 75.592.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 18.049.. \n",
      "\u001B[93mTest Loss: 71.058.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 16.693.. \n",
      "\u001B[93mTest Loss: 76.729.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 16.816.. \n",
      "\u001B[93mTest Loss: 74.228.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 16.809.. \n",
      "\u001B[93mTest Loss: 69.090.. \n",
      "\u001B[32mTest loss decreased (69.756432 --> 69.090080).  Saving model ...\n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 16.250.. \n",
      "\u001B[93mTest Loss: 75.713.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 17.636.. \n",
      "\u001B[93mTest Loss: 70.383.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 17.703.. \n",
      "\u001B[93mTest Loss: 71.975.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 16.347.. \n",
      "\u001B[93mTest Loss: 71.520.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 14.395.. \n",
      "\u001B[93mTest Loss: 69.755.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 17.774.. \n",
      "\u001B[93mTest Loss: 71.896.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 15.770.. \n",
      "\u001B[93mTest Loss: 71.805.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 14.289.. \n",
      "\u001B[93mTest Loss: 71.220.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 14.378.. \n",
      "\u001B[93mTest Loss: 71.172.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 14.189.. \n",
      "\u001B[93mTest Loss: 75.821.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 16.424.. \n",
      "\u001B[93mTest Loss: 76.389.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 16.284.. \n",
      "\u001B[93mTest Loss: 78.017.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 16.015.. \n",
      "\u001B[93mTest Loss: 71.957.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 13.561.. \n",
      "\u001B[93mTest Loss: 71.595.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 13.617.. \n",
      "\u001B[93mTest Loss: 73.060.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 12.132.. \n",
      "\u001B[93mTest Loss: 73.742.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 14.228.. \n",
      "\u001B[93mTest Loss: 86.914.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 17.490.. \n",
      "\u001B[93mTest Loss: 71.633.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 12.068.. \n",
      "\u001B[93mTest Loss: 70.566.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 11.821.. \n",
      "\u001B[93mTest Loss: 71.184.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 12.896.. \n",
      "\u001B[93mTest Loss: 71.869.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 12.915.. \n",
      "\u001B[93mTest Loss: 72.749.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 15.973.. \n",
      "\u001B[93mTest Loss: 74.437.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 16.836.. \n",
      "\u001B[93mTest Loss: 74.288.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 12.417.. \n",
      "\u001B[93mTest Loss: 70.215.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 10.155.. \n",
      "\u001B[93mTest Loss: 72.661.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 9.983.. \n",
      "\u001B[93mTest Loss: 70.449.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 9.737.. \n",
      "\u001B[93mTest Loss: 71.656.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 10.415.. \n",
      "\u001B[93mTest Loss: 72.540.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 12.556.. \n",
      "\u001B[93mTest Loss: 71.416.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 12.694.. \n",
      "\u001B[93mTest Loss: 74.199.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 11.354.. \n",
      "\u001B[93mTest Loss: 72.321.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 10.485.. \n",
      "\u001B[93mTest Loss: 70.679.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 13.849.. \n",
      "\u001B[93mTest Loss: 73.853.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 16.487.. \n",
      "\u001B[93mTest Loss: 75.365.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 12.625.. \n",
      "\u001B[93mTest Loss: 71.754.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 9.467.. \n",
      "\u001B[93mTest Loss: 75.402.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 10.874.. \n",
      "\u001B[93mTest Loss: 70.972.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 9.639.. \n",
      "\u001B[93mTest Loss: 71.476.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 9.341.. \n",
      "\u001B[93mTest Loss: 70.821.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  174.09291100502014 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 101.667.. \n",
      "\u001B[93mTest Loss: 107.557.. \n",
      "\u001B[32mTest loss decreased (inf --> 107.556587).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 79.151.. \n",
      "\u001B[93mTest Loss: 99.661.. \n",
      "\u001B[32mTest loss decreased (107.556587 --> 99.661392).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 75.966.. \n",
      "\u001B[93mTest Loss: 102.726.. \n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 74.164.. \n",
      "\u001B[93mTest Loss: 98.985.. \n",
      "\u001B[32mTest loss decreased (99.661392 --> 98.985382).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 72.225.. \n",
      "\u001B[93mTest Loss: 99.293.. \n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 71.115.. \n",
      "\u001B[93mTest Loss: 96.339.. \n",
      "\u001B[32mTest loss decreased (98.985382 --> 96.338654).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 70.279.. \n",
      "\u001B[93mTest Loss: 95.098.. \n",
      "\u001B[32mTest loss decreased (96.338654 --> 95.097710).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 69.282.. \n",
      "\u001B[93mTest Loss: 94.638.. \n",
      "\u001B[32mTest loss decreased (95.097710 --> 94.637573).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 68.005.. \n",
      "\u001B[93mTest Loss: 95.760.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 67.739.. \n",
      "\u001B[93mTest Loss: 92.892.. \n",
      "\u001B[32mTest loss decreased (94.637573 --> 92.891594).  Saving model ...\n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 66.637.. \n",
      "\u001B[93mTest Loss: 93.954.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 66.978.. \n",
      "\u001B[93mTest Loss: 93.216.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 66.900.. \n",
      "\u001B[93mTest Loss: 91.945.. \n",
      "\u001B[32mTest loss decreased (92.891594 --> 91.945465).  Saving model ...\n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 65.178.. \n",
      "\u001B[93mTest Loss: 89.391.. \n",
      "\u001B[32mTest loss decreased (91.945465 --> 89.390587).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 64.093.. \n",
      "\u001B[93mTest Loss: 93.988.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 64.607.. \n",
      "\u001B[93mTest Loss: 88.880.. \n",
      "\u001B[32mTest loss decreased (89.390587 --> 88.880058).  Saving model ...\n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 63.616.. \n",
      "\u001B[93mTest Loss: 91.791.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 62.302.. \n",
      "\u001B[93mTest Loss: 88.423.. \n",
      "\u001B[32mTest loss decreased (88.880058 --> 88.423340).  Saving model ...\n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 61.454.. \n",
      "\u001B[93mTest Loss: 88.301.. \n",
      "\u001B[32mTest loss decreased (88.423340 --> 88.300522).  Saving model ...\n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 61.361.. \n",
      "\u001B[93mTest Loss: 85.216.. \n",
      "\u001B[32mTest loss decreased (88.300522 --> 85.215919).  Saving model ...\n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 60.484.. \n",
      "\u001B[93mTest Loss: 86.176.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 59.018.. \n",
      "\u001B[93mTest Loss: 85.822.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 58.381.. \n",
      "\u001B[93mTest Loss: 82.246.. \n",
      "\u001B[32mTest loss decreased (85.215919 --> 82.246246).  Saving model ...\n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 58.019.. \n",
      "\u001B[93mTest Loss: 83.787.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 57.661.. \n",
      "\u001B[93mTest Loss: 82.822.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 57.000.. \n",
      "\u001B[93mTest Loss: 85.273.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 56.542.. \n",
      "\u001B[93mTest Loss: 83.689.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 54.396.. \n",
      "\u001B[93mTest Loss: 80.882.. \n",
      "\u001B[32mTest loss decreased (82.246246 --> 80.881805).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 54.805.. \n",
      "\u001B[93mTest Loss: 79.381.. \n",
      "\u001B[32mTest loss decreased (80.881805 --> 79.380783).  Saving model ...\n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 54.398.. \n",
      "\u001B[93mTest Loss: 82.084.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 54.860.. \n",
      "\u001B[93mTest Loss: 79.669.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 53.414.. \n",
      "\u001B[93mTest Loss: 76.729.. \n",
      "\u001B[32mTest loss decreased (79.380783 --> 76.729050).  Saving model ...\n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 52.760.. \n",
      "\u001B[93mTest Loss: 79.474.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 52.922.. \n",
      "\u001B[93mTest Loss: 77.018.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 52.942.. \n",
      "\u001B[93mTest Loss: 76.959.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 50.679.. \n",
      "\u001B[93mTest Loss: 79.567.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 51.516.. \n",
      "\u001B[93mTest Loss: 77.953.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 51.633.. \n",
      "\u001B[93mTest Loss: 77.698.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 52.984.. \n",
      "\u001B[93mTest Loss: 83.014.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 51.278.. \n",
      "\u001B[93mTest Loss: 77.959.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 52.562.. \n",
      "\u001B[93mTest Loss: 77.596.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 49.945.. \n",
      "\u001B[93mTest Loss: 79.607.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 51.060.. \n",
      "\u001B[93mTest Loss: 81.835.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 49.403.. \n",
      "\u001B[93mTest Loss: 75.093.. \n",
      "\u001B[32mTest loss decreased (76.729050 --> 75.093262).  Saving model ...\n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 48.753.. \n",
      "\u001B[93mTest Loss: 76.694.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 48.794.. \n",
      "\u001B[93mTest Loss: 78.789.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 48.816.. \n",
      "\u001B[93mTest Loss: 81.055.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 48.164.. \n",
      "\u001B[93mTest Loss: 75.144.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 46.295.. \n",
      "\u001B[93mTest Loss: 74.362.. \n",
      "\u001B[32mTest loss decreased (75.093262 --> 74.361824).  Saving model ...\n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 48.321.. \n",
      "\u001B[93mTest Loss: 75.296.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 44.744.. \n",
      "\u001B[93mTest Loss: 73.057.. \n",
      "\u001B[32mTest loss decreased (74.361824 --> 73.056778).  Saving model ...\n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 44.916.. \n",
      "\u001B[93mTest Loss: 76.644.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 46.449.. \n",
      "\u001B[93mTest Loss: 83.577.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 46.798.. \n",
      "\u001B[93mTest Loss: 84.568.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 45.717.. \n",
      "\u001B[93mTest Loss: 74.945.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 42.545.. \n",
      "\u001B[93mTest Loss: 73.162.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 43.687.. \n",
      "\u001B[93mTest Loss: 78.847.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 44.337.. \n",
      "\u001B[93mTest Loss: 72.899.. \n",
      "\u001B[32mTest loss decreased (73.056778 --> 72.898560).  Saving model ...\n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 42.934.. \n",
      "\u001B[93mTest Loss: 76.781.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 41.456.. \n",
      "\u001B[93mTest Loss: 77.341.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 43.426.. \n",
      "\u001B[93mTest Loss: 70.717.. \n",
      "\u001B[32mTest loss decreased (72.898560 --> 70.717323).  Saving model ...\n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 43.115.. \n",
      "\u001B[93mTest Loss: 71.781.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 42.248.. \n",
      "\u001B[93mTest Loss: 72.814.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 41.952.. \n",
      "\u001B[93mTest Loss: 75.053.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 43.942.. \n",
      "\u001B[93mTest Loss: 73.510.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 39.178.. \n",
      "\u001B[93mTest Loss: 72.337.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 38.568.. \n",
      "\u001B[93mTest Loss: 75.501.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 42.952.. \n",
      "\u001B[93mTest Loss: 78.305.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 44.577.. \n",
      "\u001B[93mTest Loss: 77.397.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 40.901.. \n",
      "\u001B[93mTest Loss: 68.818.. \n",
      "\u001B[32mTest loss decreased (70.717323 --> 68.817741).  Saving model ...\n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 38.930.. \n",
      "\u001B[93mTest Loss: 73.415.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 38.162.. \n",
      "\u001B[93mTest Loss: 75.718.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 39.391.. \n",
      "\u001B[93mTest Loss: 77.896.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 44.201.. \n",
      "\u001B[93mTest Loss: 72.606.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 37.523.. \n",
      "\u001B[93mTest Loss: 71.583.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 40.799.. \n",
      "\u001B[93mTest Loss: 71.357.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 38.915.. \n",
      "\u001B[93mTest Loss: 71.561.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 39.230.. \n",
      "\u001B[93mTest Loss: 70.242.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 36.631.. \n",
      "\u001B[93mTest Loss: 71.128.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 37.935.. \n",
      "\u001B[93mTest Loss: 84.464.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 40.743.. \n",
      "\u001B[93mTest Loss: 71.568.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 36.303.. \n",
      "\u001B[93mTest Loss: 71.623.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 45.229.. \n",
      "\u001B[93mTest Loss: 82.317.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 38.480.. \n",
      "\u001B[93mTest Loss: 73.338.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 37.268.. \n",
      "\u001B[93mTest Loss: 71.368.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 36.224.. \n",
      "\u001B[93mTest Loss: 70.642.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 37.863.. \n",
      "\u001B[93mTest Loss: 70.680.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 42.422.. \n",
      "\u001B[93mTest Loss: 73.979.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 38.136.. \n",
      "\u001B[93mTest Loss: 72.987.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 35.888.. \n",
      "\u001B[93mTest Loss: 70.702.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 41.893.. \n",
      "\u001B[93mTest Loss: 72.468.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 36.494.. \n",
      "\u001B[93mTest Loss: 75.013.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 38.567.. \n",
      "\u001B[93mTest Loss: 70.335.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 35.297.. \n",
      "\u001B[93mTest Loss: 72.694.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 36.307.. \n",
      "\u001B[93mTest Loss: 71.999.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 36.082.. \n",
      "\u001B[93mTest Loss: 76.519.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 37.650.. \n",
      "\u001B[93mTest Loss: 77.423.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 38.440.. \n",
      "\u001B[93mTest Loss: 73.277.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 35.076.. \n",
      "\u001B[93mTest Loss: 78.629.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 40.040.. \n",
      "\u001B[93mTest Loss: 79.007.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 39.249.. \n",
      "\u001B[93mTest Loss: 70.999.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 34.139.. \n",
      "\u001B[93mTest Loss: 72.487.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 37.446.. \n",
      "\u001B[93mTest Loss: 72.171.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 32.315.. \n",
      "\u001B[93mTest Loss: 74.346.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 38.265.. \n",
      "\u001B[93mTest Loss: 75.792.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 35.348.. \n",
      "\u001B[93mTest Loss: 71.393.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 32.016.. \n",
      "\u001B[93mTest Loss: 71.369.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 32.482.. \n",
      "\u001B[93mTest Loss: 72.573.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 41.365.. \n",
      "\u001B[93mTest Loss: 81.508.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 36.134.. \n",
      "\u001B[93mTest Loss: 71.973.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  199.6401777267456 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 101.167.. \n",
      "\u001B[93mTest Loss: 105.150.. \n",
      "\u001B[32mTest loss decreased (inf --> 105.150391).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 88.030.. \n",
      "\u001B[93mTest Loss: 102.418.. \n",
      "\u001B[32mTest loss decreased (105.150391 --> 102.417664).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 85.249.. \n",
      "\u001B[93mTest Loss: 97.692.. \n",
      "\u001B[32mTest loss decreased (102.417664 --> 97.692078).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 83.230.. \n",
      "\u001B[93mTest Loss: 97.345.. \n",
      "\u001B[32mTest loss decreased (97.692078 --> 97.344704).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 81.772.. \n",
      "\u001B[93mTest Loss: 95.946.. \n",
      "\u001B[32mTest loss decreased (97.344704 --> 95.945694).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 80.376.. \n",
      "\u001B[93mTest Loss: 99.758.. \n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 80.141.. \n",
      "\u001B[93mTest Loss: 98.157.. \n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 78.953.. \n",
      "\u001B[93mTest Loss: 95.530.. \n",
      "\u001B[32mTest loss decreased (95.945694 --> 95.530357).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 77.379.. \n",
      "\u001B[93mTest Loss: 95.469.. \n",
      "\u001B[32mTest loss decreased (95.530357 --> 95.469475).  Saving model ...\n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 76.632.. \n",
      "\u001B[93mTest Loss: 95.609.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 75.627.. \n",
      "\u001B[93mTest Loss: 95.845.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 73.541.. \n",
      "\u001B[93mTest Loss: 96.472.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 72.795.. \n",
      "\u001B[93mTest Loss: 93.816.. \n",
      "\u001B[32mTest loss decreased (95.469475 --> 93.815689).  Saving model ...\n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 70.485.. \n",
      "\u001B[93mTest Loss: 92.062.. \n",
      "\u001B[32mTest loss decreased (93.815689 --> 92.062263).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 70.037.. \n",
      "\u001B[93mTest Loss: 92.890.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 68.346.. \n",
      "\u001B[93mTest Loss: 95.061.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 67.201.. \n",
      "\u001B[93mTest Loss: 96.203.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 66.564.. \n",
      "\u001B[93mTest Loss: 93.059.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 65.020.. \n",
      "\u001B[93mTest Loss: 96.991.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 64.158.. \n",
      "\u001B[93mTest Loss: 90.245.. \n",
      "\u001B[32mTest loss decreased (92.062263 --> 90.244850).  Saving model ...\n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 62.084.. \n",
      "\u001B[93mTest Loss: 90.448.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 59.585.. \n",
      "\u001B[93mTest Loss: 94.909.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 58.451.. \n",
      "\u001B[93mTest Loss: 86.924.. \n",
      "\u001B[32mTest loss decreased (90.244850 --> 86.923592).  Saving model ...\n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 57.437.. \n",
      "\u001B[93mTest Loss: 93.927.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 57.451.. \n",
      "\u001B[93mTest Loss: 91.410.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 54.899.. \n",
      "\u001B[93mTest Loss: 89.497.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 55.043.. \n",
      "\u001B[93mTest Loss: 88.905.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 52.071.. \n",
      "\u001B[93mTest Loss: 88.282.. \n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 52.272.. \n",
      "\u001B[93mTest Loss: 82.572.. \n",
      "\u001B[32mTest loss decreased (86.923592 --> 82.572014).  Saving model ...\n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 48.430.. \n",
      "\u001B[93mTest Loss: 90.080.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 47.781.. \n",
      "\u001B[93mTest Loss: 84.393.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 48.258.. \n",
      "\u001B[93mTest Loss: 83.396.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 43.669.. \n",
      "\u001B[93mTest Loss: 81.886.. \n",
      "\u001B[32mTest loss decreased (82.572014 --> 81.886154).  Saving model ...\n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 44.354.. \n",
      "\u001B[93mTest Loss: 78.426.. \n",
      "\u001B[32mTest loss decreased (81.886154 --> 78.426010).  Saving model ...\n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 42.776.. \n",
      "\u001B[93mTest Loss: 82.062.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 41.088.. \n",
      "\u001B[93mTest Loss: 81.694.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 40.439.. \n",
      "\u001B[93mTest Loss: 77.617.. \n",
      "\u001B[32mTest loss decreased (78.426010 --> 77.617249).  Saving model ...\n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 40.106.. \n",
      "\u001B[93mTest Loss: 82.382.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 38.963.. \n",
      "\u001B[93mTest Loss: 82.500.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 37.069.. \n",
      "\u001B[93mTest Loss: 78.151.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 35.841.. \n",
      "\u001B[93mTest Loss: 80.566.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 39.196.. \n",
      "\u001B[93mTest Loss: 86.070.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 34.851.. \n",
      "\u001B[93mTest Loss: 80.695.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 34.135.. \n",
      "\u001B[93mTest Loss: 83.031.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 33.327.. \n",
      "\u001B[93mTest Loss: 77.841.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 32.730.. \n",
      "\u001B[93mTest Loss: 81.546.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 34.275.. \n",
      "\u001B[93mTest Loss: 74.667.. \n",
      "\u001B[32mTest loss decreased (77.617249 --> 74.666679).  Saving model ...\n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 32.829.. \n",
      "\u001B[93mTest Loss: 86.540.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 32.998.. \n",
      "\u001B[93mTest Loss: 73.438.. \n",
      "\u001B[32mTest loss decreased (74.666679 --> 73.437874).  Saving model ...\n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 30.930.. \n",
      "\u001B[93mTest Loss: 85.973.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 32.662.. \n",
      "\u001B[93mTest Loss: 82.087.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 32.274.. \n",
      "\u001B[93mTest Loss: 79.932.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 31.035.. \n",
      "\u001B[93mTest Loss: 77.498.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 29.215.. \n",
      "\u001B[93mTest Loss: 82.555.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 28.613.. \n",
      "\u001B[93mTest Loss: 87.301.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 30.945.. \n",
      "\u001B[93mTest Loss: 81.878.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 27.616.. \n",
      "\u001B[93mTest Loss: 76.615.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 29.193.. \n",
      "\u001B[93mTest Loss: 76.366.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 25.767.. \n",
      "\u001B[93mTest Loss: 77.902.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 27.454.. \n",
      "\u001B[93mTest Loss: 78.326.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 27.287.. \n",
      "\u001B[93mTest Loss: 79.548.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 25.667.. \n",
      "\u001B[93mTest Loss: 83.869.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 26.711.. \n",
      "\u001B[93mTest Loss: 80.191.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 25.321.. \n",
      "\u001B[93mTest Loss: 77.479.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 27.906.. \n",
      "\u001B[93mTest Loss: 75.034.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 23.962.. \n",
      "\u001B[93mTest Loss: 81.721.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 23.013.. \n",
      "\u001B[93mTest Loss: 77.701.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 24.313.. \n",
      "\u001B[93mTest Loss: 82.157.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 24.643.. \n",
      "\u001B[93mTest Loss: 80.250.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 26.236.. \n",
      "\u001B[93mTest Loss: 78.766.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 24.221.. \n",
      "\u001B[93mTest Loss: 84.109.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 27.017.. \n",
      "\u001B[93mTest Loss: 77.132.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 21.233.. \n",
      "\u001B[93mTest Loss: 76.557.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 20.801.. \n",
      "\u001B[93mTest Loss: 77.139.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 21.595.. \n",
      "\u001B[93mTest Loss: 77.626.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 23.376.. \n",
      "\u001B[93mTest Loss: 83.775.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 21.815.. \n",
      "\u001B[93mTest Loss: 83.241.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 21.344.. \n",
      "\u001B[93mTest Loss: 80.532.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 21.774.. \n",
      "\u001B[93mTest Loss: 77.282.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 20.818.. \n",
      "\u001B[93mTest Loss: 76.928.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 20.550.. \n",
      "\u001B[93mTest Loss: 81.046.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 20.564.. \n",
      "\u001B[93mTest Loss: 78.910.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 19.675.. \n",
      "\u001B[93mTest Loss: 75.801.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 18.913.. \n",
      "\u001B[93mTest Loss: 77.390.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 17.850.. \n",
      "\u001B[93mTest Loss: 75.404.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 19.433.. \n",
      "\u001B[93mTest Loss: 77.618.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 21.698.. \n",
      "\u001B[93mTest Loss: 77.383.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 21.726.. \n",
      "\u001B[93mTest Loss: 78.058.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 19.848.. \n",
      "\u001B[93mTest Loss: 74.960.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  141.9761712551117 seconds\n",
      "\u001B[32mLowest Test loss achieved for Standard Net: 69.090080\n",
      "\u001B[32mAt Epoch #73 \n",
      "\n",
      "\u001B[32mLowest Elastic-Net loss achieved: 68.817741\n",
      "\u001B[32mAt Epoch #69 \n",
      "\n",
      "\u001B[32mLowest Test loss achieved for Gradient-Net: 73.437874\n",
      "\u001B[32mAt Epoch #48 \n",
      "\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 100.050.. \n",
      "\u001B[93mTest Loss: 107.768.. \n",
      "\u001B[32mTest loss decreased (inf --> 107.767769).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 87.046.. \n",
      "\u001B[93mTest Loss: 102.283.. \n",
      "\u001B[32mTest loss decreased (107.767769 --> 102.282791).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 82.139.. \n",
      "\u001B[93mTest Loss: 101.485.. \n",
      "\u001B[32mTest loss decreased (102.282791 --> 101.484886).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 80.463.. \n",
      "\u001B[93mTest Loss: 99.034.. \n",
      "\u001B[32mTest loss decreased (101.484886 --> 99.034348).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 79.612.. \n",
      "\u001B[93mTest Loss: 96.974.. \n",
      "\u001B[32mTest loss decreased (99.034348 --> 96.974319).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 77.959.. \n",
      "\u001B[93mTest Loss: 95.044.. \n",
      "\u001B[32mTest loss decreased (96.974319 --> 95.044243).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 75.852.. \n",
      "\u001B[93mTest Loss: 95.210.. \n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 75.253.. \n",
      "\u001B[93mTest Loss: 94.676.. \n",
      "\u001B[32mTest loss decreased (95.044243 --> 94.675697).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 73.596.. \n",
      "\u001B[93mTest Loss: 98.763.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 73.049.. \n",
      "\u001B[93mTest Loss: 92.324.. \n",
      "\u001B[32mTest loss decreased (94.675697 --> 92.324097).  Saving model ...\n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 71.070.. \n",
      "\u001B[93mTest Loss: 93.103.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 70.592.. \n",
      "\u001B[93mTest Loss: 92.568.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 69.344.. \n",
      "\u001B[93mTest Loss: 94.259.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 68.274.. \n",
      "\u001B[93mTest Loss: 96.104.. \n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 66.826.. \n",
      "\u001B[93mTest Loss: 92.130.. \n",
      "\u001B[32mTest loss decreased (92.324097 --> 92.130409).  Saving model ...\n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 65.000.. \n",
      "\u001B[93mTest Loss: 93.305.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 65.066.. \n",
      "\u001B[93mTest Loss: 92.405.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 62.848.. \n",
      "\u001B[93mTest Loss: 92.320.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 62.786.. \n",
      "\u001B[93mTest Loss: 91.389.. \n",
      "\u001B[32mTest loss decreased (92.130409 --> 91.388893).  Saving model ...\n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 59.120.. \n",
      "\u001B[93mTest Loss: 95.050.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 59.261.. \n",
      "\u001B[93mTest Loss: 94.574.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 58.164.. \n",
      "\u001B[93mTest Loss: 89.786.. \n",
      "\u001B[32mTest loss decreased (91.388893 --> 89.785721).  Saving model ...\n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 56.376.. \n",
      "\u001B[93mTest Loss: 90.010.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 54.041.. \n",
      "\u001B[93mTest Loss: 88.981.. \n",
      "\u001B[32mTest loss decreased (89.785721 --> 88.980766).  Saving model ...\n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 53.322.. \n",
      "\u001B[93mTest Loss: 86.021.. \n",
      "\u001B[32mTest loss decreased (88.980766 --> 86.020760).  Saving model ...\n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 51.834.. \n",
      "\u001B[93mTest Loss: 86.256.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 50.472.. \n",
      "\u001B[93mTest Loss: 85.739.. \n",
      "\u001B[32mTest loss decreased (86.020760 --> 85.739067).  Saving model ...\n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 47.292.. \n",
      "\u001B[93mTest Loss: 84.656.. \n",
      "\u001B[32mTest loss decreased (85.739067 --> 84.655609).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 46.737.. \n",
      "\u001B[93mTest Loss: 89.259.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 44.718.. \n",
      "\u001B[93mTest Loss: 78.943.. \n",
      "\u001B[32mTest loss decreased (84.655609 --> 78.943474).  Saving model ...\n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 41.352.. \n",
      "\u001B[93mTest Loss: 80.699.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 42.073.. \n",
      "\u001B[93mTest Loss: 79.044.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 38.085.. \n",
      "\u001B[93mTest Loss: 82.665.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 36.722.. \n",
      "\u001B[93mTest Loss: 80.652.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 36.729.. \n",
      "\u001B[93mTest Loss: 76.159.. \n",
      "\u001B[32mTest loss decreased (78.943474 --> 76.158859).  Saving model ...\n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 34.866.. \n",
      "\u001B[93mTest Loss: 78.918.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 34.749.. \n",
      "\u001B[93mTest Loss: 78.808.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 35.442.. \n",
      "\u001B[93mTest Loss: 78.139.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 33.617.. \n",
      "\u001B[93mTest Loss: 74.417.. \n",
      "\u001B[32mTest loss decreased (76.158859 --> 74.416649).  Saving model ...\n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 30.854.. \n",
      "\u001B[93mTest Loss: 73.573.. \n",
      "\u001B[32mTest loss decreased (74.416649 --> 73.573006).  Saving model ...\n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 29.608.. \n",
      "\u001B[93mTest Loss: 76.804.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 28.717.. \n",
      "\u001B[93mTest Loss: 77.698.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 28.869.. \n",
      "\u001B[93mTest Loss: 82.219.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 29.272.. \n",
      "\u001B[93mTest Loss: 73.408.. \n",
      "\u001B[32mTest loss decreased (73.573006 --> 73.408134).  Saving model ...\n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 26.637.. \n",
      "\u001B[93mTest Loss: 73.853.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 27.811.. \n",
      "\u001B[93mTest Loss: 76.134.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 26.373.. \n",
      "\u001B[93mTest Loss: 75.294.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 27.201.. \n",
      "\u001B[93mTest Loss: 76.020.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 24.689.. \n",
      "\u001B[93mTest Loss: 77.046.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 24.470.. \n",
      "\u001B[93mTest Loss: 75.595.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 25.340.. \n",
      "\u001B[93mTest Loss: 80.248.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 24.114.. \n",
      "\u001B[93mTest Loss: 75.552.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 22.930.. \n",
      "\u001B[93mTest Loss: 81.916.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 23.856.. \n",
      "\u001B[93mTest Loss: 78.530.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 21.233.. \n",
      "\u001B[93mTest Loss: 73.046.. \n",
      "\u001B[32mTest loss decreased (73.408134 --> 73.046181).  Saving model ...\n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 20.674.. \n",
      "\u001B[93mTest Loss: 73.811.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 21.092.. \n",
      "\u001B[93mTest Loss: 75.309.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 20.466.. \n",
      "\u001B[93mTest Loss: 72.347.. \n",
      "\u001B[32mTest loss decreased (73.046181 --> 72.346756).  Saving model ...\n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 20.743.. \n",
      "\u001B[93mTest Loss: 72.224.. \n",
      "\u001B[32mTest loss decreased (72.346756 --> 72.223740).  Saving model ...\n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 21.782.. \n",
      "\u001B[93mTest Loss: 76.151.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 21.358.. \n",
      "\u001B[93mTest Loss: 73.171.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 19.355.. \n",
      "\u001B[93mTest Loss: 72.962.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 17.721.. \n",
      "\u001B[93mTest Loss: 71.558.. \n",
      "\u001B[32mTest loss decreased (72.223740 --> 71.557564).  Saving model ...\n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 17.482.. \n",
      "\u001B[93mTest Loss: 73.009.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 17.451.. \n",
      "\u001B[93mTest Loss: 70.822.. \n",
      "\u001B[32mTest loss decreased (71.557564 --> 70.821800).  Saving model ...\n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 20.726.. \n",
      "\u001B[93mTest Loss: 77.481.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 18.429.. \n",
      "\u001B[93mTest Loss: 72.679.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 16.249.. \n",
      "\u001B[93mTest Loss: 71.121.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 17.378.. \n",
      "\u001B[93mTest Loss: 74.334.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 20.281.. \n",
      "\u001B[93mTest Loss: 72.550.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 18.896.. \n",
      "\u001B[93mTest Loss: 71.059.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 14.439.. \n",
      "\u001B[93mTest Loss: 75.598.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 14.007.. \n",
      "\u001B[93mTest Loss: 70.697.. \n",
      "\u001B[32mTest loss decreased (70.821800 --> 70.697319).  Saving model ...\n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 15.143.. \n",
      "\u001B[93mTest Loss: 72.583.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 15.373.. \n",
      "\u001B[93mTest Loss: 71.028.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 15.909.. \n",
      "\u001B[93mTest Loss: 75.046.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 18.104.. \n",
      "\u001B[93mTest Loss: 72.909.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 17.070.. \n",
      "\u001B[93mTest Loss: 72.658.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 14.931.. \n",
      "\u001B[93mTest Loss: 71.011.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 12.789.. \n",
      "\u001B[93mTest Loss: 71.682.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 12.625.. \n",
      "\u001B[93mTest Loss: 70.505.. \n",
      "\u001B[32mTest loss decreased (70.697319 --> 70.504631).  Saving model ...\n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 12.426.. \n",
      "\u001B[93mTest Loss: 76.586.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 17.089.. \n",
      "\u001B[93mTest Loss: 75.861.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 14.372.. \n",
      "\u001B[93mTest Loss: 72.173.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 12.152.. \n",
      "\u001B[93mTest Loss: 69.997.. \n",
      "\u001B[32mTest loss decreased (70.504631 --> 69.997086).  Saving model ...\n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 11.529.. \n",
      "\u001B[93mTest Loss: 69.857.. \n",
      "\u001B[32mTest loss decreased (69.997086 --> 69.857285).  Saving model ...\n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 12.910.. \n",
      "\u001B[93mTest Loss: 80.723.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 17.409.. \n",
      "\u001B[93mTest Loss: 71.264.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 14.826.. \n",
      "\u001B[93mTest Loss: 74.788.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 12.822.. \n",
      "\u001B[93mTest Loss: 72.902.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 12.695.. \n",
      "\u001B[93mTest Loss: 73.296.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 10.663.. \n",
      "\u001B[93mTest Loss: 71.764.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 10.645.. \n",
      "\u001B[93mTest Loss: 72.036.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 12.474.. \n",
      "\u001B[93mTest Loss: 79.226.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 15.302.. \n",
      "\u001B[93mTest Loss: 73.274.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 11.298.. \n",
      "\u001B[93mTest Loss: 73.144.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 10.870.. \n",
      "\u001B[93mTest Loss: 73.564.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 11.278.. \n",
      "\u001B[93mTest Loss: 71.944.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 14.540.. \n",
      "\u001B[93mTest Loss: 72.709.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 10.506.. \n",
      "\u001B[93mTest Loss: 71.082.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 9.675.. \n",
      "\u001B[93mTest Loss: 71.206.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 9.641.. \n",
      "\u001B[93mTest Loss: 71.885.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 9.988.. \n",
      "\u001B[93mTest Loss: 71.647.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 11.092.. \n",
      "\u001B[93mTest Loss: 73.432.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 16.144.. \n",
      "\u001B[93mTest Loss: 74.042.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 11.115.. \n",
      "\u001B[93mTest Loss: 70.501.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 8.668.. \n",
      "\u001B[93mTest Loss: 69.854.. \n",
      "\u001B[32mTest loss decreased (69.857285 --> 69.854050).  Saving model ...\n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 8.593.. \n",
      "\u001B[93mTest Loss: 69.883.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 11.712.. \n",
      "\u001B[93mTest Loss: 71.688.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 10.020.. \n",
      "\u001B[93mTest Loss: 71.590.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 9.590.. \n",
      "\u001B[93mTest Loss: 77.409.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 13.131.. \n",
      "\u001B[93mTest Loss: 74.257.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 13.268.. \n",
      "\u001B[93mTest Loss: 70.506.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 12.841.. \n",
      "\u001B[93mTest Loss: 73.687.. \n",
      "\u001B[36mEpoch: 115/200..  Training Loss: 8.737.. \n",
      "\u001B[93mTest Loss: 70.608.. \n",
      "\u001B[36mEpoch: 116/200..  Training Loss: 7.549.. \n",
      "\u001B[93mTest Loss: 70.670.. \n",
      "\u001B[36mEpoch: 117/200..  Training Loss: 7.317.. \n",
      "\u001B[93mTest Loss: 70.978.. \n",
      "\u001B[36mEpoch: 118/200..  Training Loss: 7.709.. \n",
      "\u001B[93mTest Loss: 71.473.. \n",
      "\u001B[36mEpoch: 119/200..  Training Loss: 8.450.. \n",
      "\u001B[93mTest Loss: 73.200.. \n",
      "\u001B[36mEpoch: 120/200..  Training Loss: 9.447.. \n",
      "\u001B[93mTest Loss: 71.647.. \n",
      "\u001B[36mEpoch: 121/200..  Training Loss: 9.727.. \n",
      "\u001B[93mTest Loss: 72.685.. \n",
      "\u001B[36mEpoch: 122/200..  Training Loss: 14.508.. \n",
      "\u001B[93mTest Loss: 72.192.. \n",
      "\u001B[36mEpoch: 123/200..  Training Loss: 9.753.. \n",
      "\u001B[93mTest Loss: 74.678.. \n",
      "\u001B[36mEpoch: 124/200..  Training Loss: 8.404.. \n",
      "\u001B[93mTest Loss: 71.994.. \n",
      "\u001B[36mEpoch: 125/200..  Training Loss: 7.197.. \n",
      "\u001B[93mTest Loss: 70.891.. \n",
      "\u001B[36mEpoch: 126/200..  Training Loss: 6.951.. \n",
      "\u001B[93mTest Loss: 70.954.. \n",
      "\u001B[36mEpoch: 127/200..  Training Loss: 7.492.. \n",
      "\u001B[93mTest Loss: 69.499.. \n",
      "\u001B[32mTest loss decreased (69.854050 --> 69.499153).  Saving model ...\n",
      "\u001B[36mEpoch: 128/200..  Training Loss: 7.594.. \n",
      "\u001B[93mTest Loss: 71.127.. \n",
      "\u001B[36mEpoch: 129/200..  Training Loss: 11.789.. \n",
      "\u001B[93mTest Loss: 78.848.. \n",
      "\u001B[36mEpoch: 130/200..  Training Loss: 12.507.. \n",
      "\u001B[93mTest Loss: 70.607.. \n",
      "\u001B[36mEpoch: 131/200..  Training Loss: 7.766.. \n",
      "\u001B[93mTest Loss: 71.407.. \n",
      "\u001B[36mEpoch: 132/200..  Training Loss: 6.767.. \n",
      "\u001B[93mTest Loss: 71.387.. \n",
      "\u001B[36mEpoch: 133/200..  Training Loss: 6.586.. \n",
      "\u001B[93mTest Loss: 72.710.. \n",
      "\u001B[36mEpoch: 134/200..  Training Loss: 8.002.. \n",
      "\u001B[93mTest Loss: 72.083.. \n",
      "\u001B[36mEpoch: 135/200..  Training Loss: 7.703.. \n",
      "\u001B[93mTest Loss: 70.987.. \n",
      "\u001B[36mEpoch: 136/200..  Training Loss: 7.439.. \n",
      "\u001B[93mTest Loss: 76.539.. \n",
      "\u001B[36mEpoch: 137/200..  Training Loss: 14.386.. \n",
      "\u001B[93mTest Loss: 73.138.. \n",
      "\u001B[36mEpoch: 138/200..  Training Loss: 11.410.. \n",
      "\u001B[93mTest Loss: 72.010.. \n",
      "\u001B[36mEpoch: 139/200..  Training Loss: 8.735.. \n",
      "\u001B[93mTest Loss: 70.298.. \n",
      "\u001B[36mEpoch: 140/200..  Training Loss: 6.666.. \n",
      "\u001B[93mTest Loss: 71.331.. \n",
      "\u001B[36mEpoch: 141/200..  Training Loss: 6.172.. \n",
      "\u001B[93mTest Loss: 71.116.. \n",
      "\u001B[36mEpoch: 142/200..  Training Loss: 5.991.. \n",
      "\u001B[93mTest Loss: 71.090.. \n",
      "\u001B[36mEpoch: 143/200..  Training Loss: 6.307.. \n",
      "\u001B[93mTest Loss: 70.239.. \n",
      "\u001B[36mEpoch: 144/200..  Training Loss: 6.916.. \n",
      "\u001B[93mTest Loss: 72.516.. \n",
      "\u001B[36mEpoch: 145/200..  Training Loss: 7.519.. \n",
      "\u001B[93mTest Loss: 72.856.. \n",
      "\u001B[36mEpoch: 146/200..  Training Loss: 7.043.. \n",
      "\u001B[93mTest Loss: 71.567.. \n",
      "\u001B[36mEpoch: 147/200..  Training Loss: 6.693.. \n",
      "\u001B[93mTest Loss: 71.450.. \n",
      "\u001B[36mEpoch: 148/200..  Training Loss: 7.008.. \n",
      "\u001B[93mTest Loss: 74.382.. \n",
      "\u001B[36mEpoch: 149/200..  Training Loss: 14.134.. \n",
      "\u001B[93mTest Loss: 75.880.. \n",
      "\u001B[36mEpoch: 150/200..  Training Loss: 12.396.. \n",
      "\u001B[93mTest Loss: 73.389.. \n",
      "\u001B[36mEpoch: 151/200..  Training Loss: 7.467.. \n",
      "\u001B[93mTest Loss: 71.105.. \n",
      "\u001B[36mEpoch: 152/200..  Training Loss: 5.952.. \n",
      "\u001B[93mTest Loss: 70.873.. \n",
      "\u001B[36mEpoch: 153/200..  Training Loss: 5.973.. \n",
      "\u001B[93mTest Loss: 70.671.. \n",
      "\u001B[36mEpoch: 154/200..  Training Loss: 5.795.. \n",
      "\u001B[93mTest Loss: 71.613.. \n",
      "\u001B[36mEpoch: 155/200..  Training Loss: 5.644.. \n",
      "\u001B[93mTest Loss: 71.520.. \n",
      "\u001B[36mEpoch: 156/200..  Training Loss: 5.922.. \n",
      "\u001B[93mTest Loss: 72.044.. \n",
      "\u001B[36mEpoch: 157/200..  Training Loss: 6.098.. \n",
      "\u001B[93mTest Loss: 71.970.. \n",
      "\u001B[36mEpoch: 158/200..  Training Loss: 9.541.. \n",
      "\u001B[93mTest Loss: 76.186.. \n",
      "\u001B[36mEpoch: 159/200..  Training Loss: 10.364.. \n",
      "\u001B[93mTest Loss: 71.282.. \n",
      "\u001B[36mEpoch: 160/200..  Training Loss: 6.326.. \n",
      "\u001B[93mTest Loss: 71.557.. \n",
      "\u001B[36mEpoch: 161/200..  Training Loss: 6.380.. \n",
      "\u001B[93mTest Loss: 74.029.. \n",
      "\u001B[36mEpoch: 162/200..  Training Loss: 11.755.. \n",
      "\u001B[93mTest Loss: 73.093.. \n",
      "\u001B[36mEpoch: 163/200..  Training Loss: 7.856.. \n",
      "\u001B[93mTest Loss: 72.855.. \n",
      "\u001B[36mEpoch: 164/200..  Training Loss: 5.421.. \n",
      "\u001B[93mTest Loss: 71.045.. \n",
      "\u001B[36mEpoch: 165/200..  Training Loss: 4.947.. \n",
      "\u001B[93mTest Loss: 71.374.. \n",
      "\u001B[36mEpoch: 166/200..  Training Loss: 4.980.. \n",
      "\u001B[93mTest Loss: 71.561.. \n",
      "\u001B[36mEpoch: 167/200..  Training Loss: 5.673.. \n",
      "\u001B[93mTest Loss: 75.569.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  253.3046429157257 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 101.458.. \n",
      "\u001B[93mTest Loss: 106.059.. \n",
      "\u001B[32mTest loss decreased (inf --> 106.058647).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 80.110.. \n",
      "\u001B[93mTest Loss: 103.575.. \n",
      "\u001B[32mTest loss decreased (106.058647 --> 103.575218).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 76.497.. \n",
      "\u001B[93mTest Loss: 102.954.. \n",
      "\u001B[32mTest loss decreased (103.575218 --> 102.953972).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 74.720.. \n",
      "\u001B[93mTest Loss: 98.391.. \n",
      "\u001B[32mTest loss decreased (102.953972 --> 98.391220).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 72.912.. \n",
      "\u001B[93mTest Loss: 102.490.. \n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 72.654.. \n",
      "\u001B[93mTest Loss: 95.794.. \n",
      "\u001B[32mTest loss decreased (98.391220 --> 95.793808).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 71.018.. \n",
      "\u001B[93mTest Loss: 94.978.. \n",
      "\u001B[32mTest loss decreased (95.793808 --> 94.978172).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 69.117.. \n",
      "\u001B[93mTest Loss: 95.476.. \n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 69.030.. \n",
      "\u001B[93mTest Loss: 97.441.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 68.494.. \n",
      "\u001B[93mTest Loss: 97.602.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 68.030.. \n",
      "\u001B[93mTest Loss: 94.223.. \n",
      "\u001B[32mTest loss decreased (94.978172 --> 94.223198).  Saving model ...\n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 66.743.. \n",
      "\u001B[93mTest Loss: 95.617.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 66.390.. \n",
      "\u001B[93mTest Loss: 94.449.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 66.028.. \n",
      "\u001B[93mTest Loss: 93.653.. \n",
      "\u001B[32mTest loss decreased (94.223198 --> 93.652832).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 66.039.. \n",
      "\u001B[93mTest Loss: 92.485.. \n",
      "\u001B[32mTest loss decreased (93.652832 --> 92.484818).  Saving model ...\n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 64.608.. \n",
      "\u001B[93mTest Loss: 91.098.. \n",
      "\u001B[32mTest loss decreased (92.484818 --> 91.097656).  Saving model ...\n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 64.318.. \n",
      "\u001B[93mTest Loss: 88.975.. \n",
      "\u001B[32mTest loss decreased (91.097656 --> 88.974670).  Saving model ...\n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 63.438.. \n",
      "\u001B[93mTest Loss: 91.706.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 61.969.. \n",
      "\u001B[93mTest Loss: 86.120.. \n",
      "\u001B[32mTest loss decreased (88.974670 --> 86.120224).  Saving model ...\n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 61.932.. \n",
      "\u001B[93mTest Loss: 84.878.. \n",
      "\u001B[32mTest loss decreased (86.120224 --> 84.877716).  Saving model ...\n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 61.096.. \n",
      "\u001B[93mTest Loss: 84.963.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 59.516.. \n",
      "\u001B[93mTest Loss: 83.798.. \n",
      "\u001B[32mTest loss decreased (84.877716 --> 83.797729).  Saving model ...\n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 58.389.. \n",
      "\u001B[93mTest Loss: 85.026.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 57.129.. \n",
      "\u001B[93mTest Loss: 84.452.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 57.839.. \n",
      "\u001B[93mTest Loss: 81.935.. \n",
      "\u001B[32mTest loss decreased (83.797729 --> 81.935211).  Saving model ...\n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 55.309.. \n",
      "\u001B[93mTest Loss: 87.406.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 54.549.. \n",
      "\u001B[93mTest Loss: 77.949.. \n",
      "\u001B[32mTest loss decreased (81.935211 --> 77.949181).  Saving model ...\n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 54.587.. \n",
      "\u001B[93mTest Loss: 77.767.. \n",
      "\u001B[32mTest loss decreased (77.949181 --> 77.766640).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 53.937.. \n",
      "\u001B[93mTest Loss: 77.707.. \n",
      "\u001B[32mTest loss decreased (77.766640 --> 77.706848).  Saving model ...\n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 52.447.. \n",
      "\u001B[93mTest Loss: 78.009.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 53.029.. \n",
      "\u001B[93mTest Loss: 81.489.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 50.692.. \n",
      "\u001B[93mTest Loss: 74.885.. \n",
      "\u001B[32mTest loss decreased (77.706848 --> 74.884987).  Saving model ...\n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 51.814.. \n",
      "\u001B[93mTest Loss: 73.751.. \n",
      "\u001B[32mTest loss decreased (74.884987 --> 73.750648).  Saving model ...\n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 50.308.. \n",
      "\u001B[93mTest Loss: 76.478.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 48.921.. \n",
      "\u001B[93mTest Loss: 79.453.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 50.449.. \n",
      "\u001B[93mTest Loss: 78.741.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 51.124.. \n",
      "\u001B[93mTest Loss: 74.628.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 48.668.. \n",
      "\u001B[93mTest Loss: 71.642.. \n",
      "\u001B[32mTest loss decreased (73.750648 --> 71.642136).  Saving model ...\n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 48.247.. \n",
      "\u001B[93mTest Loss: 76.153.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 48.762.. \n",
      "\u001B[93mTest Loss: 78.896.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 47.954.. \n",
      "\u001B[93mTest Loss: 77.145.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 48.065.. \n",
      "\u001B[93mTest Loss: 80.076.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 46.255.. \n",
      "\u001B[93mTest Loss: 74.646.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 46.948.. \n",
      "\u001B[93mTest Loss: 82.044.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 47.728.. \n",
      "\u001B[93mTest Loss: 78.683.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 46.073.. \n",
      "\u001B[93mTest Loss: 73.370.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 48.526.. \n",
      "\u001B[93mTest Loss: 70.962.. \n",
      "\u001B[32mTest loss decreased (71.642136 --> 70.961662).  Saving model ...\n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 47.163.. \n",
      "\u001B[93mTest Loss: 75.220.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 46.486.. \n",
      "\u001B[93mTest Loss: 72.768.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 43.245.. \n",
      "\u001B[93mTest Loss: 72.307.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 43.566.. \n",
      "\u001B[93mTest Loss: 72.249.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 45.963.. \n",
      "\u001B[93mTest Loss: 78.142.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 46.112.. \n",
      "\u001B[93mTest Loss: 72.989.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 43.574.. \n",
      "\u001B[93mTest Loss: 72.526.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 45.032.. \n",
      "\u001B[93mTest Loss: 69.285.. \n",
      "\u001B[32mTest loss decreased (70.961662 --> 69.285416).  Saving model ...\n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 41.853.. \n",
      "\u001B[93mTest Loss: 70.058.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 42.919.. \n",
      "\u001B[93mTest Loss: 68.738.. \n",
      "\u001B[32mTest loss decreased (69.285416 --> 68.737640).  Saving model ...\n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 42.425.. \n",
      "\u001B[93mTest Loss: 76.559.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 45.289.. \n",
      "\u001B[93mTest Loss: 70.546.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 43.634.. \n",
      "\u001B[93mTest Loss: 72.598.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 44.140.. \n",
      "\u001B[93mTest Loss: 71.956.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 42.319.. \n",
      "\u001B[93mTest Loss: 72.390.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 43.626.. \n",
      "\u001B[93mTest Loss: 75.447.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 42.189.. \n",
      "\u001B[93mTest Loss: 70.546.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 40.959.. \n",
      "\u001B[93mTest Loss: 69.511.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 42.591.. \n",
      "\u001B[93mTest Loss: 73.391.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 40.166.. \n",
      "\u001B[93mTest Loss: 70.772.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 42.052.. \n",
      "\u001B[93mTest Loss: 80.546.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 48.032.. \n",
      "\u001B[93mTest Loss: 72.899.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 41.170.. \n",
      "\u001B[93mTest Loss: 71.406.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 39.961.. \n",
      "\u001B[93mTest Loss: 70.691.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 39.922.. \n",
      "\u001B[93mTest Loss: 70.847.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 39.630.. \n",
      "\u001B[93mTest Loss: 69.201.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 41.775.. \n",
      "\u001B[93mTest Loss: 76.670.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 43.047.. \n",
      "\u001B[93mTest Loss: 73.381.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 43.343.. \n",
      "\u001B[93mTest Loss: 68.738.. \n",
      "\u001B[32mTest loss decreased (68.737640 --> 68.737526).  Saving model ...\n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 38.295.. \n",
      "\u001B[93mTest Loss: 69.319.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 41.720.. \n",
      "\u001B[93mTest Loss: 71.604.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 37.348.. \n",
      "\u001B[93mTest Loss: 69.020.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 38.919.. \n",
      "\u001B[93mTest Loss: 68.708.. \n",
      "\u001B[32mTest loss decreased (68.737526 --> 68.708199).  Saving model ...\n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 40.032.. \n",
      "\u001B[93mTest Loss: 73.500.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 37.346.. \n",
      "\u001B[93mTest Loss: 68.876.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 38.674.. \n",
      "\u001B[93mTest Loss: 71.936.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 37.698.. \n",
      "\u001B[93mTest Loss: 70.090.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 36.600.. \n",
      "\u001B[93mTest Loss: 71.640.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 38.572.. \n",
      "\u001B[93mTest Loss: 80.109.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 43.686.. \n",
      "\u001B[93mTest Loss: 79.045.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 36.292.. \n",
      "\u001B[93mTest Loss: 67.673.. \n",
      "\u001B[32mTest loss decreased (68.708199 --> 67.672882).  Saving model ...\n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 35.161.. \n",
      "\u001B[93mTest Loss: 69.391.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 39.753.. \n",
      "\u001B[93mTest Loss: 73.940.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 37.269.. \n",
      "\u001B[93mTest Loss: 69.034.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 35.650.. \n",
      "\u001B[93mTest Loss: 71.981.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 41.842.. \n",
      "\u001B[93mTest Loss: 73.474.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 36.211.. \n",
      "\u001B[93mTest Loss: 76.162.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 35.294.. \n",
      "\u001B[93mTest Loss: 68.303.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 38.444.. \n",
      "\u001B[93mTest Loss: 70.354.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 35.921.. \n",
      "\u001B[93mTest Loss: 71.241.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 38.488.. \n",
      "\u001B[93mTest Loss: 87.696.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 38.973.. \n",
      "\u001B[93mTest Loss: 71.651.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 37.255.. \n",
      "\u001B[93mTest Loss: 73.059.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 35.772.. \n",
      "\u001B[93mTest Loss: 76.304.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 37.053.. \n",
      "\u001B[93mTest Loss: 74.535.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 35.655.. \n",
      "\u001B[93mTest Loss: 71.438.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 33.068.. \n",
      "\u001B[93mTest Loss: 77.009.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 36.216.. \n",
      "\u001B[93mTest Loss: 69.726.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 34.706.. \n",
      "\u001B[93mTest Loss: 78.757.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 38.216.. \n",
      "\u001B[93mTest Loss: 68.654.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 33.505.. \n",
      "\u001B[93mTest Loss: 70.766.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 34.930.. \n",
      "\u001B[93mTest Loss: 70.430.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 36.066.. \n",
      "\u001B[93mTest Loss: 69.685.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 32.392.. \n",
      "\u001B[93mTest Loss: 70.073.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 37.398.. \n",
      "\u001B[93mTest Loss: 71.170.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 33.431.. \n",
      "\u001B[93mTest Loss: 70.818.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 32.789.. \n",
      "\u001B[93mTest Loss: 68.230.. \n",
      "\u001B[36mEpoch: 115/200..  Training Loss: 31.482.. \n",
      "\u001B[93mTest Loss: 74.409.. \n",
      "\u001B[36mEpoch: 116/200..  Training Loss: 39.022.. \n",
      "\u001B[93mTest Loss: 71.306.. \n",
      "\u001B[36mEpoch: 117/200..  Training Loss: 32.025.. \n",
      "\u001B[93mTest Loss: 69.993.. \n",
      "\u001B[36mEpoch: 118/200..  Training Loss: 34.177.. \n",
      "\u001B[93mTest Loss: 76.009.. \n",
      "\u001B[36mEpoch: 119/200..  Training Loss: 35.731.. \n",
      "\u001B[93mTest Loss: 69.892.. \n",
      "\u001B[36mEpoch: 120/200..  Training Loss: 31.041.. \n",
      "\u001B[93mTest Loss: 73.518.. \n",
      "\u001B[36mEpoch: 121/200..  Training Loss: 35.503.. \n",
      "\u001B[93mTest Loss: 72.196.. \n",
      "\u001B[36mEpoch: 122/200..  Training Loss: 34.531.. \n",
      "\u001B[93mTest Loss: 73.060.. \n",
      "\u001B[36mEpoch: 123/200..  Training Loss: 33.776.. \n",
      "\u001B[93mTest Loss: 71.013.. \n",
      "\u001B[36mEpoch: 124/200..  Training Loss: 30.657.. \n",
      "\u001B[93mTest Loss: 68.874.. \n",
      "\u001B[36mEpoch: 125/200..  Training Loss: 30.829.. \n",
      "\u001B[93mTest Loss: 75.354.. \n",
      "\u001B[36mEpoch: 126/200..  Training Loss: 31.303.. \n",
      "\u001B[93mTest Loss: 75.735.. \n",
      "\u001B[36mEpoch: 127/200..  Training Loss: 32.562.. \n",
      "\u001B[93mTest Loss: 72.073.. \n",
      "\u001B[36mEpoch: 128/200..  Training Loss: 33.581.. \n",
      "\u001B[93mTest Loss: 70.708.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  230.4845049381256 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 101.351.. \n",
      "\u001B[93mTest Loss: 105.870.. \n",
      "\u001B[32mTest loss decreased (inf --> 105.869987).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 88.064.. \n",
      "\u001B[93mTest Loss: 102.770.. \n",
      "\u001B[32mTest loss decreased (105.869987 --> 102.770271).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 85.420.. \n",
      "\u001B[93mTest Loss: 98.420.. \n",
      "\u001B[32mTest loss decreased (102.770271 --> 98.419876).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 82.097.. \n",
      "\u001B[93mTest Loss: 99.276.. \n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 81.115.. \n",
      "\u001B[93mTest Loss: 99.320.. \n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 80.182.. \n",
      "\u001B[93mTest Loss: 97.800.. \n",
      "\u001B[32mTest loss decreased (98.419876 --> 97.800362).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 78.160.. \n",
      "\u001B[93mTest Loss: 96.796.. \n",
      "\u001B[32mTest loss decreased (97.800362 --> 96.796104).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 77.740.. \n",
      "\u001B[93mTest Loss: 96.878.. \n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 76.049.. \n",
      "\u001B[93mTest Loss: 97.380.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 76.895.. \n",
      "\u001B[93mTest Loss: 97.410.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 74.713.. \n",
      "\u001B[93mTest Loss: 97.231.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 73.699.. \n",
      "\u001B[93mTest Loss: 95.257.. \n",
      "\u001B[32mTest loss decreased (96.796104 --> 95.257439).  Saving model ...\n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 72.336.. \n",
      "\u001B[93mTest Loss: 104.978.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 71.531.. \n",
      "\u001B[93mTest Loss: 96.306.. \n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 69.668.. \n",
      "\u001B[93mTest Loss: 96.408.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 68.698.. \n",
      "\u001B[93mTest Loss: 96.864.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 66.304.. \n",
      "\u001B[93mTest Loss: 97.820.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 64.968.. \n",
      "\u001B[93mTest Loss: 97.946.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 65.493.. \n",
      "\u001B[93mTest Loss: 100.018.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 60.941.. \n",
      "\u001B[93mTest Loss: 91.843.. \n",
      "\u001B[32mTest loss decreased (95.257439 --> 91.843208).  Saving model ...\n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 61.411.. \n",
      "\u001B[93mTest Loss: 90.704.. \n",
      "\u001B[32mTest loss decreased (91.843208 --> 90.704208).  Saving model ...\n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 60.856.. \n",
      "\u001B[93mTest Loss: 90.951.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 59.048.. \n",
      "\u001B[93mTest Loss: 89.889.. \n",
      "\u001B[32mTest loss decreased (90.704208 --> 89.888542).  Saving model ...\n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 58.209.. \n",
      "\u001B[93mTest Loss: 90.105.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 56.481.. \n",
      "\u001B[93mTest Loss: 90.618.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 54.456.. \n",
      "\u001B[93mTest Loss: 89.981.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 52.506.. \n",
      "\u001B[93mTest Loss: 91.432.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 49.349.. \n",
      "\u001B[93mTest Loss: 85.228.. \n",
      "\u001B[32mTest loss decreased (89.888542 --> 85.228455).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 50.227.. \n",
      "\u001B[93mTest Loss: 88.303.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 47.005.. \n",
      "\u001B[93mTest Loss: 88.561.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 46.760.. \n",
      "\u001B[93mTest Loss: 85.381.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 44.267.. \n",
      "\u001B[93mTest Loss: 87.931.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 45.036.. \n",
      "\u001B[93mTest Loss: 79.879.. \n",
      "\u001B[32mTest loss decreased (85.228455 --> 79.878983).  Saving model ...\n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 41.413.. \n",
      "\u001B[93mTest Loss: 89.029.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 42.668.. \n",
      "\u001B[93mTest Loss: 79.976.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 38.580.. \n",
      "\u001B[93mTest Loss: 82.950.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 36.821.. \n",
      "\u001B[93mTest Loss: 91.273.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 37.069.. \n",
      "\u001B[93mTest Loss: 76.058.. \n",
      "\u001B[32mTest loss decreased (79.878983 --> 76.058167).  Saving model ...\n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 37.470.. \n",
      "\u001B[93mTest Loss: 79.633.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 34.289.. \n",
      "\u001B[93mTest Loss: 80.650.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 34.107.. \n",
      "\u001B[93mTest Loss: 79.028.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 37.247.. \n",
      "\u001B[93mTest Loss: 77.809.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 34.696.. \n",
      "\u001B[93mTest Loss: 85.831.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 34.319.. \n",
      "\u001B[93mTest Loss: 77.413.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 32.197.. \n",
      "\u001B[93mTest Loss: 80.144.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 30.217.. \n",
      "\u001B[93mTest Loss: 80.327.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 32.707.. \n",
      "\u001B[93mTest Loss: 79.649.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 31.931.. \n",
      "\u001B[93mTest Loss: 81.503.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 28.694.. \n",
      "\u001B[93mTest Loss: 78.969.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 30.268.. \n",
      "\u001B[93mTest Loss: 77.501.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 30.416.. \n",
      "\u001B[93mTest Loss: 78.890.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 29.006.. \n",
      "\u001B[93mTest Loss: 73.991.. \n",
      "\u001B[32mTest loss decreased (76.058167 --> 73.991470).  Saving model ...\n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 29.368.. \n",
      "\u001B[93mTest Loss: 79.101.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 27.585.. \n",
      "\u001B[93mTest Loss: 80.100.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 28.311.. \n",
      "\u001B[93mTest Loss: 75.699.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 28.456.. \n",
      "\u001B[93mTest Loss: 77.652.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 27.896.. \n",
      "\u001B[93mTest Loss: 80.201.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 26.907.. \n",
      "\u001B[93mTest Loss: 75.204.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 25.235.. \n",
      "\u001B[93mTest Loss: 88.314.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 26.673.. \n",
      "\u001B[93mTest Loss: 75.521.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 24.495.. \n",
      "\u001B[93mTest Loss: 84.555.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 27.659.. \n",
      "\u001B[93mTest Loss: 77.288.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 24.513.. \n",
      "\u001B[93mTest Loss: 81.489.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 24.967.. \n",
      "\u001B[93mTest Loss: 77.992.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 24.669.. \n",
      "\u001B[93mTest Loss: 75.878.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 23.351.. \n",
      "\u001B[93mTest Loss: 79.170.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 24.850.. \n",
      "\u001B[93mTest Loss: 75.684.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 24.076.. \n",
      "\u001B[93mTest Loss: 74.890.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 22.927.. \n",
      "\u001B[93mTest Loss: 80.308.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 22.544.. \n",
      "\u001B[93mTest Loss: 75.908.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 22.741.. \n",
      "\u001B[93mTest Loss: 78.799.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 23.191.. \n",
      "\u001B[93mTest Loss: 75.628.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 21.494.. \n",
      "\u001B[93mTest Loss: 76.314.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 19.590.. \n",
      "\u001B[93mTest Loss: 78.637.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 19.584.. \n",
      "\u001B[93mTest Loss: 76.340.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 20.403.. \n",
      "\u001B[93mTest Loss: 81.503.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 22.304.. \n",
      "\u001B[93mTest Loss: 78.972.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 23.065.. \n",
      "\u001B[93mTest Loss: 76.960.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 19.843.. \n",
      "\u001B[93mTest Loss: 78.054.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 18.419.. \n",
      "\u001B[93mTest Loss: 77.840.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 20.376.. \n",
      "\u001B[93mTest Loss: 75.890.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 18.165.. \n",
      "\u001B[93mTest Loss: 76.122.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 19.943.. \n",
      "\u001B[93mTest Loss: 79.662.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 20.043.. \n",
      "\u001B[93mTest Loss: 73.856.. \n",
      "\u001B[32mTest loss decreased (73.991470 --> 73.855804).  Saving model ...\n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 16.965.. \n",
      "\u001B[93mTest Loss: 74.753.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 16.718.. \n",
      "\u001B[93mTest Loss: 79.412.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 20.313.. \n",
      "\u001B[93mTest Loss: 76.998.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 21.141.. \n",
      "\u001B[93mTest Loss: 76.677.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 20.411.. \n",
      "\u001B[93mTest Loss: 76.257.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 17.293.. \n",
      "\u001B[93mTest Loss: 77.206.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 17.057.. \n",
      "\u001B[93mTest Loss: 73.992.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 15.560.. \n",
      "\u001B[93mTest Loss: 75.489.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 18.852.. \n",
      "\u001B[93mTest Loss: 75.351.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 15.728.. \n",
      "\u001B[93mTest Loss: 76.323.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 18.305.. \n",
      "\u001B[93mTest Loss: 76.120.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 16.039.. \n",
      "\u001B[93mTest Loss: 78.030.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 20.233.. \n",
      "\u001B[93mTest Loss: 80.106.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 17.671.. \n",
      "\u001B[93mTest Loss: 77.775.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 14.462.. \n",
      "\u001B[93mTest Loss: 75.844.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 13.521.. \n",
      "\u001B[93mTest Loss: 77.766.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 18.806.. \n",
      "\u001B[93mTest Loss: 76.912.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 19.594.. \n",
      "\u001B[93mTest Loss: 77.499.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 15.975.. \n",
      "\u001B[93mTest Loss: 75.348.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 18.281.. \n",
      "\u001B[93mTest Loss: 79.531.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 15.967.. \n",
      "\u001B[93mTest Loss: 78.609.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 16.478.. \n",
      "\u001B[93mTest Loss: 80.158.. \n",
      "\u001B[36mEpoch: 107/200..  Training Loss: 15.412.. \n",
      "\u001B[93mTest Loss: 75.444.. \n",
      "\u001B[36mEpoch: 108/200..  Training Loss: 13.519.. \n",
      "\u001B[93mTest Loss: 75.923.. \n",
      "\u001B[36mEpoch: 109/200..  Training Loss: 12.661.. \n",
      "\u001B[93mTest Loss: 74.966.. \n",
      "\u001B[36mEpoch: 110/200..  Training Loss: 13.387.. \n",
      "\u001B[93mTest Loss: 77.202.. \n",
      "\u001B[36mEpoch: 111/200..  Training Loss: 13.424.. \n",
      "\u001B[93mTest Loss: 76.045.. \n",
      "\u001B[36mEpoch: 112/200..  Training Loss: 14.142.. \n",
      "\u001B[93mTest Loss: 77.118.. \n",
      "\u001B[36mEpoch: 113/200..  Training Loss: 17.043.. \n",
      "\u001B[93mTest Loss: 77.020.. \n",
      "\u001B[36mEpoch: 114/200..  Training Loss: 16.220.. \n",
      "\u001B[93mTest Loss: 75.089.. \n",
      "\u001B[36mEpoch: 115/200..  Training Loss: 15.803.. \n",
      "\u001B[93mTest Loss: 84.547.. \n",
      "\u001B[36mEpoch: 116/200..  Training Loss: 17.010.. \n",
      "\u001B[93mTest Loss: 78.824.. \n",
      "\u001B[36mEpoch: 117/200..  Training Loss: 13.377.. \n",
      "\u001B[93mTest Loss: 77.215.. \n",
      "\u001B[36mEpoch: 118/200..  Training Loss: 12.299.. \n",
      "\u001B[93mTest Loss: 74.488.. \n",
      "\u001B[36mEpoch: 119/200..  Training Loss: 11.665.. \n",
      "\u001B[93mTest Loss: 73.727.. \n",
      "\u001B[32mTest loss decreased (73.855804 --> 73.727165).  Saving model ...\n",
      "\u001B[36mEpoch: 120/200..  Training Loss: 12.228.. \n",
      "\u001B[93mTest Loss: 79.805.. \n",
      "\u001B[36mEpoch: 121/200..  Training Loss: 19.781.. \n",
      "\u001B[93mTest Loss: 81.201.. \n",
      "\u001B[36mEpoch: 122/200..  Training Loss: 17.517.. \n",
      "\u001B[93mTest Loss: 80.198.. \n",
      "\u001B[36mEpoch: 123/200..  Training Loss: 15.134.. \n",
      "\u001B[93mTest Loss: 79.847.. \n",
      "\u001B[36mEpoch: 124/200..  Training Loss: 13.679.. \n",
      "\u001B[93mTest Loss: 77.504.. \n",
      "\u001B[36mEpoch: 125/200..  Training Loss: 11.878.. \n",
      "\u001B[93mTest Loss: 75.927.. \n",
      "\u001B[36mEpoch: 126/200..  Training Loss: 12.960.. \n",
      "\u001B[93mTest Loss: 76.566.. \n",
      "\u001B[36mEpoch: 127/200..  Training Loss: 11.424.. \n",
      "\u001B[93mTest Loss: 75.180.. \n",
      "\u001B[36mEpoch: 128/200..  Training Loss: 10.524.. \n",
      "\u001B[93mTest Loss: 75.532.. \n",
      "\u001B[36mEpoch: 129/200..  Training Loss: 10.530.. \n",
      "\u001B[93mTest Loss: 77.235.. \n",
      "\u001B[36mEpoch: 130/200..  Training Loss: 10.697.. \n",
      "\u001B[93mTest Loss: 76.364.. \n",
      "\u001B[36mEpoch: 131/200..  Training Loss: 16.309.. \n",
      "\u001B[93mTest Loss: 79.856.. \n",
      "\u001B[36mEpoch: 132/200..  Training Loss: 14.003.. \n",
      "\u001B[93mTest Loss: 80.732.. \n",
      "\u001B[36mEpoch: 133/200..  Training Loss: 11.197.. \n",
      "\u001B[93mTest Loss: 78.130.. \n",
      "\u001B[36mEpoch: 134/200..  Training Loss: 15.247.. \n",
      "\u001B[93mTest Loss: 77.389.. \n",
      "\u001B[36mEpoch: 135/200..  Training Loss: 13.362.. \n",
      "\u001B[93mTest Loss: 82.280.. \n",
      "\u001B[36mEpoch: 136/200..  Training Loss: 11.366.. \n",
      "\u001B[93mTest Loss: 75.930.. \n",
      "\u001B[36mEpoch: 137/200..  Training Loss: 10.887.. \n",
      "\u001B[93mTest Loss: 78.159.. \n",
      "\u001B[36mEpoch: 138/200..  Training Loss: 15.867.. \n",
      "\u001B[93mTest Loss: 76.528.. \n",
      "\u001B[36mEpoch: 139/200..  Training Loss: 11.694.. \n",
      "\u001B[93mTest Loss: 76.688.. \n",
      "\u001B[36mEpoch: 140/200..  Training Loss: 13.526.. \n",
      "\u001B[93mTest Loss: 78.694.. \n",
      "\u001B[36mEpoch: 141/200..  Training Loss: 11.772.. \n",
      "\u001B[93mTest Loss: 74.527.. \n",
      "\u001B[36mEpoch: 142/200..  Training Loss: 10.861.. \n",
      "\u001B[93mTest Loss: 80.001.. \n",
      "\u001B[36mEpoch: 143/200..  Training Loss: 10.177.. \n",
      "\u001B[93mTest Loss: 75.571.. \n",
      "\u001B[36mEpoch: 144/200..  Training Loss: 10.180.. \n",
      "\u001B[93mTest Loss: 73.052.. \n",
      "\u001B[32mTest loss decreased (73.727165 --> 73.051956).  Saving model ...\n",
      "\u001B[36mEpoch: 145/200..  Training Loss: 9.449.. \n",
      "\u001B[93mTest Loss: 76.357.. \n",
      "\u001B[36mEpoch: 146/200..  Training Loss: 9.588.. \n",
      "\u001B[93mTest Loss: 76.820.. \n",
      "\u001B[36mEpoch: 147/200..  Training Loss: 14.131.. \n",
      "\u001B[93mTest Loss: 79.382.. \n",
      "\u001B[36mEpoch: 148/200..  Training Loss: 14.701.. \n",
      "\u001B[93mTest Loss: 77.951.. \n",
      "\u001B[36mEpoch: 149/200..  Training Loss: 11.137.. \n",
      "\u001B[93mTest Loss: 78.211.. \n",
      "\u001B[36mEpoch: 150/200..  Training Loss: 11.811.. \n",
      "\u001B[93mTest Loss: 76.276.. \n",
      "\u001B[36mEpoch: 151/200..  Training Loss: 13.630.. \n",
      "\u001B[93mTest Loss: 77.088.. \n",
      "\u001B[36mEpoch: 152/200..  Training Loss: 10.466.. \n",
      "\u001B[93mTest Loss: 74.619.. \n",
      "\u001B[36mEpoch: 153/200..  Training Loss: 8.884.. \n",
      "\u001B[93mTest Loss: 74.874.. \n",
      "\u001B[36mEpoch: 154/200..  Training Loss: 8.387.. \n",
      "\u001B[93mTest Loss: 76.983.. \n",
      "\u001B[36mEpoch: 155/200..  Training Loss: 8.363.. \n",
      "\u001B[93mTest Loss: 75.189.. \n",
      "\u001B[36mEpoch: 156/200..  Training Loss: 8.509.. \n",
      "\u001B[93mTest Loss: 78.371.. \n",
      "\u001B[36mEpoch: 157/200..  Training Loss: 13.697.. \n",
      "\u001B[93mTest Loss: 77.501.. \n",
      "\u001B[36mEpoch: 158/200..  Training Loss: 13.950.. \n",
      "\u001B[93mTest Loss: 79.353.. \n",
      "\u001B[36mEpoch: 159/200..  Training Loss: 11.164.. \n",
      "\u001B[93mTest Loss: 79.010.. \n",
      "\u001B[36mEpoch: 160/200..  Training Loss: 9.736.. \n",
      "\u001B[93mTest Loss: 74.991.. \n",
      "\u001B[36mEpoch: 161/200..  Training Loss: 8.480.. \n",
      "\u001B[93mTest Loss: 74.617.. \n",
      "\u001B[36mEpoch: 162/200..  Training Loss: 13.048.. \n",
      "\u001B[93mTest Loss: 77.850.. \n",
      "\u001B[36mEpoch: 163/200..  Training Loss: 12.581.. \n",
      "\u001B[93mTest Loss: 76.759.. \n",
      "\u001B[36mEpoch: 164/200..  Training Loss: 9.033.. \n",
      "\u001B[93mTest Loss: 75.380.. \n",
      "\u001B[36mEpoch: 165/200..  Training Loss: 8.376.. \n",
      "\u001B[93mTest Loss: 79.764.. \n",
      "\u001B[36mEpoch: 166/200..  Training Loss: 12.717.. \n",
      "\u001B[93mTest Loss: 83.475.. \n",
      "\u001B[36mEpoch: 167/200..  Training Loss: 11.289.. \n",
      "\u001B[93mTest Loss: 77.037.. \n",
      "\u001B[36mEpoch: 168/200..  Training Loss: 8.831.. \n",
      "\u001B[93mTest Loss: 76.655.. \n",
      "\u001B[36mEpoch: 169/200..  Training Loss: 8.467.. \n",
      "\u001B[93mTest Loss: 76.306.. \n",
      "\u001B[36mEpoch: 170/200..  Training Loss: 10.671.. \n",
      "\u001B[93mTest Loss: 76.746.. \n",
      "\u001B[36mEpoch: 171/200..  Training Loss: 8.983.. \n",
      "\u001B[93mTest Loss: 77.197.. \n",
      "\u001B[36mEpoch: 172/200..  Training Loss: 8.426.. \n",
      "\u001B[93mTest Loss: 76.695.. \n",
      "\u001B[36mEpoch: 173/200..  Training Loss: 7.974.. \n",
      "\u001B[93mTest Loss: 75.863.. \n",
      "\u001B[36mEpoch: 174/200..  Training Loss: 7.758.. \n",
      "\u001B[93mTest Loss: 75.749.. \n",
      "\u001B[36mEpoch: 175/200..  Training Loss: 8.413.. \n",
      "\u001B[93mTest Loss: 78.145.. \n",
      "\u001B[36mEpoch: 176/200..  Training Loss: 12.179.. \n",
      "\u001B[93mTest Loss: 79.324.. \n",
      "\u001B[36mEpoch: 177/200..  Training Loss: 12.527.. \n",
      "\u001B[93mTest Loss: 78.709.. \n",
      "\u001B[36mEpoch: 178/200..  Training Loss: 10.002.. \n",
      "\u001B[93mTest Loss: 77.224.. \n",
      "\u001B[36mEpoch: 179/200..  Training Loss: 8.281.. \n",
      "\u001B[93mTest Loss: 77.309.. \n",
      "\u001B[36mEpoch: 180/200..  Training Loss: 9.291.. \n",
      "\u001B[93mTest Loss: 78.063.. \n",
      "\u001B[36mEpoch: 181/200..  Training Loss: 10.072.. \n",
      "\u001B[93mTest Loss: 82.545.. \n",
      "\u001B[36mEpoch: 182/200..  Training Loss: 10.177.. \n",
      "\u001B[93mTest Loss: 78.542.. \n",
      "\u001B[36mEpoch: 183/200..  Training Loss: 7.683.. \n",
      "\u001B[93mTest Loss: 76.478.. \n",
      "\u001B[36mEpoch: 184/200..  Training Loss: 7.039.. \n",
      "\u001B[93mTest Loss: 76.091.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  308.79874563217163 seconds\n",
      "\u001B[32mLowest Test loss achieved for Standard Net: 69.499153\n",
      "\u001B[32mAt Epoch #126 \n",
      "\n",
      "\u001B[32mLowest Elastic-Net loss achieved: 67.672882\n",
      "\u001B[32mAt Epoch #87 \n",
      "\n",
      "\u001B[32mLowest Test loss achieved for Gradient-Net: 73.051956\n",
      "\u001B[32mAt Epoch #143 \n",
      "\n",
      "\u001B[33mTime taken to run this code segment: 3500.88 seconds\n",
      "\u001B[34mOverall Average Weight Magnitude: tensor([0.2640, 0.2338, 0.3017, 0.2377, 0.2998, 0.2421, 0.3796, 0.2023, 0.1157,\n",
      "        0.2454, 0.1544, 0.2088, 0.1563, 0.1166, 0.1873, 0.1924, 0.1624, 0.2149,\n",
      "        0.1316, 0.1647, 0.1955, 0.1323, 0.1377, 0.2067, 0.1607, 0.1861, 0.1436,\n",
      "        0.1415, 0.2073, 0.2010, 0.1981, 0.1183, 0.1129, 0.2133, 0.2726, 0.2520,\n",
      "        0.2321, 0.2936, 0.2672, 0.2301], grad_fn=<MeanBackward1>)\n",
      "\u001B[34mRegularized Overall Average Weight Magnitude: tensor([0.0179, 0.0078, 0.0126, 0.0080, 0.0126, 0.0086, 0.0204, 0.0080, 0.0025,\n",
      "        0.0117, 0.0018, 0.0095, 0.0035, 0.0021, 0.0044, 0.0018, 0.0060, 0.0187,\n",
      "        0.0051, 0.0053, 0.0092, 0.0046, 0.0045, 0.0039, 0.0036, 0.0067, 0.0046,\n",
      "        0.0046, 0.0107, 0.0088, 0.0068, 0.0025, 0.0025, 0.0119, 0.0135, 0.0152,\n",
      "        0.0126, 0.0011, 0.0090, 0.0063], grad_fn=<MeanBackward1>)\n",
      "\u001B[34mGradient Overall Average Weight Magnitude: tensor([0.2364, 0.2537, 0.2751, 0.2830, 0.2572, 0.2850, 0.6354, 0.2614, 0.1447,\n",
      "        0.2368, 0.1835, 0.1821, 0.2577, 0.1525, 0.1690, 0.1247, 0.1717, 0.2458,\n",
      "        0.1981, 0.1690, 0.1917, 0.1685, 0.1673, 0.1785, 0.3213, 0.2069, 0.1425,\n",
      "        0.1430, 0.1938, 0.1919, 0.1854, 0.1431, 0.1459, 0.1977, 0.2473, 0.2805,\n",
      "        0.2190, 0.1465, 0.2145, 0.2139], grad_fn=<MeanBackward1>)\n",
      "\u001B[94mStandard-Net Weights of the 10 most important features:   Feature Name  Average Absolute Weight  Feature\n",
      "0         HAGL                 0.379594        6\n",
      "1            Y                 0.301659        2\n",
      "2          lat                 0.299827        4\n",
      "3      modPM10                 0.293638       37\n",
      "4        modNO                 0.272596       34\n",
      "5      modPM25                 0.267214       38\n",
      "6         hour                 0.264043        0\n",
      "7       modNO2                 0.251973       35\n",
      "8         atte                 0.245444        9\n",
      "9         HASL                 0.242133        5\n",
      "\u001B[94mRegularized-Net Weights of the 10 most important features:   Feature Name  Average Absolute Weight  Feature\n",
      "0         HAGL                 0.020393        6\n",
      "1         soim                 0.018714       17\n",
      "2         hour                 0.017885        0\n",
      "3       modNO2                 0.015159       35\n",
      "4        modNO                 0.013482       34\n",
      "5          lat                 0.012618        4\n",
      "6            Y                 0.012566        2\n",
      "7        modO3                 0.012560       36\n",
      "8        modCO                 0.011930       33\n",
      "9         atte                 0.011664        9\n",
      "\u001B[94mGradient-Net Weights of the 10 most important features:   Feature Name  Average Absolute Weight  Feature\n",
      "0         HAGL                 0.635417        6\n",
      "1         topo                 0.321257       24\n",
      "2         HASL                 0.284958        5\n",
      "3         long                 0.282980        3\n",
      "4       modNO2                 0.280506       35\n",
      "5            Y                 0.275115        2\n",
      "6         airm                 0.261434        7\n",
      "7          HGT                 0.257735       12\n",
      "8          lat                 0.257162        4\n",
      "9            X                 0.253650        1\n"
     ]
    }
   ],
   "source": [
    "initial_t = time.time()\n",
    "reg_parameter = 0.2 # regularization parameter for the Elastic-Net Regularized NN\n",
    "alpha = 0.5 # elastic net mixing parameter (L1 = 0.5, L2 = 0.5)\n",
    "n_top_features = 10 # for the Gradient-Net Feature Selection\n",
    "num_runs = 5\n",
    "# Tensors to store the average weight magnitudes from each run\n",
    "average_weight_abs_magnitudes = torch.zeros((input_dim, num_runs))\n",
    "reg_average_weight_abs_magnitudes = torch.zeros((input_dim, num_runs))\n",
    "grad_average_weight_abs_magnitudes = torch.zeros((input_dim, num_runs))\n",
    "\n",
    "print(Fore.CYAN + \"\", Net()) # printing the neural network architecture (which is the same for the gradient net)\n",
    "print(Fore.CYAN + \"\", RegularizedNet()) # printing the regularized neural network architecture\n",
    "for run in range(num_runs):\n",
    "    net, grad_net = Net().to(device), Net().to(device)\n",
    "    reg_net = RegularizedNet().to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    reg_optimizer = optim.Adam(reg_net.parameters(), lr=0.001)\n",
    "    grad_optimizer = optim.Adam(grad_net.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    reg_writer = SummaryWriter()\n",
    "    grad_writer = SummaryWriter()\n",
    "\n",
    "    # Train the regular neural network\n",
    "    net, test_loss_min, min_loss_epoch = train_regular_NN(net, train_loader, test_loader, criterion, optimizer, num_epochs, writer)\n",
    "    reg_net, reg_test_loss_min, reg_min_loss_epoch = train_elastic_net_NN(reg_net, reg_parameter, alpha, train_loader, test_loader, criterion, reg_optimizer, num_epochs, reg_writer)\n",
    "    grad_net, grad_test_loss_min, grad_min_loss_epoch = train_gradient_feature_selection_NN(grad_net, n_top_features, train_loader, test_loader, criterion, grad_optimizer, num_epochs, grad_writer)\n",
    "    print(Fore.GREEN + \"Lowest Test loss achieved for Standard Net: {:.6f}\".format(test_loss_min))\n",
    "    print(Fore.GREEN + \"At Epoch #{}\".format(min_loss_epoch), \"\\n\")\n",
    "    print(Fore.GREEN + \"Lowest Elastic-Net loss achieved: {:.6f}\".format(reg_test_loss_min))\n",
    "    print(Fore.GREEN + \"At Epoch #{}\".format(reg_min_loss_epoch), \"\\n\")\n",
    "    print(Fore.GREEN + \"Lowest Test loss achieved for Gradient-Net: {:.6f}\".format(grad_test_loss_min))\n",
    "    print(Fore.GREEN + \"At Epoch #{}\".format(grad_min_loss_epoch), \"\\n\")\n",
    "\n",
    "    # Extract and store weight magnitudes after training (excluding biases)\n",
    "    weights_abs = torch.abs(net.fc1.weight)\n",
    "    reg_weights_abs = torch.abs(reg_net.fc1.weight)\n",
    "    grad_weights_abs = torch.abs(grad_net.fc1.weight)\n",
    "    # now compute the average weight for each feature, it should result in a shape of (40,)\n",
    "    average_weight_abs_magnitudes[:, run] = torch.mean(weights_abs, dim=0)\n",
    "    reg_average_weight_abs_magnitudes[:, run] = torch.mean(reg_weights_abs, dim=0)\n",
    "    grad_average_weight_abs_magnitudes[:, run] = torch.mean(grad_weights_abs, dim=0)\n",
    "\n",
    "# Calculate the overall average weight magnitude\n",
    "overall_average_weight_magnitude = average_weight_abs_magnitudes.mean(dim=1)\n",
    "reg_overall_average_weight_magnitude = reg_average_weight_abs_magnitudes.mean(dim=1)\n",
    "grad_overall_average_weight_magnitude = grad_average_weight_abs_magnitudes.mean(dim=1)\n",
    "\n",
    "final_t = time.time()\n",
    "print(Fore.YELLOW + \"Time taken to run this code segment: {:.2f} seconds\".format(final_t - initial_t))\n",
    "\n",
    "# Print the results\n",
    "print(Fore.BLUE + \"Overall Average Weight Magnitude:\", overall_average_weight_magnitude)\n",
    "print(Fore.BLUE + \"Regularized Overall Average Weight Magnitude:\", reg_overall_average_weight_magnitude)\n",
    "print(Fore.BLUE + \"Gradient Overall Average Weight Magnitude:\", grad_overall_average_weight_magnitude)\n",
    "\n",
    "_, indices = torch.topk(overall_average_weight_magnitude, k) # getting the indices of the k most important features\n",
    "_, reg_indices = torch.topk(reg_overall_average_weight_magnitude, k)\n",
    "_, grad_indices = torch.topk(grad_overall_average_weight_magnitude, k)\n",
    "\n",
    "weights_fs = pd.DataFrame({'Feature': indices.detach().numpy(), 'Average Absolute Weight': overall_average_weight_magnitude[indices].detach().numpy()}) # creating a DataFrame with the indices of the 10 most important features and their corresponding absolute sum of weights\n",
    "\n",
    "reg_weights_fs = pd.DataFrame({'Feature': reg_indices.detach().numpy(), 'Average Absolute Weight': reg_overall_average_weight_magnitude[reg_indices].detach().numpy()})\n",
    "\n",
    "grad_weights_fs = pd.DataFrame({'Feature': grad_indices.detach().numpy(), 'Average Absolute Weight': grad_overall_average_weight_magnitude[grad_indices].detach().numpy()})\n",
    "\n",
    "weights_fs_names = X_test_copy.columns[indices] # getting the names of the 10 most important features\n",
    "reg_weights_fs_names = X_test_copy.columns[reg_indices]\n",
    "grad_weights_fs_names = X_test_copy.columns[grad_indices]\n",
    "\n",
    "weights_fs['Feature Name'] = weights_fs_names # adding the names of the 10 most important features to the DataFrame\n",
    "reg_weights_fs['Feature Name'] = reg_weights_fs_names\n",
    "grad_weights_fs['Feature Name'] = grad_weights_fs_names\n",
    "\n",
    "weights_fs = weights_fs[['Feature Name', 'Average Absolute Weight', 'Feature']] # rearranging the columns as Feature Name, Sum of Weights, Feature\n",
    "reg_weights_fs = reg_weights_fs[['Feature Name', 'Average Absolute Weight', 'Feature']]\n",
    "grad_weights_fs = grad_weights_fs[['Feature Name', 'Average Absolute Weight', 'Feature']]\n",
    "\n",
    "print(Fore.LIGHTBLUE_EX + \"Standard-Net Weights of the 10 most important features:\", weights_fs)\n",
    "print(Fore.LIGHTBLUE_EX + \"Regularized-Net Weights of the 10 most important features:\", reg_weights_fs)\n",
    "print(Fore.LIGHTBLUE_EX + \"Gradient-Net Weights of the 10 most important features:\", grad_weights_fs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "outputs": [],
   "source": [
    "# Creating train adn test sets corresponding to the different feature selection methods\n",
    "\n",
    "X_train_cfs, X_test_cfs = X_train_copy[correlation_fs_names], X_test_copy[correlation_fs_names] # Features selected by CFS\n",
    "X_train_elastic, X_test_elastic = X_train_copy[elastic_fs_names], X_test_copy[elastic_fs_names] # Features selected by Elastic-Net\n",
    "X_train_xgb, X_test_xgb = X_train_copy[xgb_fs_names], X_test_copy[xgb_fs_names] # Features selected by XGBoost\n",
    "X_train_xgb_perm, X_test_xgb_perm = X_train_copy[xgb_permutation_names], X_test_copy[xgb_permutation_names] # Features selected by XGBoost Permutation Importance\n",
    "X_train_weights_fs, X_test_weights_fs = X_train_copy[weights_fs_names], X_test_copy[weights_fs_names] # Features selected by Standard Weights-based Feature Selection\n",
    "X_train_reg_weights_fs, X_test_reg_weights_fs = X_train_copy[reg_weights_fs_names], X_test_copy[reg_weights_fs_names] # Features selected by Elastic-Net NN Regularization Weights\n",
    "X_train_grad_weights_fs, X_test_grad_weights_fs = X_train_copy[grad_weights_fs_names], X_test_copy[grad_weights_fs_names] # Features selected by Gradient-Net NN Regularized Weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training XGB with the features selected by the XGB model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "outputs": [
    {
     "data": {
      "text/plain": "XGBRegressor(booster='dart', eta=0.0, gamma=5, max_depth=8, min_child_weight=2,\n             num_parallel_tree=3, subsample=0.7)",
      "text/html": "<style>#sk-container-id-24 {color: black;background-color: white;}#sk-container-id-24 pre{padding: 0;}#sk-container-id-24 div.sk-toggleable {background-color: white;}#sk-container-id-24 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-24 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-24 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-24 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-24 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-24 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-24 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-24 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-24 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-24 div.sk-item {position: relative;z-index: 1;}#sk-container-id-24 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-24 div.sk-item::before, #sk-container-id-24 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-24 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-24 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-24 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-24 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-24 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-24 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-24 div.sk-label-container {text-align: center;}#sk-container-id-24 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-24 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(booster=&#x27;dart&#x27;, eta=0.0, gamma=5, max_depth=8, min_child_weight=2,\n             num_parallel_tree=3, subsample=0.7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" checked><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(booster=&#x27;dart&#x27;, eta=0.0, gamma=5, max_depth=8, min_child_weight=2,\n             num_parallel_tree=3, subsample=0.7)</pre></div></div></div></div></div>"
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if pollutant == 'NO2':\n",
    "    xgb_fs = XGBRegressor(booster = 'dart', eta = 0.0, gamma = 5, max_depth = 8, min_child_weight = 2, num_parallel_tree = 3, reg_lambda = 1, reg_alpha = 0, subsample = 0.7, colsample_bytree = 1, n_estimators = 100, random_state = 0)\n",
    "    xgb_permutation_fs = XGBRegressor(booster = 'dart', eta = 0.0, gamma = 5, max_depth = 8, min_child_weight = 2, num_parallel_tree = 3, reg_lambda = 1, reg_alpha = 0, subsample = 0.7, colsample_bytree = 1, n_estimators = 100, random_state = 0)\n",
    "else: # pollutant == 'PM2.5'\n",
    "    xgb_fs = XGBRegressor(booster = 'dart', gamma = 0, max_depth = 8, min_child_weight = 1, num_parallel_tree = 1, reg_lambda = 1, reg_alpha = 0.5, subsample = 1.0, colsample_bytree = 0.75, n_estimators = 200, random_state = 0)\n",
    "    xgb_permutation_fs = XGBRegressor(booster = 'dart', gamma = 0, max_depth = 8, min_child_weight = 1, num_parallel_tree = 1, reg_lambda = 1, reg_alpha = 0.5, subsample = 1.0, colsample_bytree = 0.75, n_estimators = 200, random_state = 0)\n",
    "\n",
    "\n",
    "xgb_fs.fit(X_train_xgb, y_train) # fitting the model\n",
    "xgb_permutation_fs.fit(X_train_xgb_perm, y_train) # fitting the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The rest of the fetures subsets obtained by the other feature selection methods are used to train NNs:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "outputs": [],
   "source": [
    "# we need to adjust the Train and Test Loaders because of reduced number of features\n",
    "X_train_cfs, X_test_cfs = numpy_to_tensor(X_train_cfs.to_numpy()), numpy_to_tensor(X_test_cfs.to_numpy())\n",
    "X_train_elastic, X_test_elastic = numpy_to_tensor(X_train_elastic.to_numpy()), numpy_to_tensor(X_test_elastic.to_numpy())\n",
    "X_train_weights_fs, X_test_weights_fs = numpy_to_tensor(X_train_weights_fs.to_numpy()), numpy_to_tensor(X_test_weights_fs.to_numpy())\n",
    "X_train_reg_weights_fs, X_test_reg_weights_fs = numpy_to_tensor(X_train_reg_weights_fs.to_numpy()), numpy_to_tensor(X_test_reg_weights_fs.to_numpy())\n",
    "X_train_grad_weights_fs, X_test_grad_weights_fs = numpy_to_tensor(X_train_grad_weights_fs.to_numpy()), numpy_to_tensor(X_test_grad_weights_fs.to_numpy())\n",
    "\n",
    "cfs_train = TensorDataset(X_train_cfs, y_train)\n",
    "cfs_test = TensorDataset(X_test_cfs, y_test)\n",
    "cfs_train_loader = DataLoader(cfs_train, batch_size=batch_size, shuffle=True)\n",
    "cfs_test_loader = DataLoader(cfs_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "elastic_train = TensorDataset(X_train_elastic, y_train)\n",
    "elastic_test = TensorDataset(X_test_elastic, y_test)\n",
    "elastic_train_loader = DataLoader(elastic_train, batch_size=batch_size, shuffle=True)\n",
    "elastic_test_loader = DataLoader(elastic_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "weights_fs_train = TensorDataset(X_train_weights_fs, y_train)\n",
    "weights_fs_test = TensorDataset(X_test_weights_fs, y_test)\n",
    "weights_fs_train_loader = DataLoader(weights_fs_train, batch_size=batch_size, shuffle=True)\n",
    "weights_fs_test_loader = DataLoader(weights_fs_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "reg_weights_fs_train = TensorDataset(X_train_reg_weights_fs, y_train)\n",
    "reg_weights_fs_test = TensorDataset(X_test_reg_weights_fs, y_test)\n",
    "reg_weights_fs_train_loader = DataLoader(reg_weights_fs_train, batch_size=batch_size, shuffle=True)\n",
    "reg_weights_fs_test_loader = DataLoader(reg_weights_fs_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "grad_weights_fs_train = TensorDataset(X_train_grad_weights_fs, y_train)\n",
    "grad_weights_fs_test = TensorDataset(X_test_grad_weights_fs, y_test)\n",
    "grad_weights_fs_train_loader = DataLoader(grad_weights_fs_train, batch_size=batch_size, shuffle=True)\n",
    "grad_weights_fs_test_loader = DataLoader(grad_weights_fs_test, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m FS_Net(\n",
      "  (fc1): Linear(in_features=10, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc4): Linear(in_features=200, out_features=1, bias=True)\n",
      ")\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 109.605.. \n",
      "\u001B[93mTest Loss: 120.285.. \n",
      "\u001B[32mTest loss decreased (inf --> 120.284554).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 105.249.. \n",
      "\u001B[93mTest Loss: 118.824.. \n",
      "\u001B[32mTest loss decreased (120.284554 --> 118.823647).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 103.209.. \n",
      "\u001B[93mTest Loss: 121.006.. \n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 102.083.. \n",
      "\u001B[93mTest Loss: 118.453.. \n",
      "\u001B[32mTest loss decreased (118.823647 --> 118.452911).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 100.863.. \n",
      "\u001B[93mTest Loss: 117.804.. \n",
      "\u001B[32mTest loss decreased (118.452911 --> 117.804253).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 99.792.. \n",
      "\u001B[93mTest Loss: 116.086.. \n",
      "\u001B[32mTest loss decreased (117.804253 --> 116.085876).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 99.507.. \n",
      "\u001B[93mTest Loss: 115.895.. \n",
      "\u001B[32mTest loss decreased (116.085876 --> 115.895401).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 99.289.. \n",
      "\u001B[93mTest Loss: 115.705.. \n",
      "\u001B[32mTest loss decreased (115.895401 --> 115.704590).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 97.962.. \n",
      "\u001B[93mTest Loss: 116.978.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 97.970.. \n",
      "\u001B[93mTest Loss: 116.183.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 97.354.. \n",
      "\u001B[93mTest Loss: 114.335.. \n",
      "\u001B[32mTest loss decreased (115.704590 --> 114.334877).  Saving model ...\n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 96.994.. \n",
      "\u001B[93mTest Loss: 114.144.. \n",
      "\u001B[32mTest loss decreased (114.334877 --> 114.143883).  Saving model ...\n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 96.371.. \n",
      "\u001B[93mTest Loss: 114.196.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 95.889.. \n",
      "\u001B[93mTest Loss: 113.043.. \n",
      "\u001B[32mTest loss decreased (114.143883 --> 113.042831).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 95.609.. \n",
      "\u001B[93mTest Loss: 113.405.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 95.106.. \n",
      "\u001B[93mTest Loss: 113.459.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 94.792.. \n",
      "\u001B[93mTest Loss: 113.257.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 94.113.. \n",
      "\u001B[93mTest Loss: 113.900.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 94.401.. \n",
      "\u001B[93mTest Loss: 113.995.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 93.200.. \n",
      "\u001B[93mTest Loss: 114.104.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 93.533.. \n",
      "\u001B[93mTest Loss: 113.605.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 93.048.. \n",
      "\u001B[93mTest Loss: 113.352.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 92.193.. \n",
      "\u001B[93mTest Loss: 114.930.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 91.574.. \n",
      "\u001B[93mTest Loss: 115.385.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 91.158.. \n",
      "\u001B[93mTest Loss: 114.109.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 91.137.. \n",
      "\u001B[93mTest Loss: 115.777.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 90.913.. \n",
      "\u001B[93mTest Loss: 115.204.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 90.333.. \n",
      "\u001B[93mTest Loss: 115.088.. \n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 89.925.. \n",
      "\u001B[93mTest Loss: 114.430.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 89.691.. \n",
      "\u001B[93mTest Loss: 115.300.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 89.533.. \n",
      "\u001B[93mTest Loss: 114.803.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 88.648.. \n",
      "\u001B[93mTest Loss: 115.109.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 88.938.. \n",
      "\u001B[93mTest Loss: 115.678.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 88.433.. \n",
      "\u001B[93mTest Loss: 115.341.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 87.423.. \n",
      "\u001B[93mTest Loss: 122.180.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 87.173.. \n",
      "\u001B[93mTest Loss: 116.521.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 86.794.. \n",
      "\u001B[93mTest Loss: 117.736.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 86.147.. \n",
      "\u001B[93mTest Loss: 115.709.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 86.178.. \n",
      "\u001B[93mTest Loss: 114.928.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 85.284.. \n",
      "\u001B[93mTest Loss: 116.536.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 85.460.. \n",
      "\u001B[93mTest Loss: 115.971.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 85.038.. \n",
      "\u001B[93mTest Loss: 116.247.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 84.795.. \n",
      "\u001B[93mTest Loss: 118.028.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 83.610.. \n",
      "\u001B[93mTest Loss: 120.028.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 84.078.. \n",
      "\u001B[93mTest Loss: 118.644.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 83.866.. \n",
      "\u001B[93mTest Loss: 116.554.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 82.547.. \n",
      "\u001B[93mTest Loss: 119.294.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 82.711.. \n",
      "\u001B[93mTest Loss: 117.733.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 82.482.. \n",
      "\u001B[93mTest Loss: 118.863.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 82.193.. \n",
      "\u001B[93mTest Loss: 119.377.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 80.770.. \n",
      "\u001B[93mTest Loss: 117.980.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 81.455.. \n",
      "\u001B[93mTest Loss: 119.477.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 80.698.. \n",
      "\u001B[93mTest Loss: 118.972.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 80.568.. \n",
      "\u001B[93mTest Loss: 118.879.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  145.40107440948486 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 100.541.. \n",
      "\u001B[93mTest Loss: 107.744.. \n",
      "\u001B[32mTest loss decreased (inf --> 107.744179).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 89.703.. \n",
      "\u001B[93mTest Loss: 104.966.. \n",
      "\u001B[32mTest loss decreased (107.744179 --> 104.965858).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 86.732.. \n",
      "\u001B[93mTest Loss: 103.555.. \n",
      "\u001B[32mTest loss decreased (104.965858 --> 103.554680).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 84.447.. \n",
      "\u001B[93mTest Loss: 103.405.. \n",
      "\u001B[32mTest loss decreased (103.554680 --> 103.405319).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 82.594.. \n",
      "\u001B[93mTest Loss: 98.938.. \n",
      "\u001B[32mTest loss decreased (103.405319 --> 98.937515).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 80.083.. \n",
      "\u001B[93mTest Loss: 98.812.. \n",
      "\u001B[32mTest loss decreased (98.937515 --> 98.812386).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 79.706.. \n",
      "\u001B[93mTest Loss: 94.983.. \n",
      "\u001B[32mTest loss decreased (98.812386 --> 94.983185).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 77.906.. \n",
      "\u001B[93mTest Loss: 95.306.. \n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 77.360.. \n",
      "\u001B[93mTest Loss: 94.870.. \n",
      "\u001B[32mTest loss decreased (94.983185 --> 94.869843).  Saving model ...\n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 76.288.. \n",
      "\u001B[93mTest Loss: 91.346.. \n",
      "\u001B[32mTest loss decreased (94.869843 --> 91.346100).  Saving model ...\n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 74.453.. \n",
      "\u001B[93mTest Loss: 92.804.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 73.059.. \n",
      "\u001B[93mTest Loss: 90.929.. \n",
      "\u001B[32mTest loss decreased (91.346100 --> 90.928650).  Saving model ...\n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 71.243.. \n",
      "\u001B[93mTest Loss: 91.193.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 71.334.. \n",
      "\u001B[93mTest Loss: 88.972.. \n",
      "\u001B[32mTest loss decreased (90.928650 --> 88.971779).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 69.520.. \n",
      "\u001B[93mTest Loss: 89.006.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 67.159.. \n",
      "\u001B[93mTest Loss: 86.507.. \n",
      "\u001B[32mTest loss decreased (88.971779 --> 86.507324).  Saving model ...\n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 69.314.. \n",
      "\u001B[93mTest Loss: 90.469.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 65.572.. \n",
      "\u001B[93mTest Loss: 84.080.. \n",
      "\u001B[32mTest loss decreased (86.507324 --> 84.079712).  Saving model ...\n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 65.255.. \n",
      "\u001B[93mTest Loss: 84.465.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 62.904.. \n",
      "\u001B[93mTest Loss: 86.036.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 62.241.. \n",
      "\u001B[93mTest Loss: 85.228.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 62.364.. \n",
      "\u001B[93mTest Loss: 86.163.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 59.789.. \n",
      "\u001B[93mTest Loss: 84.939.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 58.870.. \n",
      "\u001B[93mTest Loss: 82.530.. \n",
      "\u001B[32mTest loss decreased (84.079712 --> 82.529602).  Saving model ...\n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 59.602.. \n",
      "\u001B[93mTest Loss: 98.536.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 56.285.. \n",
      "\u001B[93mTest Loss: 84.489.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 56.581.. \n",
      "\u001B[93mTest Loss: 90.422.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 54.048.. \n",
      "\u001B[93mTest Loss: 84.551.. \n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 53.130.. \n",
      "\u001B[93mTest Loss: 82.523.. \n",
      "\u001B[32mTest loss decreased (82.529602 --> 82.523018).  Saving model ...\n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 51.897.. \n",
      "\u001B[93mTest Loss: 80.441.. \n",
      "\u001B[32mTest loss decreased (82.523018 --> 80.440613).  Saving model ...\n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 53.679.. \n",
      "\u001B[93mTest Loss: 81.229.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 50.910.. \n",
      "\u001B[93mTest Loss: 83.745.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 50.722.. \n",
      "\u001B[93mTest Loss: 83.390.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 48.948.. \n",
      "\u001B[93mTest Loss: 82.648.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 48.597.. \n",
      "\u001B[93mTest Loss: 78.326.. \n",
      "\u001B[32mTest loss decreased (80.440613 --> 78.325844).  Saving model ...\n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 47.564.. \n",
      "\u001B[93mTest Loss: 80.881.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 46.840.. \n",
      "\u001B[93mTest Loss: 80.409.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 48.484.. \n",
      "\u001B[93mTest Loss: 81.595.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 46.277.. \n",
      "\u001B[93mTest Loss: 79.142.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 46.561.. \n",
      "\u001B[93mTest Loss: 82.816.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 44.220.. \n",
      "\u001B[93mTest Loss: 81.929.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 44.664.. \n",
      "\u001B[93mTest Loss: 79.181.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 43.532.. \n",
      "\u001B[93mTest Loss: 78.067.. \n",
      "\u001B[32mTest loss decreased (78.325844 --> 78.066902).  Saving model ...\n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 43.018.. \n",
      "\u001B[93mTest Loss: 76.259.. \n",
      "\u001B[32mTest loss decreased (78.066902 --> 76.258965).  Saving model ...\n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 42.219.. \n",
      "\u001B[93mTest Loss: 78.623.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 42.377.. \n",
      "\u001B[93mTest Loss: 81.283.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 41.795.. \n",
      "\u001B[93mTest Loss: 78.523.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 41.281.. \n",
      "\u001B[93mTest Loss: 82.327.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 41.056.. \n",
      "\u001B[93mTest Loss: 81.437.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 38.748.. \n",
      "\u001B[93mTest Loss: 77.006.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 40.587.. \n",
      "\u001B[93mTest Loss: 79.229.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 37.351.. \n",
      "\u001B[93mTest Loss: 80.811.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 37.451.. \n",
      "\u001B[93mTest Loss: 79.216.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 38.229.. \n",
      "\u001B[93mTest Loss: 79.068.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 38.596.. \n",
      "\u001B[93mTest Loss: 79.183.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 38.163.. \n",
      "\u001B[93mTest Loss: 80.585.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 36.249.. \n",
      "\u001B[93mTest Loss: 78.812.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 35.978.. \n",
      "\u001B[93mTest Loss: 80.345.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 36.337.. \n",
      "\u001B[93mTest Loss: 79.648.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 34.523.. \n",
      "\u001B[93mTest Loss: 86.377.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 34.855.. \n",
      "\u001B[93mTest Loss: 80.702.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 34.401.. \n",
      "\u001B[93mTest Loss: 78.437.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 33.235.. \n",
      "\u001B[93mTest Loss: 81.731.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 33.242.. \n",
      "\u001B[93mTest Loss: 86.911.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 34.331.. \n",
      "\u001B[93mTest Loss: 79.127.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 32.499.. \n",
      "\u001B[93mTest Loss: 85.282.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 32.167.. \n",
      "\u001B[93mTest Loss: 89.350.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 32.463.. \n",
      "\u001B[93mTest Loss: 80.028.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 31.383.. \n",
      "\u001B[93mTest Loss: 81.797.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 32.283.. \n",
      "\u001B[93mTest Loss: 84.333.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 30.072.. \n",
      "\u001B[93mTest Loss: 79.853.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 29.700.. \n",
      "\u001B[93mTest Loss: 80.363.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 30.983.. \n",
      "\u001B[93mTest Loss: 82.989.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 30.898.. \n",
      "\u001B[93mTest Loss: 78.205.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 28.816.. \n",
      "\u001B[93mTest Loss: 83.660.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 29.169.. \n",
      "\u001B[93mTest Loss: 83.436.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 28.427.. \n",
      "\u001B[93mTest Loss: 82.328.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 30.617.. \n",
      "\u001B[93mTest Loss: 83.245.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 28.323.. \n",
      "\u001B[93mTest Loss: 82.190.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 30.005.. \n",
      "\u001B[93mTest Loss: 80.182.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 27.615.. \n",
      "\u001B[93mTest Loss: 81.783.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 27.849.. \n",
      "\u001B[93mTest Loss: 82.143.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 28.276.. \n",
      "\u001B[93mTest Loss: 83.717.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 26.367.. \n",
      "\u001B[93mTest Loss: 80.827.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  191.61672282218933 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 106.229.. \n",
      "\u001B[93mTest Loss: 106.759.. \n",
      "\u001B[32mTest loss decreased (inf --> 106.758560).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 90.165.. \n",
      "\u001B[93mTest Loss: 104.514.. \n",
      "\u001B[32mTest loss decreased (106.758560 --> 104.514069).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 86.954.. \n",
      "\u001B[93mTest Loss: 105.744.. \n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 84.464.. \n",
      "\u001B[93mTest Loss: 99.940.. \n",
      "\u001B[32mTest loss decreased (104.514069 --> 99.939575).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 82.234.. \n",
      "\u001B[93mTest Loss: 98.540.. \n",
      "\u001B[32mTest loss decreased (99.939575 --> 98.540337).  Saving model ...\n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 80.217.. \n",
      "\u001B[93mTest Loss: 98.030.. \n",
      "\u001B[32mTest loss decreased (98.540337 --> 98.030281).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 76.672.. \n",
      "\u001B[93mTest Loss: 93.391.. \n",
      "\u001B[32mTest loss decreased (98.030281 --> 93.391098).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 73.338.. \n",
      "\u001B[93mTest Loss: 87.775.. \n",
      "\u001B[32mTest loss decreased (93.391098 --> 87.775482).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 68.560.. \n",
      "\u001B[93mTest Loss: 93.000.. \n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 67.242.. \n",
      "\u001B[93mTest Loss: 85.131.. \n",
      "\u001B[32mTest loss decreased (87.775482 --> 85.130951).  Saving model ...\n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 64.953.. \n",
      "\u001B[93mTest Loss: 86.173.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 62.088.. \n",
      "\u001B[93mTest Loss: 79.718.. \n",
      "\u001B[32mTest loss decreased (85.130951 --> 79.717690).  Saving model ...\n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 60.403.. \n",
      "\u001B[93mTest Loss: 86.796.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 59.291.. \n",
      "\u001B[93mTest Loss: 86.030.. \n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 58.189.. \n",
      "\u001B[93mTest Loss: 81.100.. \n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 56.635.. \n",
      "\u001B[93mTest Loss: 76.752.. \n",
      "\u001B[32mTest loss decreased (79.717690 --> 76.752441).  Saving model ...\n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 55.756.. \n",
      "\u001B[93mTest Loss: 78.427.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 55.590.. \n",
      "\u001B[93mTest Loss: 75.369.. \n",
      "\u001B[32mTest loss decreased (76.752441 --> 75.368591).  Saving model ...\n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 54.118.. \n",
      "\u001B[93mTest Loss: 78.241.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 54.862.. \n",
      "\u001B[93mTest Loss: 86.698.. \n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 53.178.. \n",
      "\u001B[93mTest Loss: 76.451.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 51.810.. \n",
      "\u001B[93mTest Loss: 79.704.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 52.979.. \n",
      "\u001B[93mTest Loss: 77.716.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 51.162.. \n",
      "\u001B[93mTest Loss: 89.923.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 51.153.. \n",
      "\u001B[93mTest Loss: 79.791.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 51.078.. \n",
      "\u001B[93mTest Loss: 75.320.. \n",
      "\u001B[32mTest loss decreased (75.368591 --> 75.320099).  Saving model ...\n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 50.510.. \n",
      "\u001B[93mTest Loss: 76.696.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 49.571.. \n",
      "\u001B[93mTest Loss: 76.968.. \n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 50.998.. \n",
      "\u001B[93mTest Loss: 75.019.. \n",
      "\u001B[32mTest loss decreased (75.320099 --> 75.018974).  Saving model ...\n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 50.288.. \n",
      "\u001B[93mTest Loss: 74.210.. \n",
      "\u001B[32mTest loss decreased (75.018974 --> 74.209709).  Saving model ...\n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 49.035.. \n",
      "\u001B[93mTest Loss: 74.313.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 48.621.. \n",
      "\u001B[93mTest Loss: 75.056.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 47.776.. \n",
      "\u001B[93mTest Loss: 84.813.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 47.876.. \n",
      "\u001B[93mTest Loss: 78.561.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 47.101.. \n",
      "\u001B[93mTest Loss: 78.358.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 47.079.. \n",
      "\u001B[93mTest Loss: 78.203.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 46.503.. \n",
      "\u001B[93mTest Loss: 79.675.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 46.151.. \n",
      "\u001B[93mTest Loss: 77.778.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 46.891.. \n",
      "\u001B[93mTest Loss: 77.230.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 45.290.. \n",
      "\u001B[93mTest Loss: 76.477.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 45.154.. \n",
      "\u001B[93mTest Loss: 77.717.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 45.394.. \n",
      "\u001B[93mTest Loss: 75.666.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 44.065.. \n",
      "\u001B[93mTest Loss: 73.678.. \n",
      "\u001B[32mTest loss decreased (74.209709 --> 73.678024).  Saving model ...\n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 44.659.. \n",
      "\u001B[93mTest Loss: 76.490.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 43.425.. \n",
      "\u001B[93mTest Loss: 75.722.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 43.427.. \n",
      "\u001B[93mTest Loss: 73.740.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 42.308.. \n",
      "\u001B[93mTest Loss: 74.694.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 42.383.. \n",
      "\u001B[93mTest Loss: 75.641.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 42.242.. \n",
      "\u001B[93mTest Loss: 75.260.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 42.039.. \n",
      "\u001B[93mTest Loss: 77.038.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 42.000.. \n",
      "\u001B[93mTest Loss: 79.282.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 41.506.. \n",
      "\u001B[93mTest Loss: 74.979.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 41.774.. \n",
      "\u001B[93mTest Loss: 77.077.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 40.861.. \n",
      "\u001B[93mTest Loss: 74.962.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 40.607.. \n",
      "\u001B[93mTest Loss: 80.822.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 39.829.. \n",
      "\u001B[93mTest Loss: 78.458.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 39.612.. \n",
      "\u001B[93mTest Loss: 84.387.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 39.604.. \n",
      "\u001B[93mTest Loss: 78.286.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 39.272.. \n",
      "\u001B[93mTest Loss: 77.039.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 39.249.. \n",
      "\u001B[93mTest Loss: 75.163.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 37.890.. \n",
      "\u001B[93mTest Loss: 77.433.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 38.668.. \n",
      "\u001B[93mTest Loss: 76.508.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 37.224.. \n",
      "\u001B[93mTest Loss: 77.735.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 37.676.. \n",
      "\u001B[93mTest Loss: 79.661.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 37.044.. \n",
      "\u001B[93mTest Loss: 73.972.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 36.780.. \n",
      "\u001B[93mTest Loss: 73.435.. \n",
      "\u001B[32mTest loss decreased (73.678024 --> 73.434898).  Saving model ...\n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 37.141.. \n",
      "\u001B[93mTest Loss: 76.632.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 36.289.. \n",
      "\u001B[93mTest Loss: 74.849.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 36.111.. \n",
      "\u001B[93mTest Loss: 77.567.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 35.390.. \n",
      "\u001B[93mTest Loss: 76.340.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 35.781.. \n",
      "\u001B[93mTest Loss: 81.307.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 36.525.. \n",
      "\u001B[93mTest Loss: 75.753.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 34.805.. \n",
      "\u001B[93mTest Loss: 76.099.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 34.310.. \n",
      "\u001B[93mTest Loss: 77.952.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 34.178.. \n",
      "\u001B[93mTest Loss: 77.480.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 33.960.. \n",
      "\u001B[93mTest Loss: 78.360.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 33.931.. \n",
      "\u001B[93mTest Loss: 79.765.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 34.385.. \n",
      "\u001B[93mTest Loss: 77.262.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 34.942.. \n",
      "\u001B[93mTest Loss: 75.252.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 33.702.. \n",
      "\u001B[93mTest Loss: 78.300.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 31.981.. \n",
      "\u001B[93mTest Loss: 77.034.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 31.725.. \n",
      "\u001B[93mTest Loss: 77.415.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 31.615.. \n",
      "\u001B[93mTest Loss: 80.842.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 32.097.. \n",
      "\u001B[93mTest Loss: 78.909.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 31.816.. \n",
      "\u001B[93mTest Loss: 80.650.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 32.491.. \n",
      "\u001B[93mTest Loss: 77.206.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 30.549.. \n",
      "\u001B[93mTest Loss: 80.879.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 31.270.. \n",
      "\u001B[93mTest Loss: 79.439.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 32.086.. \n",
      "\u001B[93mTest Loss: 83.239.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 30.602.. \n",
      "\u001B[93mTest Loss: 76.259.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 29.734.. \n",
      "\u001B[93mTest Loss: 79.326.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 29.778.. \n",
      "\u001B[93mTest Loss: 79.709.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 29.624.. \n",
      "\u001B[93mTest Loss: 79.236.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 28.869.. \n",
      "\u001B[93mTest Loss: 78.398.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 28.627.. \n",
      "\u001B[93mTest Loss: 78.278.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 30.871.. \n",
      "\u001B[93mTest Loss: 79.868.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 28.384.. \n",
      "\u001B[93mTest Loss: 81.217.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 28.366.. \n",
      "\u001B[93mTest Loss: 81.697.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 28.045.. \n",
      "\u001B[93mTest Loss: 81.814.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 29.206.. \n",
      "\u001B[93mTest Loss: 79.702.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 29.803.. \n",
      "\u001B[93mTest Loss: 77.086.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 28.787.. \n",
      "\u001B[93mTest Loss: 78.167.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 27.513.. \n",
      "\u001B[93mTest Loss: 84.214.. \n",
      "\u001B[36mEpoch: 104/200..  Training Loss: 27.251.. \n",
      "\u001B[93mTest Loss: 80.594.. \n",
      "\u001B[36mEpoch: 105/200..  Training Loss: 27.106.. \n",
      "\u001B[93mTest Loss: 79.764.. \n",
      "\u001B[36mEpoch: 106/200..  Training Loss: 26.833.. \n",
      "\u001B[93mTest Loss: 84.671.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  263.7361092567444 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 98.790.. \n",
      "\u001B[93mTest Loss: 105.308.. \n",
      "\u001B[32mTest loss decreased (inf --> 105.307938).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 87.247.. \n",
      "\u001B[93mTest Loss: 103.156.. \n",
      "\u001B[32mTest loss decreased (105.307938 --> 103.155914).  Saving model ...\n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 84.513.. \n",
      "\u001B[93mTest Loss: 100.279.. \n",
      "\u001B[32mTest loss decreased (103.155914 --> 100.279007).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 82.226.. \n",
      "\u001B[93mTest Loss: 100.405.. \n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 80.594.. \n",
      "\u001B[93mTest Loss: 108.683.. \n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 79.684.. \n",
      "\u001B[93mTest Loss: 96.060.. \n",
      "\u001B[32mTest loss decreased (100.279007 --> 96.059792).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 77.646.. \n",
      "\u001B[93mTest Loss: 95.313.. \n",
      "\u001B[32mTest loss decreased (96.059792 --> 95.312920).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 75.310.. \n",
      "\u001B[93mTest Loss: 96.187.. \n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 75.066.. \n",
      "\u001B[93mTest Loss: 94.002.. \n",
      "\u001B[32mTest loss decreased (95.312920 --> 94.001556).  Saving model ...\n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 73.804.. \n",
      "\u001B[93mTest Loss: 94.155.. \n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 72.442.. \n",
      "\u001B[93mTest Loss: 95.511.. \n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 70.640.. \n",
      "\u001B[93mTest Loss: 91.111.. \n",
      "\u001B[32mTest loss decreased (94.001556 --> 91.110802).  Saving model ...\n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 70.220.. \n",
      "\u001B[93mTest Loss: 93.243.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 67.809.. \n",
      "\u001B[93mTest Loss: 90.720.. \n",
      "\u001B[32mTest loss decreased (91.110802 --> 90.719879).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 65.897.. \n",
      "\u001B[93mTest Loss: 86.567.. \n",
      "\u001B[32mTest loss decreased (90.719879 --> 86.567062).  Saving model ...\n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 63.957.. \n",
      "\u001B[93mTest Loss: 86.696.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 62.033.. \n",
      "\u001B[93mTest Loss: 85.198.. \n",
      "\u001B[32mTest loss decreased (86.567062 --> 85.198250).  Saving model ...\n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 60.732.. \n",
      "\u001B[93mTest Loss: 84.231.. \n",
      "\u001B[32mTest loss decreased (85.198250 --> 84.230927).  Saving model ...\n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 58.304.. \n",
      "\u001B[93mTest Loss: 85.111.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 57.797.. \n",
      "\u001B[93mTest Loss: 82.022.. \n",
      "\u001B[32mTest loss decreased (84.230927 --> 82.022438).  Saving model ...\n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 55.130.. \n",
      "\u001B[93mTest Loss: 92.137.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 56.233.. \n",
      "\u001B[93mTest Loss: 82.164.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 53.782.. \n",
      "\u001B[93mTest Loss: 82.212.. \n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 52.088.. \n",
      "\u001B[93mTest Loss: 80.103.. \n",
      "\u001B[32mTest loss decreased (82.022438 --> 80.102921).  Saving model ...\n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 51.769.. \n",
      "\u001B[93mTest Loss: 82.602.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 49.712.. \n",
      "\u001B[93mTest Loss: 82.104.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 50.974.. \n",
      "\u001B[93mTest Loss: 78.945.. \n",
      "\u001B[32mTest loss decreased (80.102921 --> 78.944977).  Saving model ...\n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 49.412.. \n",
      "\u001B[93mTest Loss: 78.945.. \n",
      "\u001B[32mTest loss decreased (78.944977 --> 78.944824).  Saving model ...\n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 47.676.. \n",
      "\u001B[93mTest Loss: 82.273.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 46.956.. \n",
      "\u001B[93mTest Loss: 79.659.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 46.244.. \n",
      "\u001B[93mTest Loss: 78.164.. \n",
      "\u001B[32mTest loss decreased (78.944824 --> 78.164413).  Saving model ...\n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 46.990.. \n",
      "\u001B[93mTest Loss: 78.832.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 45.116.. \n",
      "\u001B[93mTest Loss: 80.822.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 44.559.. \n",
      "\u001B[93mTest Loss: 79.811.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 44.539.. \n",
      "\u001B[93mTest Loss: 88.010.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 44.845.. \n",
      "\u001B[93mTest Loss: 78.791.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 43.665.. \n",
      "\u001B[93mTest Loss: 80.064.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 42.303.. \n",
      "\u001B[93mTest Loss: 80.671.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 42.628.. \n",
      "\u001B[93mTest Loss: 78.856.. \n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 40.819.. \n",
      "\u001B[93mTest Loss: 78.273.. \n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 40.565.. \n",
      "\u001B[93mTest Loss: 78.070.. \n",
      "\u001B[32mTest loss decreased (78.164413 --> 78.069847).  Saving model ...\n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 39.925.. \n",
      "\u001B[93mTest Loss: 82.724.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 40.578.. \n",
      "\u001B[93mTest Loss: 78.511.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 39.294.. \n",
      "\u001B[93mTest Loss: 80.212.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 38.744.. \n",
      "\u001B[93mTest Loss: 86.337.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 37.677.. \n",
      "\u001B[93mTest Loss: 79.806.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 39.282.. \n",
      "\u001B[93mTest Loss: 79.153.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 38.040.. \n",
      "\u001B[93mTest Loss: 82.088.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 37.150.. \n",
      "\u001B[93mTest Loss: 78.479.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 38.158.. \n",
      "\u001B[93mTest Loss: 79.726.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 36.650.. \n",
      "\u001B[93mTest Loss: 82.462.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 36.525.. \n",
      "\u001B[93mTest Loss: 78.963.. \n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 35.604.. \n",
      "\u001B[93mTest Loss: 80.065.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 35.560.. \n",
      "\u001B[93mTest Loss: 84.688.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 35.840.. \n",
      "\u001B[93mTest Loss: 78.274.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 35.402.. \n",
      "\u001B[93mTest Loss: 79.144.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 34.121.. \n",
      "\u001B[93mTest Loss: 78.859.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 33.490.. \n",
      "\u001B[93mTest Loss: 82.320.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 34.379.. \n",
      "\u001B[93mTest Loss: 84.907.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 34.014.. \n",
      "\u001B[93mTest Loss: 83.654.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 34.793.. \n",
      "\u001B[93mTest Loss: 79.882.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 32.331.. \n",
      "\u001B[93mTest Loss: 78.686.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 32.969.. \n",
      "\u001B[93mTest Loss: 77.417.. \n",
      "\u001B[32mTest loss decreased (78.069847 --> 77.416878).  Saving model ...\n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 32.834.. \n",
      "\u001B[93mTest Loss: 80.508.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 32.854.. \n",
      "\u001B[93mTest Loss: 81.445.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 32.711.. \n",
      "\u001B[93mTest Loss: 85.328.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 32.013.. \n",
      "\u001B[93mTest Loss: 79.365.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 31.465.. \n",
      "\u001B[93mTest Loss: 82.072.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 30.704.. \n",
      "\u001B[93mTest Loss: 79.353.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 30.791.. \n",
      "\u001B[93mTest Loss: 78.904.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 30.161.. \n",
      "\u001B[93mTest Loss: 84.506.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 30.973.. \n",
      "\u001B[93mTest Loss: 83.895.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 30.365.. \n",
      "\u001B[93mTest Loss: 82.481.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 28.742.. \n",
      "\u001B[93mTest Loss: 82.115.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 29.152.. \n",
      "\u001B[93mTest Loss: 82.584.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 29.012.. \n",
      "\u001B[93mTest Loss: 84.707.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 29.560.. \n",
      "\u001B[93mTest Loss: 83.625.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 28.677.. \n",
      "\u001B[93mTest Loss: 83.231.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 27.769.. \n",
      "\u001B[93mTest Loss: 81.426.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 29.364.. \n",
      "\u001B[93mTest Loss: 85.412.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 27.416.. \n",
      "\u001B[93mTest Loss: 81.504.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 27.554.. \n",
      "\u001B[93mTest Loss: 81.227.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 27.806.. \n",
      "\u001B[93mTest Loss: 81.062.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 26.507.. \n",
      "\u001B[93mTest Loss: 80.257.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 27.784.. \n",
      "\u001B[93mTest Loss: 83.489.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 26.873.. \n",
      "\u001B[93mTest Loss: 87.994.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 28.777.. \n",
      "\u001B[93mTest Loss: 81.187.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 27.034.. \n",
      "\u001B[93mTest Loss: 78.772.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 25.184.. \n",
      "\u001B[93mTest Loss: 81.925.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 26.772.. \n",
      "\u001B[93mTest Loss: 87.650.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 26.214.. \n",
      "\u001B[93mTest Loss: 81.479.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 25.515.. \n",
      "\u001B[93mTest Loss: 81.544.. \n",
      "\u001B[36mEpoch: 93/200..  Training Loss: 25.944.. \n",
      "\u001B[93mTest Loss: 84.940.. \n",
      "\u001B[36mEpoch: 94/200..  Training Loss: 25.134.. \n",
      "\u001B[93mTest Loss: 82.570.. \n",
      "\u001B[36mEpoch: 95/200..  Training Loss: 24.017.. \n",
      "\u001B[93mTest Loss: 82.435.. \n",
      "\u001B[36mEpoch: 96/200..  Training Loss: 24.239.. \n",
      "\u001B[93mTest Loss: 82.455.. \n",
      "\u001B[36mEpoch: 97/200..  Training Loss: 25.236.. \n",
      "\u001B[93mTest Loss: 82.061.. \n",
      "\u001B[36mEpoch: 98/200..  Training Loss: 25.466.. \n",
      "\u001B[93mTest Loss: 81.282.. \n",
      "\u001B[36mEpoch: 99/200..  Training Loss: 25.876.. \n",
      "\u001B[93mTest Loss: 83.543.. \n",
      "\u001B[36mEpoch: 100/200..  Training Loss: 24.826.. \n",
      "\u001B[93mTest Loss: 80.854.. \n",
      "\u001B[36mEpoch: 101/200..  Training Loss: 24.698.. \n",
      "\u001B[93mTest Loss: 81.090.. \n",
      "\u001B[36mEpoch: 102/200..  Training Loss: 24.342.. \n",
      "\u001B[93mTest Loss: 82.431.. \n",
      "\u001B[36mEpoch: 103/200..  Training Loss: 24.269.. \n",
      "\u001B[93mTest Loss: 81.686.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  233.86724066734314 seconds\n",
      "\u001B[36mEpoch: 1/200..  Training Loss: 104.041.. \n",
      "\u001B[93mTest Loss: 111.890.. \n",
      "\u001B[32mTest loss decreased (inf --> 111.890251).  Saving model ...\n",
      "\u001B[36mEpoch: 2/200..  Training Loss: 93.085.. \n",
      "\u001B[93mTest Loss: 116.953.. \n",
      "\u001B[36mEpoch: 3/200..  Training Loss: 92.178.. \n",
      "\u001B[93mTest Loss: 108.201.. \n",
      "\u001B[32mTest loss decreased (111.890251 --> 108.201263).  Saving model ...\n",
      "\u001B[36mEpoch: 4/200..  Training Loss: 90.215.. \n",
      "\u001B[93mTest Loss: 106.500.. \n",
      "\u001B[32mTest loss decreased (108.201263 --> 106.500191).  Saving model ...\n",
      "\u001B[36mEpoch: 5/200..  Training Loss: 89.038.. \n",
      "\u001B[93mTest Loss: 109.709.. \n",
      "\u001B[36mEpoch: 6/200..  Training Loss: 87.657.. \n",
      "\u001B[93mTest Loss: 106.229.. \n",
      "\u001B[32mTest loss decreased (106.500191 --> 106.229164).  Saving model ...\n",
      "\u001B[36mEpoch: 7/200..  Training Loss: 85.947.. \n",
      "\u001B[93mTest Loss: 102.597.. \n",
      "\u001B[32mTest loss decreased (106.229164 --> 102.596664).  Saving model ...\n",
      "\u001B[36mEpoch: 8/200..  Training Loss: 83.680.. \n",
      "\u001B[93mTest Loss: 100.033.. \n",
      "\u001B[32mTest loss decreased (102.596664 --> 100.033195).  Saving model ...\n",
      "\u001B[36mEpoch: 9/200..  Training Loss: 80.741.. \n",
      "\u001B[93mTest Loss: 95.467.. \n",
      "\u001B[32mTest loss decreased (100.033195 --> 95.467049).  Saving model ...\n",
      "\u001B[36mEpoch: 10/200..  Training Loss: 76.257.. \n",
      "\u001B[93mTest Loss: 92.541.. \n",
      "\u001B[32mTest loss decreased (95.467049 --> 92.541290).  Saving model ...\n",
      "\u001B[36mEpoch: 11/200..  Training Loss: 73.257.. \n",
      "\u001B[93mTest Loss: 89.308.. \n",
      "\u001B[32mTest loss decreased (92.541290 --> 89.308311).  Saving model ...\n",
      "\u001B[36mEpoch: 12/200..  Training Loss: 71.165.. \n",
      "\u001B[93mTest Loss: 97.575.. \n",
      "\u001B[36mEpoch: 13/200..  Training Loss: 69.255.. \n",
      "\u001B[93mTest Loss: 91.259.. \n",
      "\u001B[36mEpoch: 14/200..  Training Loss: 68.557.. \n",
      "\u001B[93mTest Loss: 85.659.. \n",
      "\u001B[32mTest loss decreased (89.308311 --> 85.659431).  Saving model ...\n",
      "\u001B[36mEpoch: 15/200..  Training Loss: 67.835.. \n",
      "\u001B[93mTest Loss: 85.551.. \n",
      "\u001B[32mTest loss decreased (85.659431 --> 85.550552).  Saving model ...\n",
      "\u001B[36mEpoch: 16/200..  Training Loss: 66.806.. \n",
      "\u001B[93mTest Loss: 86.105.. \n",
      "\u001B[36mEpoch: 17/200..  Training Loss: 67.165.. \n",
      "\u001B[93mTest Loss: 87.770.. \n",
      "\u001B[36mEpoch: 18/200..  Training Loss: 66.627.. \n",
      "\u001B[93mTest Loss: 87.634.. \n",
      "\u001B[36mEpoch: 19/200..  Training Loss: 66.256.. \n",
      "\u001B[93mTest Loss: 87.483.. \n",
      "\u001B[36mEpoch: 20/200..  Training Loss: 66.845.. \n",
      "\u001B[93mTest Loss: 84.891.. \n",
      "\u001B[32mTest loss decreased (85.550552 --> 84.891151).  Saving model ...\n",
      "\u001B[36mEpoch: 21/200..  Training Loss: 66.341.. \n",
      "\u001B[93mTest Loss: 86.994.. \n",
      "\u001B[36mEpoch: 22/200..  Training Loss: 66.874.. \n",
      "\u001B[93mTest Loss: 87.688.. \n",
      "\u001B[36mEpoch: 23/200..  Training Loss: 65.822.. \n",
      "\u001B[93mTest Loss: 83.665.. \n",
      "\u001B[32mTest loss decreased (84.891151 --> 83.664574).  Saving model ...\n",
      "\u001B[36mEpoch: 24/200..  Training Loss: 66.414.. \n",
      "\u001B[93mTest Loss: 86.214.. \n",
      "\u001B[36mEpoch: 25/200..  Training Loss: 66.246.. \n",
      "\u001B[93mTest Loss: 83.742.. \n",
      "\u001B[36mEpoch: 26/200..  Training Loss: 65.560.. \n",
      "\u001B[93mTest Loss: 84.895.. \n",
      "\u001B[36mEpoch: 27/200..  Training Loss: 65.360.. \n",
      "\u001B[93mTest Loss: 85.863.. \n",
      "\u001B[36mEpoch: 28/200..  Training Loss: 65.088.. \n",
      "\u001B[93mTest Loss: 86.690.. \n",
      "\u001B[36mEpoch: 29/200..  Training Loss: 65.343.. \n",
      "\u001B[93mTest Loss: 85.296.. \n",
      "\u001B[36mEpoch: 30/200..  Training Loss: 64.901.. \n",
      "\u001B[93mTest Loss: 84.622.. \n",
      "\u001B[36mEpoch: 31/200..  Training Loss: 65.309.. \n",
      "\u001B[93mTest Loss: 84.752.. \n",
      "\u001B[36mEpoch: 32/200..  Training Loss: 65.189.. \n",
      "\u001B[93mTest Loss: 85.306.. \n",
      "\u001B[36mEpoch: 33/200..  Training Loss: 64.998.. \n",
      "\u001B[93mTest Loss: 87.620.. \n",
      "\u001B[36mEpoch: 34/200..  Training Loss: 64.552.. \n",
      "\u001B[93mTest Loss: 90.749.. \n",
      "\u001B[36mEpoch: 35/200..  Training Loss: 64.821.. \n",
      "\u001B[93mTest Loss: 86.725.. \n",
      "\u001B[36mEpoch: 36/200..  Training Loss: 65.062.. \n",
      "\u001B[93mTest Loss: 86.417.. \n",
      "\u001B[36mEpoch: 37/200..  Training Loss: 64.983.. \n",
      "\u001B[93mTest Loss: 84.120.. \n",
      "\u001B[36mEpoch: 38/200..  Training Loss: 64.076.. \n",
      "\u001B[93mTest Loss: 84.321.. \n",
      "\u001B[36mEpoch: 39/200..  Training Loss: 64.283.. \n",
      "\u001B[93mTest Loss: 83.433.. \n",
      "\u001B[32mTest loss decreased (83.664574 --> 83.433128).  Saving model ...\n",
      "\u001B[36mEpoch: 40/200..  Training Loss: 64.668.. \n",
      "\u001B[93mTest Loss: 83.165.. \n",
      "\u001B[32mTest loss decreased (83.433128 --> 83.165054).  Saving model ...\n",
      "\u001B[36mEpoch: 41/200..  Training Loss: 64.524.. \n",
      "\u001B[93mTest Loss: 90.348.. \n",
      "\u001B[36mEpoch: 42/200..  Training Loss: 64.289.. \n",
      "\u001B[93mTest Loss: 83.573.. \n",
      "\u001B[36mEpoch: 43/200..  Training Loss: 64.203.. \n",
      "\u001B[93mTest Loss: 83.865.. \n",
      "\u001B[36mEpoch: 44/200..  Training Loss: 64.922.. \n",
      "\u001B[93mTest Loss: 85.037.. \n",
      "\u001B[36mEpoch: 45/200..  Training Loss: 64.934.. \n",
      "\u001B[93mTest Loss: 84.365.. \n",
      "\u001B[36mEpoch: 46/200..  Training Loss: 64.541.. \n",
      "\u001B[93mTest Loss: 84.011.. \n",
      "\u001B[36mEpoch: 47/200..  Training Loss: 64.125.. \n",
      "\u001B[93mTest Loss: 84.894.. \n",
      "\u001B[36mEpoch: 48/200..  Training Loss: 63.565.. \n",
      "\u001B[93mTest Loss: 84.157.. \n",
      "\u001B[36mEpoch: 49/200..  Training Loss: 63.838.. \n",
      "\u001B[93mTest Loss: 83.748.. \n",
      "\u001B[36mEpoch: 50/200..  Training Loss: 63.749.. \n",
      "\u001B[93mTest Loss: 85.291.. \n",
      "\u001B[36mEpoch: 51/200..  Training Loss: 64.248.. \n",
      "\u001B[93mTest Loss: 86.450.. \n",
      "\u001B[36mEpoch: 52/200..  Training Loss: 64.352.. \n",
      "\u001B[93mTest Loss: 82.672.. \n",
      "\u001B[32mTest loss decreased (83.165054 --> 82.671547).  Saving model ...\n",
      "\u001B[36mEpoch: 53/200..  Training Loss: 63.942.. \n",
      "\u001B[93mTest Loss: 86.987.. \n",
      "\u001B[36mEpoch: 54/200..  Training Loss: 63.960.. \n",
      "\u001B[93mTest Loss: 83.748.. \n",
      "\u001B[36mEpoch: 55/200..  Training Loss: 63.359.. \n",
      "\u001B[93mTest Loss: 83.731.. \n",
      "\u001B[36mEpoch: 56/200..  Training Loss: 63.231.. \n",
      "\u001B[93mTest Loss: 85.363.. \n",
      "\u001B[36mEpoch: 57/200..  Training Loss: 63.394.. \n",
      "\u001B[93mTest Loss: 86.598.. \n",
      "\u001B[36mEpoch: 58/200..  Training Loss: 63.367.. \n",
      "\u001B[93mTest Loss: 87.861.. \n",
      "\u001B[36mEpoch: 59/200..  Training Loss: 63.284.. \n",
      "\u001B[93mTest Loss: 85.662.. \n",
      "\u001B[36mEpoch: 60/200..  Training Loss: 63.163.. \n",
      "\u001B[93mTest Loss: 83.234.. \n",
      "\u001B[36mEpoch: 61/200..  Training Loss: 63.150.. \n",
      "\u001B[93mTest Loss: 84.003.. \n",
      "\u001B[36mEpoch: 62/200..  Training Loss: 62.868.. \n",
      "\u001B[93mTest Loss: 84.572.. \n",
      "\u001B[36mEpoch: 63/200..  Training Loss: 63.552.. \n",
      "\u001B[93mTest Loss: 84.096.. \n",
      "\u001B[36mEpoch: 64/200..  Training Loss: 62.638.. \n",
      "\u001B[93mTest Loss: 84.811.. \n",
      "\u001B[36mEpoch: 65/200..  Training Loss: 62.759.. \n",
      "\u001B[93mTest Loss: 84.912.. \n",
      "\u001B[36mEpoch: 66/200..  Training Loss: 63.154.. \n",
      "\u001B[93mTest Loss: 86.964.. \n",
      "\u001B[36mEpoch: 67/200..  Training Loss: 62.913.. \n",
      "\u001B[93mTest Loss: 82.675.. \n",
      "\u001B[36mEpoch: 68/200..  Training Loss: 62.740.. \n",
      "\u001B[93mTest Loss: 84.616.. \n",
      "\u001B[36mEpoch: 69/200..  Training Loss: 62.514.. \n",
      "\u001B[93mTest Loss: 83.296.. \n",
      "\u001B[36mEpoch: 70/200..  Training Loss: 62.009.. \n",
      "\u001B[93mTest Loss: 86.741.. \n",
      "\u001B[36mEpoch: 71/200..  Training Loss: 62.570.. \n",
      "\u001B[93mTest Loss: 88.928.. \n",
      "\u001B[36mEpoch: 72/200..  Training Loss: 62.618.. \n",
      "\u001B[93mTest Loss: 84.503.. \n",
      "\u001B[36mEpoch: 73/200..  Training Loss: 63.275.. \n",
      "\u001B[93mTest Loss: 85.952.. \n",
      "\u001B[36mEpoch: 74/200..  Training Loss: 62.642.. \n",
      "\u001B[93mTest Loss: 86.539.. \n",
      "\u001B[36mEpoch: 75/200..  Training Loss: 62.055.. \n",
      "\u001B[93mTest Loss: 86.078.. \n",
      "\u001B[36mEpoch: 76/200..  Training Loss: 62.222.. \n",
      "\u001B[93mTest Loss: 84.093.. \n",
      "\u001B[36mEpoch: 77/200..  Training Loss: 62.311.. \n",
      "\u001B[93mTest Loss: 84.193.. \n",
      "\u001B[36mEpoch: 78/200..  Training Loss: 62.303.. \n",
      "\u001B[93mTest Loss: 83.994.. \n",
      "\u001B[36mEpoch: 79/200..  Training Loss: 62.095.. \n",
      "\u001B[93mTest Loss: 84.433.. \n",
      "\u001B[36mEpoch: 80/200..  Training Loss: 61.772.. \n",
      "\u001B[93mTest Loss: 84.764.. \n",
      "\u001B[36mEpoch: 81/200..  Training Loss: 62.375.. \n",
      "\u001B[93mTest Loss: 84.575.. \n",
      "\u001B[36mEpoch: 82/200..  Training Loss: 62.023.. \n",
      "\u001B[93mTest Loss: 85.570.. \n",
      "\u001B[36mEpoch: 83/200..  Training Loss: 62.330.. \n",
      "\u001B[93mTest Loss: 83.488.. \n",
      "\u001B[36mEpoch: 84/200..  Training Loss: 62.008.. \n",
      "\u001B[93mTest Loss: 83.438.. \n",
      "\u001B[36mEpoch: 85/200..  Training Loss: 61.867.. \n",
      "\u001B[93mTest Loss: 84.315.. \n",
      "\u001B[36mEpoch: 86/200..  Training Loss: 61.624.. \n",
      "\u001B[93mTest Loss: 84.634.. \n",
      "\u001B[36mEpoch: 87/200..  Training Loss: 61.569.. \n",
      "\u001B[93mTest Loss: 84.489.. \n",
      "\u001B[36mEpoch: 88/200..  Training Loss: 61.639.. \n",
      "\u001B[93mTest Loss: 85.521.. \n",
      "\u001B[36mEpoch: 89/200..  Training Loss: 61.744.. \n",
      "\u001B[93mTest Loss: 84.492.. \n",
      "\u001B[36mEpoch: 90/200..  Training Loss: 61.441.. \n",
      "\u001B[93mTest Loss: 84.553.. \n",
      "\u001B[36mEpoch: 91/200..  Training Loss: 61.683.. \n",
      "\u001B[93mTest Loss: 84.017.. \n",
      "\u001B[36mEpoch: 92/200..  Training Loss: 61.514.. \n",
      "\u001B[93mTest Loss: 83.665.. \n",
      "\u001B[31mTest loss did not decrease for 40 epochs. Stopping training. Loading model with lowest test loss.\n",
      "\u001B[35mTime to train the model:  243.72210144996643 seconds\n",
      "\u001B[32mCFS Lowest Test loss achieved: 113.042831\n",
      "\u001B[32mAt Epoch #13 \n",
      "\n",
      "\u001B[32mElastic Net Lowest Test loss achieved: 76.258965\n",
      "\u001B[32mAt Epoch #43 \n",
      "\n",
      "\u001B[32mWeights Net Lowest Test loss achieved: 73.434898\n",
      "\u001B[32mAt Epoch #65 \n",
      "\n",
      "\u001B[32mRegularized Weights Net Lowest Test loss achieved: 77.416878\n",
      "\u001B[32mAt Epoch #62 \n",
      "\n",
      "\u001B[32mGradient Weights Net Lowest Test loss achieved: 82.671547\n",
      "\u001B[32mAt Epoch #51 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs_input_dim = k # adjusting the input dimension to the number of features selected by the feature selection methods\n",
    "class FS_Net(nn.Module): # defining the NN architecture using reduced number of features\n",
    "    def __init__(self):\n",
    "        super(FS_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(fs_input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "print(Fore.CYAN + \"\", FS_Net())\n",
    "\n",
    "cfs_net = FS_Net().to(device)\n",
    "elastic_net = FS_Net().to(device)\n",
    "weights_net = FS_Net().to(device)\n",
    "reg_weights_net = FS_Net().to(device)\n",
    "grad_weights_net = FS_Net().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "cfs_optimizer = optim.Adam(cfs_net.parameters(), lr=0.001)\n",
    "elastic_optimizer = optim.Adam(elastic_net.parameters(), lr=0.001)\n",
    "weights_optimizer = optim.Adam(weights_net.parameters(), lr=0.001)\n",
    "reg_weights_optimizer = optim.Adam(reg_weights_net.parameters(), lr=0.001)\n",
    "grad_weights_optimizer = optim.Adam(grad_weights_net.parameters(), lr=0.001)\n",
    "\n",
    "cfs_writer = SummaryWriter()\n",
    "elastic_writer = SummaryWriter()\n",
    "weights_writer = SummaryWriter()\n",
    "reg_weights_writer = SummaryWriter()\n",
    "grad_weights_writer = SummaryWriter()\n",
    "\n",
    "# Training the NNs\n",
    "cfs_net, cfs_test_loss_min, cfs_min_loss_epoch = train_regular_NN(cfs_net, cfs_train_loader, cfs_test_loader, criterion, cfs_optimizer, num_epochs, cfs_writer)\n",
    "cfs_net.load_state_dict(torch.load('regular_net.pt')) # loading the model with the lowest test loss\n",
    "torch.save(cfs_net.state_dict(), 'cfs_FS_net.pt') # saving the model by a suitable name\n",
    "\n",
    "elastic_net, elastic_test_loss_min, elastic_min_loss_epoch = train_regular_NN(elastic_net, elastic_train_loader, elastic_test_loader, criterion, elastic_optimizer, num_epochs, elastic_writer)\n",
    "elastic_net.load_state_dict(torch.load('regular_net.pt'))\n",
    "torch.save(elastic_net.state_dict(), 'elastic_FS_net.pt')\n",
    "\n",
    "weights_net, weights_test_loss_min, weights_min_loss_epoch = train_regular_NN(weights_net, weights_fs_train_loader, weights_fs_test_loader, criterion, weights_optimizer, num_epochs, weights_writer)\n",
    "weights_net.load_state_dict(torch.load('regular_net.pt'))\n",
    "torch.save(weights_net.state_dict(), 'weights_FS_net.pt')\n",
    "\n",
    "reg_weights_net, reg_weights_test_loss_min, reg_weights_min_loss_epoch = train_regular_NN(reg_weights_net, reg_weights_fs_train_loader, reg_weights_fs_test_loader, criterion, reg_weights_optimizer, num_epochs, reg_weights_writer)\n",
    "reg_weights_net.load_state_dict(torch.load('regular_net.pt'))\n",
    "torch.save(reg_weights_net.state_dict(), 'reg_weights_FS_net.pt')\n",
    "\n",
    "grad_weights_net, grad_weights_test_loss_min, grad_weights_min_loss_epoch = train_regular_NN(grad_weights_net, grad_weights_fs_train_loader, grad_weights_fs_test_loader, criterion, grad_weights_optimizer, num_epochs, grad_weights_writer)\n",
    "grad_weights_net.load_state_dict(torch.load('regular_net.pt'))\n",
    "torch.save(grad_weights_net.state_dict(), 'grad_weights_FS_net.pt')\n",
    "\n",
    "print(Fore.GREEN + \"CFS Lowest Test loss achieved: {:.6f}\".format(cfs_test_loss_min))\n",
    "print(Fore.GREEN + \"At Epoch #{}\".format(cfs_min_loss_epoch), \"\\n\")\n",
    "print(Fore.GREEN + \"Elastic Net Lowest Test loss achieved: {:.6f}\".format(elastic_test_loss_min))\n",
    "print(Fore.GREEN + \"At Epoch #{}\".format(elastic_min_loss_epoch), \"\\n\")\n",
    "print(Fore.GREEN + \"Weights Net Lowest Test loss achieved: {:.6f}\".format(weights_test_loss_min))\n",
    "print(Fore.GREEN + \"At Epoch #{}\".format(weights_min_loss_epoch), \"\\n\")\n",
    "print(Fore.GREEN + \"Regularized Weights Net Lowest Test loss achieved: {:.6f}\".format(reg_weights_test_loss_min))\n",
    "print(Fore.GREEN + \"At Epoch #{}\".format(reg_weights_min_loss_epoch), \"\\n\")\n",
    "print(Fore.GREEN + \"Gradient Weights Net Lowest Test loss achieved: {:.6f}\".format(grad_weights_test_loss_min))\n",
    "print(Fore.GREEN + \"At Epoch #{}\".format(grad_weights_min_loss_epoch), \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "outputs": [],
   "source": [
    "net = Net()\n",
    "if pollutant == 'NO2':\n",
    "    net.load_state_dict(torch.load('best_NO2_model.pt')) # loading best NO2 Neural Network model\n",
    "else: # pollutant == 'PM25'\n",
    "    net.load_state_dict(torch.load('best_PM25_model.pt')) # loading best PM25 Neural Network model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "outputs": [],
   "source": [
    "# converting y_test and X_test to torch.Tensor if it is not already\n",
    "if type(y_test) != torch.Tensor:\n",
    "    y_test = y_test.to_numpy().flatten() # shape y_test with dim 0\n",
    "    y_test = torch.Tensor(y_test)\n",
    "if type(X_test) != torch.Tensor:\n",
    "    X_test = torch.Tensor(X_test)\n",
    "\n",
    "results_df = pd.DataFrame() # creating a dataframe to store the results\n",
    "results_df[\"Actual Pollutant\"] = test_pollutant # adding the actual pollutant column to the dataframe\n",
    "\n",
    "if pollutant == 'NO2':\n",
    "    results_df[\"Chimere Predicted Pollutant\"] = X_test_unscaled[\"modNO2\"] # adding the Chimere predicted pollutant column to the dataframe\n",
    "else:\n",
    "    results_df[\"Chimere Predicted Pollutant\"] = X_test_unscaled[\"modPM25\"]\n",
    "\n",
    "results_df[\"Real Error\"] = y_test # adding the real error column to the dataframe\n",
    "results_df[\"NN Predicted Error\"] = net(X_test).detach().numpy()\n",
    "results_df[\"NN Corrected Pollutant\"] = results_df[\"Chimere Predicted Pollutant\"] + results_df[\"NN Predicted Error\"] # Correction is CHIMERE's estimation + the model's error estimation\n",
    "results_df[\"RF Predicted Error\"] = rf.predict(X_test)\n",
    "results_df[\"RF Corrected Pollutant\"] = results_df[\"Chimere Predicted Pollutant\"] + results_df[\"RF Predicted Error\"]\n",
    "results_df[\"XGB Predicted Error\"] = xgb.predict(X_test)\n",
    "results_df[\"XGB Corrected Pollutant\"] = results_df[\"Chimere Predicted Pollutant\"] + results_df[\"XGB Predicted Error\"]\n",
    "\n",
    "# NOW FEATURE SELECTED RESULTS:\n",
    "results_df[\"XGB FS Predicted Error\"] = xgb_fs.predict(X_test_xgb)\n",
    "results_df[\"XGB FS Corrected Pollutant\"] = results_df[\"Chimere Predicted Pollutant\"] + results_df[\"XGB FS Predicted Error\"]\n",
    "results_df[\"XGB Permutation FS Predicted Error\"] = xgb_permutation_fs.predict(X_test_xgb_perm)\n",
    "results_df[\"XGB Permutation FS Corrected Pollutant\"] = results_df[\"Chimere Predicted Pollutant\"] + results_df[\"XGB Permutation FS Predicted Error\"]\n",
    "\n",
    "results_df[\"CFS NN Predicted Error\"] = cfs_net(X_test_cfs).detach().numpy()\n",
    "results_df[\"CFS NN Corrected Pollutant\"] = results_df[\"Chimere Predicted Pollutant\"] + results_df[\"CFS NN Predicted Error\"]\n",
    "results_df[\"Elastic Net Predicted Error\"] = elastic_net(X_test_elastic).detach().numpy() # Features selected by regular Elastic Net, not the one with the NN\n",
    "results_df[\"Elastic Net Corrected Pollutant\"] = results_df[\"Chimere Predicted Pollutant\"] + results_df[\"Elastic Net Predicted Error\"]\n",
    "results_df[\"Weights FS NN Predicted Error\"] = weights_net(X_test_weights_fs).detach().numpy()\n",
    "results_df[\"Weights FS NN Corrected Pollutant\"] = results_df[\"Chimere Predicted Pollutant\"] + results_df[\"Weights FS NN Predicted Error\"]\n",
    "results_df[\"Elastic Weights FS NN Predicted Error\"] = reg_weights_net(X_test_reg_weights_fs).detach().numpy()\n",
    "results_df[\"Elastic Weights FS NN Corrected Pollutant\"] = results_df[\"Chimere Predicted Pollutant\"] + results_df[\"Elastic Weights FS NN Predicted Error\"]\n",
    "results_df[\"Gradient Weights FS NN Predicted Error\"] = grad_weights_net(X_test_grad_weights_fs).detach().numpy()\n",
    "results_df[\"Gradient Weights FS NN Corrected Pollutant\"] = results_df[\"Chimere Predicted Pollutant\"] + results_df[\"Gradient Weights FS NN Predicted Error\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mChimere - Actual Pollutant MAE:  6.493756069899476\n",
      "\u001B[32mNN Corrected - Actual Pollutant MAE:  3.248153369509493\n",
      "\u001B[32mRF Corrected - Actual Pollutant MAE:  4.577561347676703\n",
      "\u001B[32mXGB Corrected - Actual Pollutant MAE:  4.1828235050756035\n",
      "\u001B[34mXGB FS Corrected - Actual Pollutant MAE:  4.509673186557626\n",
      "\u001B[34mXGB Permutation FS Corrected - Actual Pollutant MAE:  4.473053424058212\n",
      "\u001B[34mCFS NN Corrected - Actual Pollutant MAE:  5.877089565556908\n",
      "\u001B[34mElastic Net Corrected - Actual Pollutant MAE:  4.808691234595688\n",
      "\u001B[34mWeights FS NN Corrected - Actual Pollutant MAE:  4.612971366687664\n",
      "\u001B[34mElastic Weights FS NN Corrected - Actual Pollutant MAE:  4.760401161600757\n",
      "\u001B[34mGradient Weights FS NN Corrected - Actual Pollutant MAE:  4.96644528620596 \n",
      "\n",
      "\u001B[32mNN Relative reduction in MAE:  49.980360602615384 %\n",
      "\u001B[32mRF Relative reduction in MAE:  29.508264579030236 %\n",
      "\u001B[32mXGB Relative reduction in MAE:  35.58699372056404 %\n",
      "\u001B[34mXGB FS Relative reduction in MAE:  30.55370207911989 %\n",
      "\u001B[34mXGB Permutation FS Relative reduction in MAE:  31.1176247473759 %\n",
      "\u001B[34mCFS NN Relative reduction in MAE:  9.496299178852805 %\n",
      "\u001B[34mElastic Net Relative reduction in MAE:  25.949001119930777 %\n",
      "\u001B[34mWeights FS NN Relative reduction in MAE:  28.962971244482336 %\n",
      "\u001B[34mElastic Weights FS NN Relative reduction in MAE:  26.692639662480453 %\n",
      "\u001B[34mGradient Weights FS NN Relative reduction in MAE:  23.519682095437243 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Calculating Mean Absolute Error (MAE)\n",
    "chimere_actual_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"Chimere Predicted Pollutant\"])\n",
    "NN_corrected_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"NN Corrected Pollutant\"])\n",
    "RF_corrected_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"RF Corrected Pollutant\"])\n",
    "XGB_corrected_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"XGB Corrected Pollutant\"])\n",
    "\n",
    "# NOW FEATURE SELECTED RESULTS:\n",
    "XGB_FS_corrected_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"XGB FS Corrected Pollutant\"])\n",
    "XGB_Permutation_FS_corrected_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"XGB Permutation FS Corrected Pollutant\"])\n",
    "CFS_NN_corrected_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"CFS NN Corrected Pollutant\"])\n",
    "Elastic_Net_corrected_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"Elastic Net Corrected Pollutant\"])\n",
    "Weights_FS_NN_corrected_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"Weights FS NN Corrected Pollutant\"])\n",
    "Elastic_Weights_FS_NN_corrected_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"Elastic Weights FS NN Corrected Pollutant\"])\n",
    "Gradient_Weights_FS_NN_corrected_mae = metrics.mean_absolute_error(results_df[\"Actual Pollutant\"], results_df[\"Gradient Weights FS NN Corrected Pollutant\"])\n",
    "\n",
    "print(Fore.RED + \"Chimere - Actual Pollutant MAE: \", chimere_actual_mae)\n",
    "print(Fore.GREEN + \"NN Corrected - Actual Pollutant MAE: \", NN_corrected_mae)\n",
    "print(Fore.GREEN + \"RF Corrected - Actual Pollutant MAE: \", RF_corrected_mae)\n",
    "print(Fore.GREEN + \"XGB Corrected - Actual Pollutant MAE: \", XGB_corrected_mae)\n",
    "# NOW FEATURE SELECTED RESULTS:\n",
    "print(Fore.BLUE + \"XGB FS Corrected - Actual Pollutant MAE: \", XGB_FS_corrected_mae)\n",
    "print(Fore.BLUE + \"XGB Permutation FS Corrected - Actual Pollutant MAE: \", XGB_Permutation_FS_corrected_mae)\n",
    "print(Fore.BLUE + \"CFS NN Corrected - Actual Pollutant MAE: \", CFS_NN_corrected_mae)\n",
    "print(Fore.BLUE + \"Elastic Net Corrected - Actual Pollutant MAE: \", Elastic_Net_corrected_mae)\n",
    "print(Fore.BLUE + \"Weights FS NN Corrected - Actual Pollutant MAE: \", Weights_FS_NN_corrected_mae)\n",
    "print(Fore.BLUE + \"Elastic Weights FS NN Corrected - Actual Pollutant MAE: \", Elastic_Weights_FS_NN_corrected_mae)\n",
    "print(Fore.BLUE + \"Gradient Weights FS NN Corrected - Actual Pollutant MAE: \", Gradient_Weights_FS_NN_corrected_mae, \"\\n\")\n",
    "\n",
    "# print(Style.RESET_ALL) # resetting the color\n",
    "\n",
    "# printing relative reduction in MAE\n",
    "print(Fore.GREEN + \"NN Relative reduction in MAE: \", 100*(chimere_actual_mae - NN_corrected_mae)/chimere_actual_mae, \"%\")\n",
    "print(Fore.GREEN + \"RF Relative reduction in MAE: \", 100*(chimere_actual_mae - RF_corrected_mae)/chimere_actual_mae, \"%\")\n",
    "print(Fore.GREEN + \"XGB Relative reduction in MAE: \", 100*(chimere_actual_mae - XGB_corrected_mae)/chimere_actual_mae, \"%\")\n",
    "# NOW FEATURE SELECTED RESULTS:\n",
    "print(Fore.BLUE + \"XGB FS Relative reduction in MAE: \", 100*(chimere_actual_mae - XGB_FS_corrected_mae)/chimere_actual_mae, \"%\")\n",
    "print(Fore.BLUE + \"XGB Permutation FS Relative reduction in MAE: \", 100*(chimere_actual_mae - XGB_Permutation_FS_corrected_mae)/chimere_actual_mae, \"%\")\n",
    "print(Fore.BLUE + \"CFS NN Relative reduction in MAE: \", 100*(chimere_actual_mae - CFS_NN_corrected_mae)/chimere_actual_mae, \"%\")\n",
    "print(Fore.BLUE + \"Elastic Net Relative reduction in MAE: \", 100*(chimere_actual_mae - Elastic_Net_corrected_mae)/chimere_actual_mae, \"%\")\n",
    "print(Fore.BLUE + \"Weights FS NN Relative reduction in MAE: \", 100*(chimere_actual_mae - Weights_FS_NN_corrected_mae)/chimere_actual_mae, \"%\")\n",
    "print(Fore.BLUE + \"Elastic Weights FS NN Relative reduction in MAE: \", 100*(chimere_actual_mae - Elastic_Weights_FS_NN_corrected_mae)/chimere_actual_mae, \"%\")\n",
    "print(Fore.BLUE + \"Gradient Weights FS NN Relative reduction in MAE: \", 100*(chimere_actual_mae - Gradient_Weights_FS_NN_corrected_mae)/chimere_actual_mae, \"%\", \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "outputs": [],
   "source": [
    "# converting y_train and y_test from Torch Tensors to DataFrame\n",
    "if type(y_train) == torch.Tensor and type(y_test) == torch.Tensor:\n",
    "    y_train = pd.DataFrame(y_train.detach().numpy())\n",
    "    y_test = pd.DataFrame(y_test.detach().numpy())\n",
    "\n",
    "test_data = data.loc[y_test_index] # loading the original test data by using index of y_test on the original dataset\n",
    "\n",
    "test_data_NN, test_data_RF, test_data_XGB = test_data.copy(), test_data.copy(), test_data.copy() # making copies of the test data for each model\n",
    "\n",
    "# replacing the error column with the corrected errors, which is Actual Pollutant minus the corrected prediction\n",
    "test_data_NN[\"Err\"] = list(results_df[\"Actual Pollutant\"] - results_df[\"NN Corrected Pollutant\"])\n",
    "test_data_RF[\"Err\"] = list(results_df[\"Actual Pollutant\"] - results_df[\"RF Corrected Pollutant\"])\n",
    "test_data_XGB[\"Err\"] = list(results_df[\"Actual Pollutant\"] - results_df[\"XGB Corrected Pollutant\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "outputs": [],
   "source": [
    "stations_names = data[\"station\"].unique() # getting the names of the AQM stations\n",
    "stations_mean = {\"Station\": [], \"Error Mean\": []} # creating a dictionary to store the mean of the errors for each station (CHIMERE)\n",
    "stations_mean_NN = {\"Station\": [], \"Error Mean\": []} # NN\n",
    "stations_mean_RF = {\"Station\": [], \"Error Mean\": []} # RF\n",
    "stations_mean_XGB = {\"Station\": [], \"Error Mean\": []} # XGB\n",
    "for station in stations_names: # for every station in the dataset, inserting the station name and the mean error\n",
    "    stations_mean[\"Station\"].append(station)\n",
    "    stations_mean[\"Error Mean\"].append(test_data[test_data[\"station\"] == station][\"Err\"].mean())\n",
    "\n",
    "    stations_mean_NN[\"Station\"].append(station)\n",
    "    stations_mean_NN[\"Error Mean\"].append(test_data_NN[test_data_NN[\"station\"] == station][\"Err\"].mean())\n",
    "\n",
    "    stations_mean_RF[\"Station\"].append(station)\n",
    "    stations_mean_RF[\"Error Mean\"].append(test_data_RF[test_data_RF[\"station\"] == station][\"Err\"].mean())\n",
    "\n",
    "    stations_mean_XGB[\"Station\"].append(station)\n",
    "    stations_mean_XGB[\"Error Mean\"].append(test_data_XGB[test_data_XGB[\"station\"] == station][\"Err\"].mean())\n",
    "\n",
    "stations_mean = pd.DataFrame(stations_mean)\n",
    "stations_mean_NN = pd.DataFrame(stations_mean_NN)\n",
    "stations_mean_RF = pd.DataFrame(stations_mean_RF)\n",
    "stations_mean_XGB = pd.DataFrame(stations_mean_XGB)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "outputs": [],
   "source": [
    "# for all models, we count how many stations have a mean error between -threshold to +threshold\n",
    "\n",
    "thresholds = [1, 3, 4, 7, 10] # thresholds of the mean error to be considered\n",
    "models = [\"CHIMERE\", \"NN\", \"RF\", \"XGB\"]\n",
    "stations_mean_models = {\"Model\": [], \"Threshold\": [], \"Number of Stations\": []}\n",
    "for model in models:\n",
    "    for threshold in thresholds:\n",
    "        stations_mean_models[\"Model\"].append(model)\n",
    "        stations_mean_models[\"Threshold\"].append(threshold)\n",
    "        if model == \"CHIMERE\":\n",
    "            stations_mean_models[\"Number of Stations\"].append(len(stations_mean[(stations_mean[\"Error Mean\"] <= threshold) & (stations_mean[\"Error Mean\"] >= -threshold)]))\n",
    "        elif model == \"NN\":\n",
    "            stations_mean_models[\"Number of Stations\"].append(len(stations_mean_NN[(stations_mean_NN[\"Error Mean\"] <= threshold) & (stations_mean_NN[\"Error Mean\"] >= -threshold)]))\n",
    "        elif model == \"RF\":\n",
    "            stations_mean_models[\"Number of Stations\"].append(len(stations_mean_RF[(stations_mean_RF[\"Error Mean\"] <= threshold) & (stations_mean_RF[\"Error Mean\"] >= -threshold)]))\n",
    "        elif model == \"XGB\":\n",
    "            stations_mean_models[\"Number of Stations\"].append(len(stations_mean_XGB[(stations_mean_XGB[\"Error Mean\"] <= threshold) & (stations_mean_XGB[\"Error Mean\"] >= -threshold)]))\n",
    "\n",
    "stations_mean_models = pd.DataFrame(stations_mean_models)\n",
    "stations_mean_models = stations_mean_models.pivot(index=\"Model\", columns=\"Threshold\", values=\"Number of Stations\") # arranging the dataframe in a more readable way"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "outputs": [
    {
     "data": {
      "text/plain": "Threshold  1    3    4    7    10\nModel                            \nCHIMERE    28   73   85  100  109\nNN         74  110  113  114  115\nRF         60  102  108  113  114\nXGB        73  108  109  112  114",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Threshold</th>\n      <th>1</th>\n      <th>3</th>\n      <th>4</th>\n      <th>7</th>\n      <th>10</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>CHIMERE</th>\n      <td>28</td>\n      <td>73</td>\n      <td>85</td>\n      <td>100</td>\n      <td>109</td>\n    </tr>\n    <tr>\n      <th>NN</th>\n      <td>74</td>\n      <td>110</td>\n      <td>113</td>\n      <td>114</td>\n      <td>115</td>\n    </tr>\n    <tr>\n      <th>RF</th>\n      <td>60</td>\n      <td>102</td>\n      <td>108</td>\n      <td>113</td>\n      <td>114</td>\n    </tr>\n    <tr>\n      <th>XGB</th>\n      <td>73</td>\n      <td>108</td>\n      <td>109</td>\n      <td>112</td>\n      <td>114</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_mean_models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "outputs": [
    {
     "data": {
      "text/plain": "Threshold         1          3          4          7           10\nModel                                                            \nCHIMERE    24.347826  63.478261  73.913043  86.956522   94.782609\nNN         64.347826  95.652174  98.260870  99.130435  100.000000\nRF         52.173913  88.695652  93.913043  98.260870   99.130435\nXGB        63.478261  93.913043  94.782609  97.391304   99.130435",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Threshold</th>\n      <th>1</th>\n      <th>3</th>\n      <th>4</th>\n      <th>7</th>\n      <th>10</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>CHIMERE</th>\n      <td>24.347826</td>\n      <td>63.478261</td>\n      <td>73.913043</td>\n      <td>86.956522</td>\n      <td>94.782609</td>\n    </tr>\n    <tr>\n      <th>NN</th>\n      <td>64.347826</td>\n      <td>95.652174</td>\n      <td>98.260870</td>\n      <td>99.130435</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>RF</th>\n      <td>52.173913</td>\n      <td>88.695652</td>\n      <td>93.913043</td>\n      <td>98.260870</td>\n      <td>99.130435</td>\n    </tr>\n    <tr>\n      <th>XGB</th>\n      <td>63.478261</td>\n      <td>93.913043</td>\n      <td>94.782609</td>\n      <td>97.391304</td>\n      <td>99.130435</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translating the results into percentages\n",
    "stations_mean_models_percentages = stations_mean_models.apply(lambda x: x / len(stations_mean) * 100, axis=1)\n",
    "stations_mean_models_percentages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractional Bias for CHIMERE: 0.5042374931437609\n",
      "Fractional Bias for RF: 0.014269432410574549\n",
      "Fractional Bias for NN: -0.002306747303681785\n",
      "Fractional Bias for XGB: 0.021356499897137903\n"
     ]
    }
   ],
   "source": [
    "# calculating mean fractional bias for the CHIMERE model, the RF model, the NN model, and the XGB model.\n",
    "if pollutant == 'NO2':\n",
    "    test_data_RF[\"modNO2\"] = results_df[\"RF Corrected Pollutant\"] # replacing CHIMERE's predictions with predictions corrected by RF\n",
    "    test_data_NN[\"modNO2\"] = results_df[\"NN Corrected Pollutant\"] # NN\n",
    "    test_data_XGB[\"modNO2\"] = results_df[\"XGB Corrected Pollutant\"] # XGB\n",
    "    FB = 2 * (test_data[\"NO2\"].mean() - test_data[\"modNO2\"].mean()) / (test_data[\"NO2\"].mean() + test_data[\"modNO2\"].mean())\n",
    "    FB_RF = 2 * (test_data[\"NO2\"].mean() - test_data_RF[\"modNO2\"].mean()) / (test_data[\"NO2\"].mean() + test_data_RF[\"modNO2\"].mean())\n",
    "    FB_NN = 2 * (test_data[\"NO2\"].mean() - test_data_NN[\"modNO2\"].mean()) / (test_data[\"NO2\"].mean() + test_data_NN[\"modNO2\"].mean())\n",
    "    FB_XGB = 2 * (test_data[\"NO2\"].mean() - test_data_XGB[\"modNO2\"].mean()) / (test_data[\"NO2\"].mean() + test_data_XGB[\"modNO2\"].mean())\n",
    "else:\n",
    "    test_data_RF[\"modPM25\"] = results_df[\"RF Corrected Pollutant\"]\n",
    "    test_data_NN[\"modPM25\"] = results_df[\"NN Corrected Pollutant\"]\n",
    "    test_data_XGB[\"modPM25\"] = results_df[\"XGB Corrected Pollutant\"]\n",
    "    FB = 2 * (test_data[\"PM25\"].mean() - test_data[\"modPM25\"].mean()) / (test_data[\"PM25\"].mean() + test_data[\"modPM25\"].mean())\n",
    "    FB_RF = 2 * (test_data[\"PM25\"].mean() - test_data_RF[\"modPM25\"].mean()) / (test_data[\"PM25\"].mean() + test_data_RF[\"modPM25\"].mean())\n",
    "    FB_NN = 2 * (test_data[\"PM25\"].mean() - test_data_NN[\"modPM25\"].mean()) / (test_data[\"PM25\"].mean() + test_data_NN[\"modPM25\"].mean())\n",
    "    FB_XGB = 2 * (test_data[\"PM25\"].mean() - test_data_XGB[\"modPM25\"].mean()) / (test_data[\"PM25\"].mean() + test_data_XGB[\"modPM25\"].mean())\n",
    "\n",
    "print(\"Fractional Bias for CHIMERE: \" + str(FB))\n",
    "print(\"Fractional Bias for RF: \" + str(FB_RF))\n",
    "print(\"Fractional Bias for NN: \" + str(FB_NN))\n",
    "print(\"Fractional Bias for XGB: \" + str(FB_XGB))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 2000x1000 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABokAAAN1CAYAAACjKRJ1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3gUVdsG8HtmS5JNL5SE0HtVijQRpAiCiKIUBVERFEXs5QX9RAUVXxVfC0oRUEGpUpTee5Ui0kvo6b1vtsx8f2y2JbvJJtlkE3L/vHJld+bMmWfO7k7kPHvOEWRZlkFERERERERERERERETViujpAIiIiIiIiIiIiIiIiKjiMUlERERERERERERERERUDTFJREREREREREREREREVA0xSURERERERERERERERFQNMUlERERERERERERERERUDTFJREREREREREREREREVA0xSURERERERERERERERFQNMUlERERERERERERERERUDTFJREREREREbiVJkqdDcCuj0ejpEIioDMrzM8z7AxEREVV1Sk8HQERERFQVxMfHY+3atdi/fz+ioqKQkZEBlUqF2rVr4+6778YjjzyCrl27Oj1+zJgxOHr0qMN9giDAy8sLgYGBaNiwIfr164fBgwcjODjYaX3ff/89Zs2aVaJrqFOnDnbu3FmiY8oiISEB33zzDfbv34+UlBT4+/ujffv2+PHHH4s87siRI3j66actzxs0aIAtW7a4dE6DwYAePXogNTUVANC5c2csXry49BdRhRT1HivK0KFD8fnnn7slhhs3buCjjz7C9OnTERkZadl++/Zt9O3bFwAwY8YMPPbYY245X3mTJAlLlizB9evX8X//9392+yZPnow1a9ZU+OeqLFavXo0pU6aU6tiLFy+6OZqqzfY9XRKLFi1Cly5dyiGisjt58iSeeOIJAIBGo8G+ffvg5+dX5DG276kdO3ZYPvfu+szv2LED27Ztw8mTJ5GYmAiDwYCQkBDUr18f9957L4YOHYoaNWo4PT4rKwtff/012rVrh0cffbRUMTij0+kwd+5ceHl54YUXXrDbZ74fV6e/QURERFR1MUlEREREVAStVotvvvkGv/32G/R6vd0+vV6Pq1ev4urVq1i9ejXuu+8+fPHFFwgJCSnROWRZhlarhVarRXx8PA4fPowffvgBH330ER588EF3Xk6FycvLw5gxY3D9+nXLtpSUFKhUqhLXdf36dZw/fx4tW7YstuyBAwcsCSKqWBcuXMCIESOQl5fn6VDc5p133sH69esxdOhQT4dCVO5WrVpleZyTk4N169bhySef9EgscXFxeO211/DPP/8U2hcbG4vY2FgcPnwYs2fPxltvvYWnnnrKYT0DBw5EQkIC2rRp4/YYn376aZw8eRKTJk1ye91EREREFYlJIiIiIiIn0tPTMW7cOJw+fRoA0KZNGzz55JPo0KEDQkJCkJKSgqioKPzyyy84duwY9u3bhyeeeALLly93OgooIiIC69evt9smyzKys7MRExODffv24ddff0VqaireeOMNCIKAAQMGFBnnhg0bEB4eXuz1iGLFzTR8/PhxS4Jo/PjxePrpp6FSqSAIQqnq27x5s0tJoo0bN5aq/juJo/dYUUqTuHMkPT3daYJIpVKhXr16AFDsyITKJCEhwem+0NBQ1KtXD7Vr167AiNxn3rx56NSpk6fDuCNMmDABEyZMcKmst7d3OUdTOrm5uZb75913341//vkHy5cv90iSKCcnB8899xyioqLg5+eHsWPH4v7770dERAQUCgWSkpLw999/Y968eYiOjsb06dOhVCoto6BsFfUZLqui6q5Zsybq1auHmjVrltv5iYiIiNyFSSIiIiIiB2RZxuuvv25JEI0bNw7vvPOOXZIjKCgIjRo1wgMPPIB58+Zh5syZuHHjBqZMmYI5c+Y4rFcQBPj6+hba7ufnh1q1aqF9+/Z4+OGHMWbMGCQmJuLtt99Gy5YtLR3sjnh7ezus05MSExMtjydOnFjq+Jo1a4ZLly5h8+bNeOONN4osq9PpsH37dgBAjRo17GKoTpy9xzypVq1a2LZtm6fDcKt33nkH77zzjqfDKLXKeN+oqlQqVZVvy82bNyM7Oxve3t547bXXMHbsWJw/fx7//PMP7r777gqNZcWKFYiKioJKpcLixYvRqlUru/2BgYFo3Lgx+vfvj8cffxwxMTGYOXMmBg8eXGmS0DNnzvR0CEREREQuq7ivkxIRERFVIWvXrsXBgwcBACNGjMC7775b5CiYF154Affffz8AYNeuXThx4kSpz92wYUN89dVXAEyJj5KuPVQZ2C7kXZbO00GDBgGwTjlXlD179iArKwvNmzdHw4YNS31OIqLqZvXq1QCALl26oGvXrqhVqxYAYNmyZRUei3mNrx49ehRKENkKCQnBu+++CwDIyMjA/v37KyQ+IiIiojsNRxIREREROfDTTz8BMC3eXdwIFrOXX34Ze/fuRdOmTREXF1em83ft2hU9evTA/v37sW7dOrz33nsICgoqU52lYTQasWHDBvz11184e/YsMjMzERgYiNatW2PIkCF46KGH7JJn33//faGkVvPmzQEAderUsXT+uap9+/YIDw9HbGwsNm3aVOSUc+Yp1gYPHox9+/YVW/eePXvwxx9/4OTJk0hLS4Ofnx9at26NoUOHFrqugvbu3Yv169fjn3/+QVJSEnQ6HQICAtCiRQsMHDgQjz76aKFp3MwLvJvb4d9//8XChQtx7NgxpKWlITQ0FN27d8f48ePRuHFjF1uofMTGxmLRokXYt28fbt26BcA0vVqHDh0wYsQIdO7c2VLWdoF6M/PzSZMm4ZVXXilyEfs+ffogOjoaM2bMwIABA7BgwQJs2rQJ0dHR8Pf3R/v27TFx4kRLZ/GJEycwf/58nDx5EllZWYiMjMSQIUMwfvx4p1PnnTx5EqtXr8bx48eRkJAArVYLPz8/NG7cGH379sUTTzwBjUZjKT958mSsWbPG8nzNmjWW5xcvXrQr4+x9nZeXh1WrVmHTpk24dOkSsrOzERwcjPbt22PYsGHo2bOnw1jNn5dFixahVatWmD9/PrZu3YqYmBio1Wq0bNkSI0eOxEMPPeTw+Ipgfi/XqlULW7ZswYwZM7Bp0ybo9XrUrVsX7733HurWrWt5zbdu3Ypdu3bhl19+QXJyMmrUqIExY8Zg7Nixljpv3bqFRYsW4cCBA4iJiYEgCIiIiMC9996LsWPHOpxS03y/6dChA3744Qd8/PHH2Lt3LwRBQIMGDTBjxgw0b94csixj06ZNWLt2Lc6cOYP09HT4+fmhYcOG6N27N0aNGgV/f/8Ka7+CkpKSsHjxYuzZswc3btyA0WhE7dq10bVrVzz99NNo0qRJoWNceQ26devm0vlv3ryJv//+GwDwwAMPQBRFPPTQQ1i4cCE2bdqEKVOmIDAw0K3XXBTzKFBX1je799570axZMwQFBUGptHZvmO8rZlOmTMGUKVPQuXNnLF682K6Okt7Px4wZg6NHj1qez5o1C7NmzbK7F5jLODofAGRlZWHJkiXYvn07rl69iry8PISFhaFTp04YPXq0w9FbtvfRHTt2QKVS4aeffsLu3bsRHx8PX19f3HXXXRg9erTT+0tJ7u1ERERUfTBJRERERFTAxYsXERUVBQDo168fQkJCXDquXbt2+Pvvv9023c3AgQOxf/9+SJKEI0eOFLs2kbslJydj0qRJhUZFJSUlYc+ePdizZw9WrFiB7777rtwSWIIgYODAgVi4cCE2b96MN99802G5nJwc7NmzB4Bp9FFRSSKdTofJkydjw4YNdttTU1Oxf/9+7N+/H6tXr8Z3331X6LXMzc3FG2+8gV27dhWqNzk5GQcOHMCBAwewfv16LFy4EAqFwmEMy5Ytw7Rp0+xGXMXFxWH16tVYv3495s2b53IHr7udPn0azz33HDIyMuy2R0dHIzo6GuvWrcO4ceMs3+B3l6SkJDz++OO4du2aZVteXh62bduG/fv347fffsO///6LTz75xK7drl69im+++Qbnz5/Hd999Z1en0WjERx99hBUrVhQ6X2pqKo4dO4Zjx45hzZo1WLp0qds+u9evX8fLL7+MK1eu2G1PSEjAli1bsGXLFgwaNAiff/45vLy8HNZx+/Zt/Oc//0FsbKxlm1arxZEjR3DkyBEcPnwY06dPd0u8pSXLcqHPw5UrV9CwYUMYDAbLtoULF9qNSImOjkaNGjUsz1euXInp06cXSgpcuXIFV65cwbJly/Dpp5/i4YcfdhiHTqfD+PHjcfbsWcu2mzdvon79+gBMUwOuW7fO7pi0tDScPHkSJ0+exO+//45FixahQYMGJW+EMtq9ezfeeeedQp+3Gzdu4MaNG/jjjz/w1ltvYdy4cQ6PL+o1cNWqVasgyzLUajX69+8PAHjkkUewcOFCaLVarFmzBs8++2zJL66UIiMjcfXqVRw5cgR79+51mvAAgICAgEKvravcdT8vqVOnTuHVV18t9GWSmJgY/PXXX/jrr7/wzDPPYPLkyU7XEjx9+jQ+/PBDpKenW7bpdDrs3r0bu3fvxhtvvIEXX3yx0DGeuLcTERFR5cfp5oiIiIgKsE2KlPRbte5cD6FNmzaWxydPnnRbva7Q6XR44YUXcOLECQiCgFGjRmHt2rU4cuQI1q5di5EjRwIAjhw5gokTJ1o6hCdMmIATJ07g448/ttR14sQJnDhxolBSxlUDBw4EYOo0PXfunMMyO3bsQG5uLu6++25ERkYWWd/7779viWXEiBFYvXo1jh49io0bN2LixIlQqVQ4cOAA3njjDciybHfsl19+aelQfOqpp7BmzRocOnQI27ZtwzfffGP5xv/hw4eddlwmJiZi2rRpaNiwIb7//nscPHgQO3bswCuvvAKFQgGdToepU6cWOndFkGUZ7777LjIyMtCgQQP88MMP2LVrFw4ePIhffvkF7dq1AwAsWLDA8jmpU6cOTpw4gXnz5lnq2bBhA06cOIEJEya4fO5Zs2bh5s2beOmllywjT958800IgoDc3Fy8+eabmD59Otq1a4dFixbh8OHDWLVqleUzumXLlkIJzV9++cWSIHrooYewfPlyHDhwADt37sS8efPQvn17AMClS5fwyy+/WI6bNm0aTpw4gY4dOwIAHn74Ycv7uDipqakYN24crly5ApVKhRdffBEbN27EkSNHsHz5ckuyd+PGjZgyZYrTeqZPn46UlBS89tpr2Lp1Kw4dOoTZs2db3t8rVqywjP7wlISEBOzatQujR4/Grl27sHnzZnz22WeoXbu2Xblly5ahc+fO+PPPP7F3715MmzYNDzzwAADT6/bBBx8gLy8P9erVw8yZM7Fv3z7s378fX331FSIjI5GXl4d33nnHkggu6MyZMzh79ixee+017Nu3D3/++SemT58Ob29vrF+/3vJZfOaZZ/DXX3/h8OHD2LJlC9544w0olUrEx8dj2rRp5dtYDpw8eRKvvPIKMjIyEBYWhmnTpmHXrl2W17pFixYwGo344osvsHTpUod1uPoaOCNJEtauXQvANPrGPGKoRYsWlpGby5cvL/vFlsDQoUMBmJK8L7zwAl544QWsWbMGCQkJLtdhvgeZffzxxzhx4oRllDBQ+vv5Tz/9hBMnTiAiIgKA9e+eK3/jbty4gRdeeAFxcXHw9fXFu+++i23btuHw4cP49ddfLfezX3/9FV9//bXTeqZMmQJZlvHBBx9g586dOHDgAL788kvLFza+//57y0ghoHT3diIiIqo+OJKIiIiIqICbN29aHjdq1MhjcZg7oADTKAtntFotsrOzi6xLrVY7nYrLkRUrVuDMmTMAgA8++ACjR4+27AsKCrIkOT7//HMcP34cy5cvx+jRo6FWqy0/ZmVd0L1du3aIjIzE7du3sXnzZodrVJg754qbguvQoUP466+/AJimC7Od7iowMBCvvfYaWrZsiVdeeQV79+7Ftm3bLN+sz8zMtCQchg8fjg8++MBybEhICOrVq4dOnTqhX79+0Gq12LdvHx599NFCMeh0OtStWxfLly+3SypOmjQJubm5mD9/Pm7evImzZ8/aJQpdJctyse8HM1EU4ePjY3l+5coVXL16FQDw6aefolOnTpZ93bp1w4IFC9CvXz+kp6dj48aN6NChAwRBgK+vL7y9vS1lvb29S/y65+Xl4f3338fTTz9t2TZhwgQcP37cMg1XixYtsGjRIsv7Kzg4GLNmzcJ9992HvLw8HDx4EB06dABg6vxeuHAhANOUVDNnzrSbQrBOnTro3LkzBgwYgPj4eOzfvx+TJk0CAMt72DxyQKlUunw98+bNw+3btyEIAr777jv06dPHsu/uu+/Gd999h08++QSLFy/Ghg0b8Mgjj6BXr16F6snNzcWcOXPQu3dvy7Y+ffqgXr16lvf55s2bcc8997gUV0Gu3DfMvL29nY6i6NixI6ZOnWp57mgEi0ajwaxZsywJCHOSWafT4dNPP4Usy2jQoAGWL19uNyrx4Ycfxr333osRI0bg1q1bmDp1KrZt22Z3f7EtO3HiRABAzZo10aJFCwCmqe4A0/v3vffes5QPDg7Giy++CIPBYEnWpqamIjg42KU2saXX64ttS4VCYfcZAYBPPvkEOp0OwcHBWL58uV2Cu0+fPujWrRueeeYZnDp1Cv/9738xYMAAhyNbXXkNnNm/f79lRIs5OWP26KOP4vz585ZRPV26dHG53rIYNGgQDh06hBUrVkCWZcvIVQCoX78+OnbsiC5duqBHjx4ICwtzWIftfQ0wfaZtP8NluZ+bX0fz/USlUrl8f5g5cybS0tKgVquxaNEiu3t8165dcc899+DVV1/F9u3bMX/+fDzyyCNo2rRpoXr0ej2WLFli9/dwyJAhCAwMxAsvvACDwYBt27bhueeeA1C6ezsRERFVH0wSERERERWQmZlpeVyaDkN3sV0jJS0tzWk5V9YmmTJlSommCzJ3nrVu3douQWRr7Nix+Ouvv3Du3DksW7bMaTl3GDRoEObNm+dwyjnzguWiKFpGHTlj/jZ+nTp18Mwzzzgs079/f3To0AEnTpzAihUr7JJEzz77LG7fvm2XXLJVo0YNNGzYEOfPn0dKSorTOJ566imHo8569+6N+fPnAzBNN1aaJFFMTIzLHXwF19PR6XSWx44SkwEBAfjhhx+gVCotU3m5i6+vL0aNGlVoe6dOnSwdxE8//XShBEFgYCAaNmyICxcu2I00yM7OxvDhw3Hr1i2MGDHC4RpTPj4+aNu2LeLj44t8vVwlSRJWrVoFwPQ+sk0Q2Xr33XexadMmJCUlYenSpQ6TRM2bN7dLEJk1adIEdevWxa1bt3D79u1Sx/rCCy+4XHbRokVOEwQPPvhgscffe++9Dte02bNnD+Lj4wGYkraOpq0MCQnB5MmT8fLLLyMuLg67du1yOPWms8+++T2dlpYGg8Fgt24NAIwaNQpt27ZFZGRkqdclmjt3LubOnVtkmRYtWuDPP/+0PD937pwlET9p0iSHIyB9fHwwbdo0PPLII8jNzcXatWstnf62XHkNnFm9ejUAICwsDD169LDb9/DDD+PLL7+EwWDAsmXLKixJBJhG0t1111345ptvLGsUAdZp+FavXg1RFNGtWze8+eabJb5XuvN+7qrU1FRL0vKpp55yGLNCocC0adOwd+9e6HQ6LFu2zC6BZdazZ0+HX5jo3r071Go1dDqd3f3Bk/d2IiIiqvw43RwRERFRAbZrAOj1eo/FYdup46iDu7ykpaXh0qVLAIrvfDR3zF66dAmpqanlFtOgQYMAOJ5ybsuWLdDr9ejSpYvdOieOmKfnatWqFXJzc5Gdne3wx7xo+MmTJy3TvkVERODtt9/GN998g8aNGxeqW6fT4dixY8jNzQUAuzVZCjJP7VOQ7SgBrVZb5LWUhyZNmlg66t99911MmzYNBw8etHsv3nPPPWjfvr3La3W5qnXr1oU68AH7NmndurXDY80JN9s4/f398frrr2PmzJkOO7cNBgNOnz6N5ORky/OyunjxomWNkKLWEFOr1ZYF6P/++2+HUwveddddTo8PDQ0FAMt7zZMcdVQXZJ62rKCjR48CMCVDilp3pnfv3paRIc6m2HMWh3mk1fnz5zFixAgsWbLErvM8JCQEvXr1QuPGjR2+/8qL+dqBot8rLVq0sIwMsj3GliuvgSNpaWnYsWMHAFNCqOD1h4aG4r777gMAbNu2zfJZqSjDhg3D7t278dNPP2H06NGFRvZKkoQDBw5g+PDhdtPIucKd93NXHT9+3PJZL+o1Dw0NtUw75+w1d3Z/UKlUloSs7f3Bk/d2IiIiqvw4koiIiIioANvRQ+WZ+CiO7YimgIAAp+V27NhR7Do8JREfH2/pyCpuuj3b/XFxceU28qply5Zo0KABrl+/jk2bNtl1im7cuBFA8SOqsrKyLN8G37ZtG7Zt21bsebOyspCZmVmo/aOionDs2DFcu3YNN2/exI0bN3D9+nWXOxKddcLZjpKRJMmlugoqODqoJLy8vPDhhx/i7bffRl5eHn7//Xf8/vvv0Gg0uOeee9CrVy888MADqFmzZqnqL4qz945t0tbZml/OFnc3i46OxpEjR3D16lXL63Xt2jXk5eWVPmAHYmNjLY8ddTzbMu939h4rqqPW/D4py7pVRY0OKglXPvPOrsU8zVmDBg2cTmcHmEZXNGjQAOfPn0dMTEyJzjF69Ghs2bIFp06dwtmzZ3H27FkApinZevTogb59+6JLly7FvoeKMmnSJLzyyislOsZ87QEBAcUmtxs3boxr167Zvb9slfa+u27dOkuSoE2bNpYvB9jq0KEDdu3aBb1ej1WrVpVoBJo7KJVK9OzZ05JETE5OxtGjR3HgwAFs27YNaWlpkCQJX331FerVq1dk8sWZst7PXVXS+8P+/fudvuYlvT948t5ORERElR+TREREREQF2Hbe3Lp1C127dnX5WKPRWGRnZ0nYLjrtziRQcbKysiyPi1tnwXZKPFfXNymtQYMG4ccff8TmzZvx1ltvATB1GB45cgQqlcoyLZwzpY0vKyvL0oEfHR2N9957D4cPHy5ULigoCF26dMG5c+fsXjtHKnLEQkkNGjQI9evXx7x587B7925otVrk5ORY1gX59NNP8cgjj+CDDz6we/3Lyp11maWlpWHq1KnYunVroYSKr68vunbtisTExEKj00rL9rNT3PXY7s/JySmUJKrM7xFbXl5epS5jbi9XXnvzSKKcnJwSncPb2xu//fYbFi9ejD/++MOyLsu1a9dw7do1LF68GHXq1MFHH31U5Ggmd6uIay+OeWpEAJZ7alGWL1+O559/vkJHthYUGhqKgQMHYuDAgXjvvfcwe/ZszJs3DwDwww8/lChJ5K77uatKcn8o7jUvzf3BU/d2IiIiqvyqxr88iIiIiCqQ7YLOBw8exPDhw10+dvjw4ZZvPk+aNKlMcZw6dcry2Nn0ZOXBNjFUXGLFdn95dyo99NBD+PHHH3Hz5k2cPXsWrVu3xubNm2E0GtGrVy+Ha57Ysl00/vnnn8fbb79dovNnZGRgzJgxiI6OhiiK6NmzJzp16oSmTZuicePGqFu3LgDgySefdFunoqe0bt0a3377LbRaLY4cOYJDhw7hwIEDuHTpEoxGI1avXo2srCx8//33ng7VKb1ej3HjxlnWfencuTO6deuGZs2aoVGjRmjQoAFEUcTbb7/ttiRRwcRPUWw/O+YO4erG3F7FtRVgba/StJVarca4ceMwbtw4XL9+HQcOHMDBgwdx6NAhZGdnIzo6GhMnTsTy5cudTmnobhV17c6cO3cO58+fL9Ext2/fxr59+8o1mbZ//34cOXIEWq0W77//fpFlNRoN3nrrLdy8eRObN2/GpUuXoNVq7e71znjifl7w/lDUGljl8ZoDd8a9nYiIiNyPSSIiIiKiAiIiItCuXTv8+++/2Lt3L1JSUlyao//y5cuWqYxq165d5jg2bNgAwNSxVJLRTGVVu3ZtCIIAWZYt37p35sqVK5bHERER5RpXkyZN0KxZM1y6dAmbN29G69atsWnTJgDA4MGDiz0+ICAAfn5+yMrKQnR0dJFlZVku9G35JUuWWI779ttvnY5c8uQUhe7m7e2NXr16oVevXgBM0zK98847OHv2LLZu3Yr4+HjUqlXLw1E6tnnzZkuCaPLkyU4Xp3fn61WnTh3L46ioKLRo0cJpWfNnx8/Pr9gE553KfM+4fv16kaMwDQYDrl+/DsC+jUujQYMGaNCgAUaPHg2dToelS5dixowZ0Ov1WLp0KT755JMy1e8q87VnZGQgMTGxyCnnoqKi7I5xh9WrVwMwrXe3bds2S1LEkbNnz+Kxxx4DACxbtqxck0QHDhzAwoULAQDjx4936f7SuXNnbN68GbIsIy8vz6UkkSfu5wXvD+a17xwpj9fcVlW+txMREZH7lX7iZSIiIqI7mLlDOSsrC//73/9cOmbmzJmWx6NGjSrT+ffs2WNJOA0ePLjYad/cKSgoCE2bNgVg6mgvypYtWwCY1vcwL4pdngYOHGg5b0JCAo4fPw6NRoM+ffoUe6wgCOjYsSMA0wgx20W9Cxo/fjy6d++OZ5991jJN2cmTJwGY1v9w1qEYFxdn6cwu7ZpCnvTHH3/g0UcfRZ8+fRyud9O4cWO89NJLlufx8fGWx56cgsoR8+sFACNHjnRYJjc3F//88w8A97xezZo1s0wbZ/5sOKLT6SzrRhXVUXynM4/azM3Nxd69e52W2717t2X9qPbt27tcf25uLsaNG4eePXvi999/L7RfrVbjmWeeQbNmzQDYv5/Lm+2I1aLeK+fPn8eNGzcAmNYHcgedTod169ZZ6iwqQQSYRp+0bNkSgOm1KM92Mt+jAeC3335z6Rhz+9SoUcPlhKsn7ucdOnSwrH1V1GuenJyMv//+G0DJ3u9FKcu9nYiIiO58TBIREREROTBw4EB07twZALBixQp8+eWXTheJlyQJ//3vf7Fr1y4AwAMPPFCmkT9RUVF47733AJhGEZV12rrSGDFiBADTN8gdda4CwK+//mqZrqgkU/KVxaBBgwCYOgVnzZoFSZLQp08fl6fkMV9XWloavvzyS4dltm3bhv379yM5ORn16tWzJD/MoxzS09ORmJhY6Li8vDy8//77lveJXq8v2cVVAn5+fjh//jyio6MtI9kKMr/moijarZVlOwqkMly7bTy2I97MJEnCtGnTLOuEOIrZvO6Hq9ejUCjw+OOPAwC2bt1quScU9NVXXyE5ORlAxX12KqPevXtbRtB8/vnnSE9PL1QmNTUV//3vfwEAISEhLiWEzXx8fBAfH4/4+HgsX77ckmiylZ6ejpiYGABAvXr1SnMZpdKmTRvL1HazZs1yOLpRq9Xi448/BmBad2jIkCFuOff27duRlpYGAHjkkUdcOmbYsGEATOvurVixwi1xONKrVy/Ur18fALBgwQKsWbOmyPKnT5/GypUrAQBPPPFEof3OPsPuuJ+X9P4QEhKCfv36ATAlwMwjHW1JkoSPP/4Yer0egiBY2r2synJvJyIiojsfk0REREREDgiCgJkzZ1o6q+bPn48hQ4Zg2bJluHDhAtLS0nDr1i38+eefGD58uGV6nGbNmuHTTz91Wq8sy8jOzrb7ycjIQExMDA4ePIgZM2Zg2LBhSEpKgiiK+OKLLzwy5cvIkSPRpk0bAMD06dPx8ccf48KFC0hPT8eFCxfw8ccfY8aMGQBM33R+5plnKiSuBg0aoFWrVgBg6ah86KGHXD6+b9++uP/++wEAv//+OyZOnIhjx44hNTUVV69exQ8//GBZwD04OBgvv/yy5dgePXoAMHXiTZgwAYcOHUJycrLlfTBs2DDs37/fUr649ZzKi6P3WHE/Zn379kWDBg0AAP/3f/+HH3/8EZcvX0ZqaiquXLmC77//HnPnzgUAPPjgg3bTMNqOJNuwYQMyMjLsFmqvaObXCwDeeust7NixAwkJCYiNjcW2bdvw1FNPWabcAhy/XuZrOnbsGG7cuIGUlJRiz/vSSy+hTp06kGUZr7zyCr755htERUUhLS0N//77L15//XX8+uuvAIABAwbgwQcfLOOVlo5Wqy3Re6Q8En9qtRpTp04FYJpybvjw4diwYQMSExORmJiIjRs3YsSIEbh58yYA4LPPPivxGi3jxo0DAFy8eBFjx47Fvn37LImjPXv24LnnnkNmZiYUCoXTEWfl5cMPP4RKpUJqaipGjhyJFStWIDY2FikpKdi9ezdGjRplGfHyn//8xy3TmALAqlWrAJja3zw6szhDhgyBl5cXAGDlypUwGo1uiaUglUqFb7/9FgEBATAajZg8eTLGjBmD1atXWz5H8fHxOHToEKZNm4ZRo0YhJycHbdu2xXPPPVeoPvNneMeOHUhJSbFMH+eO+7m5bvN7ypX7w7vvvouAgADodDo888wzWLBgAW7duoXU1FQcOXIEzz33nGWU0dixY922HmFZ7u1ERER05+OaRERERERO1KxZE0uXLsV7772H3bt349KlS/jwww+dln/wwQcxffp0y3RTjsTExLg0ZVBoaCg++eSTEn1r3p3UajXmzp2Ll19+Gf/88w+WLFmCJUuWFCrXo0cPfPnll5ZvVFeEgQMH4ty5c5BlGYGBgXbJgOKYk39vvfUWdu/ejR07dmDHjh2FyoWFhWH27Nl2Cbphw4Zh48aN+Pvvv3H27Fk8++yzhY5r1KgRWrVqhfXr1yM6Ohp6vR4qlapU11larr7HbP39998ICAiASqXCd999h+eeew5JSUn49ttv8e233xYqf9ddd2HatGl22+rXr4/w8HDExsbi+++/x/fff4+hQ4fi888/L9P1lFavXr3w0EMPYcOGDbh58yYmTpxYqEzNmjXRp08fLFu2DLm5uYXW4ejSpQs2btyIuLg4y5RUO3bsKPJb9oGBgVi4cCFefPFFXLt2DbNnz8bs2bMLlRsyZIhllIgnvPDCCyUqP2XKFIfv+bLq378/PvnkE3z88ce4ceMG3nzzzUJlNBoNpk2bht69e5e4/qFDh+LUqVNYunQpjh8/jvHjxxcqo1KpMH36dMu0cxXlrrvuwqxZs/Dmm28iMTERH3zwQaEySqUSb775JkaPHu2Wc8bFxeHgwYMATCO5ivp7ZSsgIAAPPPAA1q9fj/j4eOzatcsyKsbdWrZsiV9//RX/93//h7Nnz+Lo0aM4evSo0/L9+vXD9OnTodFoCu3r0qULNmzYgD179qBbt26oU6cOdu7c6Zb7eZcuXXDq1CmcO3cOPXv2hEqlwsmTJ4u859etWxcLFizAxIkTkZiYiC+++AJffPFFoXLPPfec5QsL7lCWezsRERHd+ZgkIiIiIipCaGgo5s6di+PHj2PDhg04efIkbt++jezsbHh5eSEiIgIdO3bEY489Vuq1RQRBgI+PD8LCwtC0aVP06tWrwtchciQsLAxLlizBunXrsG7dOpw7dw6ZmZmoUaMGmjdvjscffxx9+/a1rLFQUQYNGmRZ/+mBBx6AWq0u0fF+fn6YO3cutm/fjrVr1+LUqVNITU2FSqVCw4YN0adPH4wZM6bQ2hZqtRoLFy7Er7/+io0bN+LatWvQ6/UICAhAkyZN8OCDD+Lxxx/HmTNnsH79euTm5mL//v2l6tj2pObNm2P9+vVYtGgR9uzZg+vXryMvLw+BgYFo0aIFBg0ahKFDh9pN5waYOrPnzJmDzz77DKdPnwbgudFUZjNnzkSXLl2wZs0aXLp0CXl5efDz87O8ziNHjkR2djZWrFgBSZIsI4zMRowYgaSkJKxatQqJiYkICgpCXFxcsVMxNWjQAH/99RdWrFiBzZs34/Lly8jJyUGtWrXQrl07DB8+HN26dSvvy68yhg8fjq5du+LXX3/FgQMHEBsbC4VCgcjISPTu3RsjR45EeHh4qev/6KOP0Lt3b/zxxx/4999/kZycDJVKhVq1auHee+/FmDFjLKMsKtr999+PrVu34tdff8WePXtw69YtAEBERAS6d++OJ554Ao0bN3bb+dasWWNZX8fVqebMhg0bhvXr1wMAli5dWm5JIgBo1aoV/vjjD+zevRt79uzByZMnkZKSgrS0NPj4+KBmzZro1KkTHnroIcvUsI5MnToVSqUSe/bsQU5ODgDTVHJeXl5lvp9PmjQJubm52Lx5M9LS0hASEoK4uLhi13hq164dNm/ejN9++w07duywnLt27dro1KkTRo4c6bYRRLZKe28nIiKiO58gO5tcn4iIiIiIiIiIiIiIiO5YXJOIiIiIiIiIiIiIiIioGmKSiIiIiIiIiIiIiIiIqBpikoiIiIiIiIiIiIiIiKgaYpKIiIiIiIiIiIiIiIioGmKSiIiIiIiIiIiIiIiIqBpikoiIiIiIiIiIiIiIiKgaYpKIiIiIiIiIiIiIiIioGmKSiIiIiIiIiIiIiIiIqBpSejoAskpJyYQkeToKzxIEIDTUH8nJmZBlT0fjeWwPK7aFFdvCHtvDim1hj+1hxbawYlvY6969E+LiYlG7djgOHjzm6XA8iu8Ne2wPK7aFFdvCHtvDim1hj+1hxbawYlvYY3tYsS3ssT2sRBEICfEv9/MwSVSJyDKq/RvfjG1hj+1hxbawYlvYY3tYsS3ssT2s2BZWbAuTrKwsZGZmws/Pn+2Rj+8Ne2wPK7aFFdvCHtvDim1hj+1hxbawYlvYY3tYsS3ssT0q7vo53RwREREREREREREREVE1xCQREREREVE15evrC39/f/j6+no6FCIiIiIiIvIATjdHRERERFRNHTp0HGFh/khK4nzfRERERERE1RFHEhEREREREREREREREVVDTBIRERERERERERERERFVQ0wSERERERERERERERERVUNck4iIiIiIqJr66KP/g1abDW9vX3z44SeeDoeIiIiIiIgqGJNERERERETV1OrVfyA2Ngbh4RFMEhEREREREVVDnG6OiIiIiIiIiIiIiIioGuJIojuILMuQJCMkSfJ0KKUmCIBWq4Ver4Msezoaz2N7WFW2thAEAQqFEoIgeDoUIiIiIiIiIiIiolJhkugOIMsycnKykJubCYNB7+lwyiwlRazSiS53Y3tYVba2EAQR3t4a+Pj4Qa328nQ4RERERERERERERCXCJNEdIDMzFTk5mfDy0sDPLwgKhQJA1R3doFAIMBorwVCRSoLtYVV52kKGJEnQ6fKg1WYjNzcbQUFh8PbWeDowIiIiIiIiIiIiIpcxSVTF5eZmIycnEwEBodBo/DwdjlsolSIMhsozWsTT2B5Wla0tvLx84OcXiPT0ZKSlJSEkpBZHFBEREREREREREVGVIXo6ACobrTYbKpX3HZMgIqpqBEFAYGAoFAoFcnOzPB0OERERERERERERkcuYJKrCZFmGTqeFl5ePp0MhqtYEQYC3twZabQ5kuTJMh0dERERERERERERUPCaJqjCj0QBZlqFSqT0dClG1p1Z7Q5YlGI1GT4dCRERERERERERE5BKuSVSFmUcsiKLg4UiISBRNOXdZrjxrJhERERXngQcGICcnExqNv6dDISIiIiIiIg9gkuiOwCQRkefxc0hERFXPzJnfIizMH0lJmeCMqURERERERNUPp5sjIiIiIiIiIiIiIiKqhpgkIiIiIiIiIiIiIiIiqoaYJCIiIiIiIiIiIiIiIqqGmCQiIiIiIqqm+vXrhcjISPTr18vToRAREREREZEHKD0dgLt88sknWLx4MWbMmIHHHnusyLJ6vR5Lly7FX3/9haioKMiyjDp16qBfv34YO3YsgoKCijz+4sWLmD9/Po4cOYKUlBQEBQWhTZs2GDVqFHr27OnGq6LKLDExAZs3b8DRo4dx/fo1ZGZmQKVSo2bNmmjdui0efPAhdOjQyeGxn376ETZtWg8A2L//mEvnK+qY2NgYDB8+BAAwduzzGDdugt3+Hj2scQiCgNWrN6BGjZounXfKlLewb98ey/OC57aNqyTuvrsDZs2aZ3l+4sQxvPrqi8Uep1arERAQiHr16qNbtx547LFh8PLyLlTOtk1K6rvv5jh97YiIiO4kCQnxiI2NgSTJng6FiIiIiIiIPOCOSBJt374dv//+u0tl8/LyMH78eBw9etRu+5UrV3DlyhWsXr0aCxYsQLNmzRwev2PHDrz22mvQ6/WWbYmJidi1axd27dqFMWPG4P/+7/9KfzFU6eXlaTFv3mysWrUcBoPBbp/BYMCNG9dx48Z1bNy4Dl26dMcHH0wrNvFYkWRZxq5d2zFixKhiy2ZlZeHw4YMVEJXrdDodkpISkZSUiBMnjmHNmpWYNWseatas5enQiIiIiIiIiIiIiKqUKp8k2rlzJ15//XVIkuRS+SlTpuDo0aNQqVSYNGkSBg8eDLVajT179uDLL79EQkICXnzxRaxfvx4ajcbu2HPnzuHNN9+EXq9H27Zt8e6776Jp06a4ffs25syZg+3bt2Px4sVo2LAhRo8eXR6XSx6WkZGBt96ahPPnzwEAWrRohUcffRzt2t2FoKBgpKWl4tq1a1ixYglOnTqJI0cO4sUXx2Lu3J8RGBjk2eBtuJok2rNnp11CtDhbt+51uaxC4Xy2y7ffnoL+/Qc6OEZARkYmLl26iGXLfsPJk8cRExON999/Bz/9tMhpfWPGjMWYMWNdjs3Ly8vlskRERERERERERERVVZVdk0iSJHz33Xd4+eWXXe7EPn36NDZs2AAAeP/99/Hiiy8iMjISNWvWxPDhw/HLL79ApVIhOjoaixYV7nD+9ttvodVqUb9+ffz666/o3LkzgoOD0bZtW8yaNQsPPvggAOC7775DVlaW+y6WKgVZljF16mRLgmjUqDH46adfMXjwI6hXr0H+FGgN0KtXb/zww0+YMGESAOD27Vv47LOPPRm6RePGTQAAZ86cRkJCfLHld+zYCgAIDQ11qX6NRuPyj6Mp4szUarXDY3x9fVGjRk3ce+99+Pbb2ejUqTMA4Pz5czh27KjT+pRKZYliUygULl0vERERERERERERUVVWJZNE+/btwyOPPIIffvgBkiShdevWLh33888/AwAiIyMxYsSIQvtbtWqFRx99FACwcuVKu31RUVHYvXs3AGDChAnw9fW12y8IAiZPngxRFJGWloZt27aV8Kqostu8eYMlEfHww0MxceJrEATBafkxY55F9+49AAAHDuzD6dOnKiTOonTtei80Gl/Isozdu3cUWTY1NRXHj/8NhUKBnj37VFCErhNFEU899azl+alTJz0XDBEREREREREREVEVVCWTROPHj8elS5egUqnwyiuv4Jtvvin2GFmWsW/fPgBA7969nY4U6Nu3LwDg9u3buHDhgmW7+VhBENC7d2+Hx4aHh6Nly5YATOsk0Z3l999/BQD4+PjghRcmunTM2LHPQxRFNG7c1KWRO+VNrVajR4+eAICdO4t+j+7atR1GoxEdO3auVGsq2apRo6blcXJykgcjISIiIiIiIiIiIqp6qmSSSBAE9O/fH3/++ScmTZoEUSz+Mm7fvo2MjAwAKHLkUatWrSyPz5w5Y3l8/vx5AEBERARCQkKKPf7s2bPFxkRVR1TUFVy/fg0A0LPn/QgODnbpuJYtW2PTpp349del6Nu3f3mG6DJzHGfPFj3lnHmquQceGFAhcZXGtWtRlsdhYTU8GAkRERERERERERFR1aP0dAClsWnTJjRs2LBEx0RHR1seR0ZGOi1Xo0YNqFQq6PV63L59u9DxRR0LmJJIABAXFweDwQClsnI0sdEIpKY6nxqtMlEqAYOhZLEGB8soz2VkTp/+x/K4ffuOJTrW19fPzdGUTefOXeHn54+srEzs2rUdI0eOLlQmMTEB//77D9RqNXr2vB/Llv3ugUiLlpenxaJFP1uem0dIEREREREREVUIvR5CejrEjDQI6emmn4x0iOlpgDEP6pp1kNfnAcDb+Zq8REREnlY5MhglVNIEEWBaX8UsICDAaTlRFOHr64u0tDTLyCPb4wMDA4s8j7+/PwDT9HYZGRlFjjqqKH/9pcTkyV5ISqqSA8dcEhYm4fPP8zBkiKFc6rdNGNar18Ctdffo0cmt9RVHpVKhZ8/7sXHjOuzatcNhkmjHjq2QZRldu95boiRXTk6Oy2U1Go3TfTqdzmFdsmxAcnIqzp07g6VLf0NU1GUAwODBj6BZsxZO6zMYDC7HplKpoFKpXCpLREREREREVZjRCCEjPT/Rkw4hLc36OD0dQkYaxPT87Rnppsfm8unpEHKyi6w+AID2seHInLOgYq6HiIioFKpkkqg08vLyLI+9i/kGh5eXV6FjzI/N+5yxrVun05UoRkEw/ZSkvCvefNMbGRlVYxRRaSUliXjzTW8MGZJVLvVnZ1vrrazr85RE3779sXHjOpw9exrx8XGoVau23X7zVHP9+pVsqrn+/V0fzbN//zGn+776aga++mpGsXUoFAoMHTockya9XmS5xYt/xuLFPxdZxmzs2OcxbtwEl8o6UtLPcWnPYfu7umN7WLEt7LE9rNgWVmwLex9+OA0KhQyjUaj2bcL3hj22hxXbwoptYY/tYeWxtpAkCFmZhZM76ekQ0tMgphcc4ZOf5DGXz8os9xC9NvyFrGr8HuHnxIptYY/tYcW2sMf2sKqoNqg2SSJFGeciK+vxrggJ8S9Rea1Wi5QUEQqFAKXyzh0lVBLl1Q4KhbVeSTKW+TyCzSd85879Lh3z3/9+ii1bNgEofJ228Ymi8/eDeV+XLl0QGBiE9PQ07N27E08++ZSlzO3bt3D+/DloNBr07NkTSqUIUbTGW7BuoZR3q6KuoTgdOnRCp0734IEHBqBu3XoOy5SkPltFtV9RJEmAKIoIDvYtNhHtLqGhJbtn3OnYHlZsC3tsDyu2hRXbwmTChHGeDqHS4XvDHtvDim1hxbawx/awKnFbyDKQlQWkpdn/pKYW3uZoe3q6qY5KTOjcGWGhftW+t5OfEyu2hT22hxXbwh7bo+JUmySRj4+P5bHtCCFHzPttO3rNxxc3Okir1VoeFzfqqKCUlExIkuvl9XodJEmC0SjDYHB+4Ndfa6vNdHNFtUNZBAQEWR6npKSW+Tyyzf/EqtVFJxSUShEGgwRRtCYqC57faLQ+lyTn7wfrPhG9evXGX3+twfbt2zB8+ChLmS1bNgMA7ruvF5RKNQwGCZJkjbdg3bbXUtTooIKKuob33vsQgwY9bKk/KysLhw4dwI8/foOkpCTExESjbdvnER4e6fRabesr6eig0ry+RqMMSZKQmpoNlUpf4uNLQhBMfyiTkzMr+7+HKgTbw4ptYY/tYcW2sGJb2GN7WLEt7LE9rNgWVmwLe2wPmBI0ubkQM9IRIhqQfj0aSEuzH9GTkQ4xf+SOdaq2NMtjwWj09FW4heztDTkgEFJgIOTAQKjCQpHTojVyx02AnFw+s55UBfycWLEt7LE9rNgW9kzt4Yfk5Iz8fkcJgGzzY30uCI72oVBZQSi6Hms5ON1n+9xUX8H9KFTWPr6irkN2uE8QlPD3f7LsjVqMapMksl2HKDPT+XBiSZKQnW2aUzY4ONiy3bzWUFHHArCsY6RQKIpdv6ggWS7ZF2BcLTtkiAEPPWRAamrV+NaKUinAYCjZHTE4WEZ5DvZq0MC6DlZ09G106OD6OkJGo7FCRqKVVN++/fHXX2tw7twZuynnSjvVXHkRBAH+/v7o3/9B3H333Xj++WcQFxeLt956FV999W2JXouKUNLPcVU5V1XA9rBiW9hje1ixLazYFvbYHlZsC3tsDyu2hRXbwl6Vb4+8PAiWxI7N2juWKdtsp2tLs0zXZkkE6a1fkitZL0jlIiuVkIOCIAWYkjxyYBCkwCDI+c+l/G2WxwH5ZfL3w+aLxoIAhIX5Iycpv8O3Kr8/3KTKf07cyNoWzjqrHXUeF+xoht1zawezK53pcFC2qPMV7gR33EFe+LjCHfgF93lDpcqBLEuQIQGQIEOGACl/W36dggTIsk0Z23qNAGTIsqlOGTZlzeeXZWtd5mPz67Q7p9112F+XbBO3DBmQ7dvMss2FpIBgPn9+ewIyUnJE6JV6u23OEggCJEDIfyPZvK6C7TmF/PNYjjFts3tus01wUE4w11vwHIKj8rBuK1hnfuLFeiwKlIPdPiG/vtBQEPwBMEnkNg0aNLA8jomJQceOHR2WS0xMhD7/f3DCw8Mt2xs2bIijR48iJiamyPPExsYCAGrVqgVRrDwjdxQKICysavw1Lk2SqLy1a9fe8vjYsSN4+OFHXT72+eefgVKpRLdu92Ls2OfLIbrSad++I0JDQ5GcnIxdu7bjiSeewrVrV3H1ahQCAwNxzz1dPR1iIREREfjoo8/w2msvQafLw3vvvYNfflmK2rVrF38wERERFXLlymXEx3shMzMPjRs39XQ4RETkKlkGtFoIOTkQcrLtf2dnmX5nZNis05PmeERPRjoEmxlRqjJZFJ0ndwICCySAAiEFmBI+5u3w8XF5SjitQYuYrNu4nXUbSRmnIKVLNrNsGKFUaBEYJ0CbmwGVmAelmAelwvRbIeogCLYd0jadxpbO1gLbbDpj5fxj86/a1JkqOz7OYaewTXm7DmQnnbrO6inY6Wvp6BUkUzHBXE7G9RgRRqPR8tw2dgG2ncLWjmZzOUEoGJPtfhmiJQ5YygiCfT3mV1UU7K/N9ljB0jnt2j4BsOvQNu83t42p0zt/n2B9bJSAkFBAdO2tVi3YfK+fiDyk2iSJatasiaCgIKSlpeHcuXN4+OGHHZY7e/as5XGrVq0sj5s1awYAuHXrFrKysuDn5+fw+HPnzgEAWrZs6a7QqRKoXbs2WrZsjfPnz+Lw4YNIS0tDUFBQscddvRqFS5cuADC9BysTURRx//19sWrVCuzatQNPPPEUdu7cBgC4//6+UCor5+2hffuOeOKJ0ViyZDGysjIxffoHmDVrXqnXRiIiIqrOHnvsYcTGxiA8PAKnTl3wdDhERHcWWQZ0OpvkTbZdQge227JznJaz3YacHOv2ksxXXwXIgmBN7JiTOQGBkIKsCR/rvqBCSR/Z19V1f2QAOghCNgQhB4KQCaUQl/84GzKykaWLQ7o+Blm6OOQaE5FnTIFeSoOEDABZUCp08FUB9WsBrVSArxrwzf/tXTn/KU1ERORUtfrT1atXL/z555/YvXs33n33XYedyjt37gQA1KhRAy1atLA7dvr06TAajdi9ezcGDx5c6NjY2FicP38eAHDfffeV01WQpzzxxGh8+OF7yM7Oxty5P+A//3m/2GPmzp1lefzYYyPKM7xS6dOnP1atWoFz584gISHekiSqLFPNOTNu3ATs3bsHt2/fxKlTJ7F27SoMHTrM02EREREREVFVZJvIMSdgbJMy2Y6SNzkQcrMBgw4BKWkQcnLyEzgFyt4h6+24SvLzt0/umB9bpmcLhBQUXDgZFBgI2T8AsMzIIgHItSRu7H+bHotCEpRCNoDC+wQh124bUHB/0a9LEIDIcm0pIiKqKqT8AZhyEY9lOf85Sl62qOMEAM3Dyv8aq1WSaOjQofjzzz9x9epVLFmyBKNHj7bbf+7cOaxduxYA8Mwzz9glkerWrYuOHTvi+PHj+P7779GrVy/LOkUAIMsyPv/8c0iShODgYDzyyCMVck1Ucfr0eQBr167CyZPHsW7dGvj7++Oll15xmGyUJAmzZ3+PAwf2AQB69uxd6dbOAYB27e5CzZq1kJAQj19+mY8bN66jRo2auOuu9sUf7EFeXt54550peO21lwCYknH33Xc/wsIq4K5JREREREQVz2AoNJ0a7EbfZNkneRxsg6NET0623Zo6paF20yVWBrJGY0rc5I/esUvuBAZCDgy2JneCfIEQNeRAFRCsQnCkHzKyUmFK2jhK3ORAFK5DgXNFJn9MCZ4cTzcFUbUiyaXvxC5J2YroVK+QeEpYj91v2bLSj6WsZTUf2TQBo7W8ZSLFAsdaH8sQANlcRrA5n2XSQ9M5bLYVjMEyYaO5HgiAnH+cYHqcP5li/npatvUIhc5nOaelnPmxfQy2dViOzY9NqVLCoJdN5QTzNYrWsg7OiUJ1iabrFMyP7c8NQYAgC/n1AgJE07UK1jgEQQQgmP7L7wM2Py60zWa76bSCg2Ph0rHIf6xR+WDmQyh31SpJ1K1bN/Tp0wc7d+7Ep59+ioSEBAwbNgze3t7Ys2cPvvzyS+j1ekRGRuLJJwsvCDVlyhSMGDEC169fx6hRo/Cf//wHrVq1QmxsLGbPno1t20yjMF555RVoNJqKvjwqZ4Ig4MMPP8WkSc/j9u1bWLJkEY4cOYihQ4ejTZt2qFmzJrKysnD69CmsXLkMFy6Yph5s3LgJJk/+wMPROyYIAnr37ovly5dg3bq1AEzJsNKup5WTU7L/kff29i71uTp2vAcDBgzCli0bkZWVhW+++QKffPKFw7IGg6FEsSkUCnh5eZUqLiIiIiKiastotCRg4HD0TdHb4GQ0j5CTA0Gn8/TVVXqytzdkjQayxtf0298fcg1/SDV9gRoayKEayMFeQLAKCFJC9lcCASLgB0ADwNsIQakrlMARhVsALjpI5hROrrkwK3u1I8lArl5ErkGEVi9Ca1DkdyDbdwQX7LC177gV7MpZO45h08Ga/7hAh7P5sanT1rY+285b+7KyTYew6RrMna6wdMJaO6bN/6Yv2NkLSLJpn1KpgMEg23V4F+zMtXTaWlYgEgHLtYiWfdZzFjy2QFkHx5rPa+qEFVGwg9lyLbJt/aIlZmsHsmhpZ0EQIcuipRPbVN7mnLJo01EsQqPxRna2zlRGEG2OsR4r5HdomzuxIYsQ8uM1x2461jZG0bTPsl20dm7nd5SbnpvqMreHkF/OaQe4TWe3YO5Uh205FD7WQUe5o2NFUUBIsB9SU7NtjywQByzHwnKVjuu3HCvkb7M5r/PY4OS8rhzruI0KJwmKJwhAWJg/kpIyLZ/J6oztYVXKbtMSq1ZJIgD4/PPPMW7cOJw+fRpz5szBnDlz7PaHhYVh4cKFDtccatu2LT799FN88MEHuHTpEsaNG1eozNixYwuNUKI7R1hYGGbPXoAZM6bh4MH9iIq6gq++muG0fO/e/fDuu+/bjTqrbPr06Y/ly5dYFtl84IHSTzXXv3/PEpX/+eff0bRp81Kfb9KkN3Do0AFkZKRj9+6d2L9/L3r0KBzD4sU/Y/Hin12u9777emHGjJmljouIiIiIqNKSJAcjamySM5bRN/bbkJMDMScHMOQhIC3D8cgdrdbTV1fpyV5e9okcPw3kEC/Iod5AkBoIUgGBSsgB5gSOCPgKgK8MaGTARwK8jYCXAYJaD0GtB1R5EBR5EESbKdaQmp/MubPWLSoveQYgWw9k64Acvelxjl6AUfKCLPtCgD+UQiC8FCHwVoTBV1UL/qraUIuhkGUNAA1k2ReybPoN+CAkpBaSkiTIsjfMSROv/J9CBCeP7wDs7LViW9iztIfI9iDytGqXJAoMDMTSpUuxdOlSrFu3DlFRUdDpdKhTpw569+6N559/HqGhoU6Pf+yxx9C6dWssWLAAR44cQXJyMjQaDdq0aYNRo0ahX79+FXg15AnBwSH44otv8O+//2D79i04c+ZfxMTEIDc3B2q1F2rVqo127e7CoEFD0KZNW0+HW6zWrdsgPDwCsbExiIysixYtWnk6JJcFBwfjpZdewX//+wkA4Ouv/4sOHTpxJB8RERER3Tm0WohxsRCTk6xr4ziYTg1FTrtmkxDKzS1zSHfS9GqOyCqVNYnj62t5DE3+SJxGEhAiQg5UwjtUjTy1EbKvaEriaGTA25TIEbxNiRyodRCUeYAiD4Jou6ZOIgQhz9OXW2Vk66yJHIe/CyR5iivvpQhBsDoSId71UNu3Lur41UWkfyTq+OX/aGpCFJx/hVvWA85ePdPgAX8AmeXQEkREVN7i4gTExJin+bP9sU7NZ/5d8LHtT8HtZkWXtZ5XrQZGjSr/6xVkmbnayiI5ORNSCb7ko9frkJwci9DQcKhUd87/piuVIgwGftvJjO1hVZnboiI/j/z2kT22hxXbwh7bw4ptYcW2sHfXXS0QGxuD8PAInDp1wdPheBTfG/bYHlbl1hayDCEtFWJsLBSx0RBjYyHGxpgSQrExUMTGQoyNhpiS4saT3jlkhQKyr1/+qBzTyBz4+tqP0tH45id5bLflJ358fQFNgX35CSGoAIXiOhSKK1AoovJ/m39iPH3plZgCkmQeTaMBYH2ckJ2No3FnkJiTU2TyxlmCJ1cPlOTjp1FqTIke/0hE+tVFhF8dRPrXtWyL8K0Db6V3eTUE76E22BZWbAt7bA8rtoU9T7fH1KlemDOncvS1+/sDGRnlf55qN5KIiIiIiIiIqFzp9RDj4yxJH0VsDMSYGIhxMflJIdP2O316NlkU7RIwsE3UWBI4Drb52ozccVIOarVlrYhSRgdRjIVCcQVKxfECiaDrEASj29qhMpFlb8uUaOYEjvmxaco0TYH9jsoV3gf4IiwsFCkp9h166Xlp+PDA+1hyYbHbrkEURNTWhOcngCJRx78u6vjVQR2/upZtQV7BLq8FQkREZLZ9u6LSJIgqEpNERERERERERC4SMjNMCR/bBJB5BFBMDBSxMRCSEiFUka8Cy4JgSroUlbwxj7Dx1UBTIwRZssLJKJ0CI3K8vMqYyCk7QUgtkAAyjQxSKq9AEHI8GpsjpilmfGFN2DhP1tiXK7jPcTlAUS5xO3qZd9zYijd3v4rY7JKNvgryCkKEnzkBFGlK/vjVQR3/uoj0i0Rt33AoRXZnERGRe+XlAf/3f+U3yrQy419VIiIiIiIiIqMRYmKCKeGTP9WbIjYWYlwMkJSAoJu3TNuzszwSnuztDdnPPz85o3GYlLGMsnGwzVE5WaMBfHxcTuQIAqAJ84e20k2HkwuF4qolEaRUWpNCopjs9rOZEjmBkGVfKBR+MBi8HSZmnG0zJWwcldMA8AZQtUfAZOSlY+qB95yOHor0q4v6AQ0cjwTyqwM/tX8FR0xERATMnavG1av2a9H5+MgQBBT6AQpuk+22OXtc3LaC2/39AcD5+njuwiQREREREVE1tXXrbgQF+SAtrewL2RNVatnZUMTFWNf9yU/+KGymgBMT4iEYnU8xVp7/eJbCasAYHgEpIgJS7QhI4eGm57XDIYWbnssBgR4fleNZRojizUJJIFMi6Lalc8adJCkMRmMTGAxNYDTa/jQE4GNZMyEtrbIlzTxnx41teHPXq4jJji60z0/lj4/v/RRPtXyGU8EREVGlEhsr4Ouv7aeZ69jRiA0bciCWf47GKdO5y//LE0wSERERERFVU7Vr10ZYmD+8vdnBSVWXkJQERWy0zRRw9uv+iDExEDPSPRKb7OUFqXZ+wic83JQAiojITwDlb6tV27S+DgGQIQgJhZJApp9rEASd+88oa2ySQI3tkkGyHOz2892pMvLS8Z8/X8fCfxY63N8rsjf+13sWIv3rVnBkRERExfv4Yy/k5Fi/wCAIMmbM0Ho0QVSRmCQiIiIiIiKiKkVISoL38iXwXrIIysuXPBKDFBwMKbwOjOH5o31sRv0Y85NBcnBINR/945ggZFrWBlIoLtutFySKGW4/nywrYTQ2KDAayPQjSeGo6tO7lTejZERibgJisqIRnRWNWPPv7GjEZMUgJisacTmxMEiGQsdy9BAREXmawQDExwuIiREQGysiJkZAdLSI2FgBMTGm39HR9tmg0aP1uPtuyUMRVzwmiYiIiIiIiKjykySo9u+F92+/wGvDOgh6fbmcRlYqTQkfm6SPT9NGyAgIgVQ7Asb8ffDxKZfz3zn0UCiuW5I/9smg2HI5o9EYkZ/8aVpgVFB9AKpyOWdVV5YEUHE4eoiIiMpbXh4QF2dN/tgmgsy/ExIESJLrX1QIDJTx3nvuH71cmTFJRERERERUTS1a9DMAAwAlxowZ6+lwiBwSEhLgvex3+Pz2CxTXr5WpLsk/IH/dH1MCyGieAi5/PSBj7QjIYWGwnVtEEACfMH/okjgtY2EyRDG+QALI/Pg6BKHkiYXiSFJQodFApuniGgHwc/v5qrLyTAAVhaOHiIjIHXJz4TDpY34cHS0gKcn988H95z95CAurXv/TxyQREREREVE19dVX/0VsbAzCwyOYJKLKRZKg2rMLPot/gXrzBgiGojuxZVGEVLOWg3V/zKOBTCOA4MckQmkIQgYUiigolZcB3IKf31lLUkgUs9x+Pln2thsJZF0zqAlkOQScHs5zCaCiCBDQv8GDmHHfVxw9RERERcrKMo0Asp32zTYRFBcHJCf7V2hMgiBj+HADnn22fEarV2ZMEhEREREREVGlIMbFwnvpb/D+fREUN28UWVb28kLekKHQjnkW+k6dASX/eVs2ttPDFRwZFG9X0tu77GeTZQGSVB9GY+P8JFBTm3WCIgFUk5WiHaiMCSAAUIkqhPtGINwvAnX86iDct47pt18dRPrXwd0NWgM5XhxxR0RUjckykJmJQkkf2/V/YmJEpKdX/Bc+vL1lhIfLiIiQCvw2Pa5bV0JISIWHVSnw/6KJiIiIiIjIc4xGqHfvgPeiX6DeugmC0VhkcUOLltCOeRbaYSMhB1fTf8mXmgxRjHUwNZx5erii2740JCkURmNTm9FA5mRQQwBuyDZVMVUxAWT+XcOnBkTBcfJOEIAwjT+ScjIrNG4iIqo4sgykpcEu2WM79Zt5W3Z2xSeANJrCSZ+CiaCQEBmcBdUxJomIiIiIiIjII1T798L/9ZeLHzXk44O8Rx5D7phnYejUGfwXvq1ciGISRDEJgpCU/zi5wHPTY4UiHoKQ7fYIZNknf0RQU5tp4kyPTdPD3RlkWUa2IRuZeRnI1GUiU58OIc2A20nxyMjflqFLR5YuExk68/MMZOkyLM+TtUkeSwBF+NVBhF9EiRNARERU/SQlCTh7Vsz/UeDsWRHXronIza34/wfz9y+c9KlTx35bQAD/97AsmCQiIiIiIiKiCqc8eRyBo4ZB0GqdljG0aoPcMc8ib9gIyIFBFRecx8gQhKwCyZ1ky2PHiSD3J30cRiaLkKR6UChaIDe3od3IIEmKQGWfHi7PmGdK5OgzTEkefWZ+Ysf8Y03sZOrSCzw3lc/UZUCSJU9fih0mgIiIqCwMBuDKFbFQQighoWL+bgQHywgPlxARYf0dESGhZUsf+Ppmo3ZtCf4VuzRRtcQkEREREREREVUoMS4WAc+McpggkjUaaIcOg3bMszC071jFvxYqQxDSikn22O8ThDyPRixJNWA0Nim0TpDR2BCC4IWwMH9kZ2dW2LozRsmILL3zkTmOnluTQOmmY/MyoJN0FROwGxVMAEX4RSLCN4IJICIiKpW0NFiSQObfFy+KyMsrn//XCguzTvXmKBFUu7YMX9/CxwkCEBYGJCVJXOeugjBJRERERERERBUnNxcBzzwJRVys3WZD67bIfXYc8h4bBtk/wEPBFccIQUh1mOixPk+2eywIFTu1mCtkWWMzEqixTTKoMWQ52E3nkJFjyLFMvZaRPzrHOmon3SbJY04CFR7Vk63Pcks8lQ0TQEREVF6MRuD6daFQQig62j1/UwRBRo0acoGkjynxY95Wu7YM7+q39GCVxSQRERERERERVQxZhv8bk6A6ecJus+7e+5C+Yi2gUlVwQPr8RI7z6dxEMQlACkJCEiAIqRCEyjXdmJksKyFJoZDlMEhSWIHHph9ZDoPR2BCSFI6ipofTGXUO19fJ1KdDvqpHTEqCTSLHPrGTZZMAMsrGimsAD9AoNQj0DoSv0g8B6gD4qQMQoA6Av9o//7fpx7zNXx2AYO9gJoCIiMhtsrJsRweJOHdOgfPnReTklH10kI+PjBYtJLRubUTr1hJatpRQt66EWrVkqNVuCJ4qDSaJiIiIiIiIqEL4fP8/eK9eabfNWL8BMhYsclOCSOtklI+jRFAyRDHN5ZrFCu7Pl2Uvm+ROqF2ixz7xE5r/OwiSLCNLl+lkfZ1UZOpuIlO3y7K/4Cge83Ot0fk6UXcCpagslMQxJXnskzvWZI8/AtSBNvtNCR+VQomwMH8kJVXc9HtERFQ9yTJw86ZglxA6e1aBGzfc8z8o4eESWre2JoRat5bQqJEEhcIt1VMlxyQREREREVE11bhxEwQHByEkJMzToVA14LV6JXw//dhum+Tnj/TFyyGHhJa4PkFIhrf3L/Dy2gJRjMtP/FTeqclkWWMZ4SNJNZwme8yJIFn2A+D4W8AxWdFYffkP7Lo5Fwk58dZRPPrMir2oCiZAKJSoKThqx/lzf/irA+Gv9oe3whtClV7rioiI7gRGI5CWJiA1VcDFi8DVq0okJwtISTH9pKYCKSkCEhNFXLokIjOz7H+71GoZzZtLaNXKNiFkREiIGy6IqiwmiYiIiIiIqqk1a9bzW/BU7hSXLsJv6hSod2632y4LAjLnzIexRcuS1ae4BB+fH+HtvRSCkOvOUEtEkgLsEjv2o3wKT/UGaMp0voy8dKy/+hf+uLQcB6L3QUbV+tD6KH3gp/JHgFcA/FX+8PcKhL/d8wD4qwIszwO8AuBX4Lmvyo9TtBERUaVkNAKpqdYET8Ef877kZOvjtDRAlm0TPz5ujalGDckyKsicEGrSRKr42X2p0mOSiMhFw4Y9jLi4WKhUKixY8BsaNWpc7DGTJr2Af/45gbvv7oBZs+aVS11ERERERJWRkJoCzZcz4PPzfAjGwmvTZL//EXT9B7pYmwyVajd8fH6Al9dW9waaT5KCHUznZkr2+PnVRXq6L4xG8/5QAF7lEoetPGMedtzYhlWXV2Dr9U3IM+aV+zkLUggKu5E5Ib5B8BF9TQkfm+nabKdqsx/pY3qsVnDxAiIiqhoMBmvCJzVVsIzuMT92nPDx3AhVpVJG06YFRwdJqFmzan2hhDyHSSKiEtLr9fjss48xd+7PUJRxYk531kVEREREVCkYDPD+dQF8v/gMYmqqwyK5o8Yg95XXXahMC2/vlfDx+RFK5VmXQ5Bl0cEon6LW9QmBs38eCwLg5+cPvb5iRtxJsoQjsYfwx6UVWBe1Bml5aaWuy1fl52BqtsBip2qzLeOj9LFMzSYI4OhDIiKqUvR6xyN8CiZ/bPelp1feKUmDg2W7aeJat5bQrJkEr/L/7grdwZgkIiqFCxfOYcmSRRgzZmylqouIiIiIyJNUu3bAb+oUKC9ecLjfWKs2st//EHkjR5kyDk4IQiJ8fObDx2c+RDHRaTlZViIv7zHk5T0MWa5hM/onGEDVmpbsfPI5rLq0Aqsvr8TtrFvFlvdXB2BwoyG4L7IXgryC4Jc/gsecFPJT+UMh8otoRER059Dr4XQqN2cJn4yMypvwseXvDwQHSwgJkREcLCMkREZoqOlxcLCMunVNo4PCw+Wi/heKqFSYJCIqpZ9//gk9evRCw4aNKlVdRERERK568cVxyMxMh79/IGbPXuDpcKgKU0Rdhu+H78Nr62aH+2UvL+S89ApyXn0T8PNzXo/iHHx8foC39woIgvOp1SQpGFrtc8jNfR6SFFHm+D0lJisaqy//gVWXVuBs8uliy6tEFfrW74/hzUaiX/0B8FG6d+0CIiKiiqLTAbGxwOXLomXaNmdTuZl/MjOrRnbE39+U2AkNlS1JH3PCxzb5ExJi/alTxx9JSdkcqUsewSQRUQk1aNAQN25ch06nw2effYw5cxaWeqo4d9ZFREREVFIHDx5AbGwMwsOrbic7eZaQkQ7NV/+Fz4K5EPR6h2W0Q4Yie+o0SPXqO61HqTwGX9/pUKt3FXk+g6EpcnMnQqt9EoCmLKF71N7bu/HN8a9wIHofZBTfG9Q1vDsebzYCQxo/imDvkAqIkIiIqPSMRuDGDQEXLihw8aKIqCgRSUn2CZ+sLHPCx9ejsRYnIKBwwsc2uVPwJzhYhrqEy/BxZBB5GpNERCXUsmVrdOrUBX/8sQznz5/F0qWL8dRTz3q8LiIiIiKiCmM0wnvpb/D97GOISUkOi+jb3oXsTz6Hvtu9RVbl7f0z/PzegiAYnJbR6e5Hbu5E6HT9UdWmkSto+YUleGXni8WWax7cAsOajcRjzYajrn+9CoiMiIioZCQJuHVLwMWLIi5cUODCBREXL4q4fFmEVlv5Mh+BgYUTOs5G9pj3q1Sejpqo/DFJRFQKL744CYcO7Ud09G0sXDgPPXr0QoMGDT1eFxERERFReVMeOQy/99+F6t9/HO6XatRE9vsfQjtyFFDkKHkDfH2nQKOZ63CvLKuh1Y5Abu5EGI1tyh54JbD9xha8vutlp/tr+4bjsabD8XizEWgT2hYCv1pMRESVgCwD0dHmZJCIixdNCaFLl0Tk5Hjmb1VQkOtTuZnLKdkTTuQQPxrVhdEIITXV01G4RFAKEAwlm4BTDg4u5h+g7uXt7Y0pU6bilVcmWKaKmz17QamminNnXURERERE5UWMiYbvtKnwXr3S4X5ZrUbuhJeR8/pbkP0DiqxLEFIREPCsw+nlJCkMubnjkJv7PGS5pltirwyOxR3F+C3PwCgb7bb7qwMwuNEQDGs2Et0jekAh8t8BRETkGbIMxMcLOH/eNCLIPELo4kXRZno49xIEGUFBsEv4FDetW1AQEz5E7sSPUzWg/msN/Ce/DTEp0dOhlBsprAYyP/8KuiFDK+ycd9/dAY8/PgJ//LEc586dwbJlv2H06Gc8XhcRERERkVtptdDM/h6ab2dCyMlxWCRv4GBkffQJpIaNiq1OobiEgICRUCqjCu3LyXkD2dlTAHiXNepK5VLKRYzeMBw5Bvv2e6HdS3i/60fwUfp4KDIiIqqOZBlITBQsiSBrUkiB9HT3JYPCwiQ0b276qVPHPtETGiqjaVNfGI1ZEKv2TLJEVR6TRNWA/5uvQsxI93QY5UpMSoT/m68iuQKTRADw4ouv4ODB/YiJicaCBaap4urXb+DxuoiIiIiIykyWod6wDr5T34fi5nWHRQzNWyDrk/9C36u3S1WqVNsREDAWomj/7xNZ9kJm5izk5Y0sa9SVTkxWNEauH4rUPPuZHYY1G4lp986AKLBnjIiIyk9ysu00cdbfKSnu+/sTFCSjRQsjmjeX0KKFNTFUo4bzmYIEAQgLA5KSTEkrIvIcJomIysA8Vdyrr74InS4PM2ZMw48/zodYiq9AOKurqi/MS0RERERVj+LSRWDqZATs2OFwvxQQiJx3pyB37PNwbUVnGT4+s+Hr+x4EQbLbYzTWQkbGEhgM97gh8solTZuKJ9Y/huis23bb+9Trh297/8gEERERuZUsA/v2KbB5s9KSEEpMdN/fGn9/OT8RZLQkg1q0kFCzpgwuo0dUdTFJVA1kfv1dtZluzhPat++Ixx4bjlWrVuDMmX+xbNnvGDVqjNvqevppTjtHRERERBVHdegAAp8cBuRkF9onCwK0Y8Yie/L/QQ4Lc7FGHfz83oSPz6JCe/T6u5GRsRSSVKeMUVc+2fpsjN44AhdSzttt71CzI+YPWASVwpXkGhERkWv0euDtt72xdGnZ/75oNLLNiCBTQqhFCwnh4UwGEd2JmCSqBnRDhiL5oSEQUlOLL1wJKJUCDIaSjTOVg4MBhecWeH3xxVdw6NABxMREY/78OejR4z7Uq9fALXX16tULderUc2/AREREREQOqA4fROCTwyA4SBDpu3RD1mdfwND2LpfrE4REBAY+BZXqUKF9Wu1QZGbOBqApS8iVktagxdObnsTfcUfstjcJaorfH/oDfio/D0VGRER3oqwsYPx4H+zcWbKuXh8fGU2bSjajgkxTxkVGylwniKgaYZKoulAoSvBNP8+SlSJkg1R8wUrEx8cHkyd/gNdeewk6XR4++8w8VVzZ6/rkk4/www+lq4uIiIioKGPGPAO9XguVytvToVAloDp8EIFPPF4oQWSMqIPsD6cj79HHUZKvDysUZxAY+AQUipuF9mVnv4ecnP8AuPO+jqw36jF+y9PYd3u33fZamtpYNng1Qn1CPRMYERHdkRISBIwe7YNTp5x/edrLS0aTJtbp4czJoHr1ZE9+55qIKgkmiYjcpEOHTnj00WFYs2Ylzpz5F8uXL3FLXadPl60uIiIiImfeeWcKwsL8kZSUyQWDqznl4UMOE0R5gwYj44efAF/fEtWnVq9DQMALEAT7+mTZBxkZc6DTDS1zzJWRUTJi4vbnsfXGZrvtId4hWDnkT9QLqO+hyIiI6E4UFSVg5EgNbt60H/bj7S1j4kQd2rQxJYQaNJChZC8wETnB2wORG7300is4fPggYmOjMX/+bISElP5bgu6si4iIiIjIGeWRwwh8snCCCI88gswfFwAqdQlqk6HRfAlf308K7TEaI5GRsRQGg+vT1VUlkizhjd2T8GfUarvt/uoArHh4LVqEtPRQZEREdCc6dkzEU0/5ICXFPkEUEiJh8eJc3HNP1Zqlh4g8h7NLErmRRqPBlCkfQBAE5OXlITY2plLURURERETkiPLoEQQ+8RjE7Cy77XkDHwJWrADUrieIBCEN/v7POUwQ6fWdkZq6645NEOmNery//10su/C73XaNUoOlD61Cuxp3eyYwIiK6Y0gSkJgo4PRpEb//rsLjj2sKJYjq1ZOwfn0OE0REVCIcSUTkZh06dMIjjzyOtWv/cEtdQ4cOw+rVK90QGRERERGRleLsGccJogEDkTn/V3ip1QDyiqhBhkJxBWr1ZqjVm6FSHYQgGAuV0mpHITPzWwBebo3f0xJyErDz5jZsu7EFu2/tRKYuw26/l8ILiwYtQ+fwLh6KkIiIqgJZBlJTgbg4EXFxAhISBMvjuDgB8fFi/m8BBoPztfzatTPi999zUasW5xAmopJhkoioHEyc+CqOHDnoltE/kya9hkOHDnAkEREREbldu3YtEBsbg/DwCJw6dcHT4VAFEpKTEfjMkxCzMu225/V/EBnzF0FwOoJIB5XqANTqLVCrN0OpvOr0HLIsIjt7OnJzJwFw3qlVVUiyhFMJ/2DbjS3YfmML/kk4CRmOO+KUohILBixCz8j7KzZIIiKqNGQZyMiwJn/i4wVkZQFRUV75CSAR8fGmRJBOV7a/k717G7BgQS78/NwUPBFVK0wSEZUDjUaDyZM/wOuvT4RcxlWg3VkXERERERH0egSMfxqKmzfsNuc9MAAZCxYDXvYjfgQhAWr1Nnh5bYZKtROiaJ9YckSSApCZuRA6XX+3hl7RMnUZ2HN7F/Yd2IkNlzYiISe+2GNEQcTsfvPRv8HACoiQiIg8ISsLlkSPOQFkm/QxP87NdZT8Kclaf8UbOVKPr7/WQqVya7VEVI0wSUTkoj/+WFei8h073oN9+/4u97qIiIiIiErC74PJUB/YZ7dN36mzTYJIhkLxL4BdCAz8E0rlCQiCa19WkmUv6HQPIDv7YxiNTd0ffDmTZRlRaVcso4UOxx6EXtK7fHz7mh0wpctU3F+3TzlGSURE5SUnB3ZTvBWc7s2cFMrO9twIWUGQUaOGjLp1ZYwZo8OTTxogVP0Bu0TkQUwSERERERERVRPei3+Bz8Kf7LYZwyOQ8ctPUAfsyl9faAsUCtNUx658K9lorA2d7sH8n14AfMsh8vKTZ8zDwej92H5jC7bd2ILrGddcPtZX5Yf76/bBA/UHoG+9B1DLt3Y5RkpERKWl1aLQaB/bpI/5cUaGZ7MtYWESatWSUbu2jNq17R/Xri2jVi1TgkjJHl0iciPeUoiIiIiIiKoB5eFD8Jv8lnVDJCAPVUL6oC5CwrpAELQu16XXd8xPCg2AwXAXqtqaQ/HZcdh6YzO23diCvbd2I8eQ7fKxjQIb44EGD+KB+gPQNbw71Ar3ThtEREQlI8vA2bMirl+3TQDZJ39SUz37dyo42JToqVtXgZAQvcMEUI0aMpwuCUhEVI6YJCIiIiIiIrrDCUlJCHzuKQh6PdAGwHsARgCCwgAVjhR7vCT5Qa/vg7y8B6HTPQBZrlXuMbtblj4LG6+uw8qLy7D39m7IcG0KPZWoQq8GvdAroi8eqDcAjYKalHOkRETkqlu3BLzwgg+OH1d45PyBgTJq1Sqc8DGN+jE9rllThrc3IAhAWJg/kpK04JLTRFSZMElERERERER0h/P5+SeIDROB+QAecfWoRsjNHYC8vAeh13cH4FV+AZYTo2TE/ui9WHFxKTZcXefyiKFamtroV78/+tUfgPvr3o8GERFISspkpx4RUSVy+rSIJ5/0QUKC6Pa6/fzk/MSP8wRQrVoyNBq3n5qIqMIxSURERERERHQHU6kOwGfoLOCLosvJsgJ6fXfodA9Crx+A4OAOyM7OqpKJkQsp57Hy4jL8cWk5YrNjii0vQECHWh3Rr/4APFB/ANqG3QUhfxVwLgZORFT57NypwLhxPsjOLtlN2sfHfpSPORFkTv6Yk0J+fuUUOBFRJcQkERERERER0R1Hhkq1Hb6+X0GlOgR0c1xKkvyg0z2Uv75QX8hyEABzYqRqZUcScxKx5vJKrLi0DP8m/lNseT+VP/rWewD96vdH3/r9EeYTVv5BEhFRmf3+uwpvv+0Fo9H+71RoqITGja0jfxwlgPz9mfwnIiqISSIiIiIiIqI7hgS1ej00mplQqU46LyUFIzf3JeTmToAsB1dgfO6Va8jFlmsbsfLSMuy8uR1G2VhkeVEQ0btuX4xo/iQGNBgEjYrzBBERVRWyDPz3v2p8/XXh6U/vvdeAn3/ORVBQxcdFRFTVMUlERERERFRNzZ79E7y9FdBqi+5Yp6rAAC+vVdBoZkKpvOC8WBxg2NAKaY9sgyz7V1x4biTJEo7GHsaKi0vxZ9QaZOoyij2mTVg7jGj+BIY2HY5amloVECUREbmTTge8+aY3VqxQFdr32GN6fPutFl5Vb+k8IqJKgUkiIiIiIqJq6t5770NYmD+SkjKr5LozZCKKNxEYOBJK5VnnhW7AtCbRQiD3s5eqRIIoz5iHq2lRuJx6EZfTLpl+p15GVNpl5Bhyij2+lqY2hjUbieHNn0Cr0NYVEDEREZUHWQaee84HW7cW7sZ89dU8vPeeDqLogcCIiO4QTBIRERERERFVUYKQicDAEVAqzzkucAnADAC/A9CbNun6PlBB0bkmVZuCy6mXcSXtEi6lXsSVVNPvm5k3IMlSierSKDUY1OhhDG/2BHpG3g+FqCinqImIqKKsX68slCASRRmff56HZ5/VeygqIqI7R7VMEm3btg0rV67E6dOnkZmZiZCQENxzzz14+umncddddzk9Tq/XY+nSpfjrr78QFRUFWZZRp04d9OvXD2PHjkUQJz4lIiIiIqIKI8Hff4LjBNG/AD4DsBKATZ5Ff3d7SOERFRSflSRLiM66jcup1hFBl9Mu4nLqJSTlJpapbgECekT2wvBmIzG40RD4qSv/KCkiInLdhg323ZcajYx583LRvz+nyyUicodqlSTSarV49913sWXLFrvt8fHxWL9+PdavX4/XXnsNEydOLHRsXl4exo8fj6NHj9ptv3LlCq5cuYLVq1djwYIFaNasWbleAxERERGRuxw4sM+yJlH37vd5OhwqIY1mBry81ttvvAzgLQDrARSYQlCqURNZn31ZrjFpDVpcTY/CldRLpZoiriSaBTfHiOZP4vGmI1DHP9KtdRMRUeVgNAK7d9uPCp06NY8JIiIiN6pWSaKpU6daEkTt27fHa6+9hhYtWiA5ORnLli3D4sWL8e2330KWZbz88st2x06ZMgVHjx6FSqXCpEmTMHjwYKjVauzZswdffvklEhIS8OKLL2L9+vXQaDSeuDwiIiIiohJ56aXnERsbg/DwCJw6dcHT4ZCrZBk+0TPh2/6/9tuTATwI4Kr9Zn3HTsh9ZhzyHnkM8PFxSwjunCKuOOG+EWga3BxNg5uiSVAzNAtujqbBzVDbN9yt5yEiosrn5EkRKSn2Cw7172/wUDRERHemapMkOnnyJP78808AQI8ePTBnzhyoVCoAQHBwMP7v//4PtWrVwldffYU5c+bgoYceQoMGDQAAp0+fxoYNGwAA77//Pp588klLvcOHD0fr1q0xYsQIREdHY9GiRXjxxRcr9uKIiIiIiOjOl5UF79Ur4X1oFlS/XrbfZwAwApYEkazRQPv4CGifHQdDW+dTahdFkiXcyriJS+UwRVxBSlGJRoGNLUmgJsFN0TSoGZoGN+P0cURE1dj27fZdly1aGBEZKTspTUREpVFtkkTmBJFKpcInn3xiSRDZGjduHJYtW4bbt2/jl19+wUcffQQA+PnnnwEAkZGRGDFiRKHjWrVqhUcffRQrV67EypUrmSQiIiIiIiK3EaNvw2fOD/D+fRFEdSZwHIBvgUJvANgJGJq3QO6z45A3/AnIAYElPpcsyzgUcwDzz8zFrpvbka3PdsclWPirA9A0qGn+yKBmpt9BzVA/oAFUisL/RiMiourHYADOnhVx5IgCK1bY/23o04fTzBERuVu1SRKdPXsWANC2bVuEhzuelkAURXTv3h0rVqzA3r17AZj+kbRv3z4AQO/evaFQKBwe27dvX6xcuRK3b9/GhQsX0KJFi3K4CiIiIiIiqi4UFy9AM+sbeK1aAcGQP7XOBwAa2JeTFwrIi3kM2r+eh75LN0AQSnyuXEMu1lz+Az/9Owdnk0+XOfYI3zpoEtwMTYObWhJBzYKbo6amFoRSxEdERHeunBzgxAkFDh9W4MgRBY4dUyA72/Hfin79ONUcEZG7VZskUXp6OgAgIiKiyHIhISEAgOjoaGRlZSE1NRUZGRkAgNatWzs9rlWrVpbHZ86cYZKIiIiIiIhKRXn4EDQ/fAOvLZsK7+xh/9R4oy5Su22B/HBkqc4VnXkbv5xdgMXnfkaKNqVkceZPEWdOAjUJbmqaKi6oKaeIIyIip5KTBRw9akoKHT2qwKlTIgyG4r9AEBAgo3NnjiQiInK3apMk8vU1zceQnV30dAnmZBIAxMfHIzHROtd2ZKTzf3jVqFEDKpUKer0et2/fLmO0RERERERUrUgS1Fs3Q/P9/6D6+4jTYnIwYNuNlhPyH8iakiWIZFnGkbjDmP/vHGy4+heMctEdbv7qADQLboYmQc2s08RxijgiInKBLAPXrgEbNypx5IhppNClS45n6SmKSiVj2jQt1OpyCJKIqJqrNkmiZs2a4dy5czh58iS0Wi28vb0dljt69KjlsXkkkVlAQIDT+kVRhK+vL9LS0iwjj0pKEEo2MwRnaSCqfEr6OS7tOWx/V3dsDyu2hT22hxXbwopt4Vx1bxOPvTd0OnitWgmfH76F8uIFp8Vkb29oR42Buu0mKGD9Uposh7gcs9agxdorq/DTv3Pxb+I/RZZtGNgIr3d7DX1qD0AtTXi1niKO9w0rtoU9tocV28JedW4PoxE4f160TB13+LACcXEA4FPiuho3ltCliwFduxrRq5cRERGy2+OtSNX5feEI28OKbWGP7WFVUW1QbZJEAwcOxNq1a5GWloaZM2fi/fffL1RmxYoViIqKsjzX6/XIy8uzPHeWWDLz8vICALtjSiIkpGRTMmi1WqSkiFAoBCiVYqnOWVlVxut59NGHEBcXW2QZhUIJX18NatWqjTZt2uGJJ0ahfv0Ghcq99NLzOHnyeInOP2jQw5g69eMSHXMnqozvDQCQJAGiKCI42LfYe4W7hIZyGhdbbA8rtoU9tocV28KKbWEiioLld1gY2wSowPdGZiYwbx7wv/8B0dHOywUHA5MmQXjlFfjUqAEg2G53QEAdAEXHHJMZg9l/z8bc43ORmJNYZNkBjQfg1S6v4sEmD0IUKuf/d3kK7xtWbAt7bA8rtoW96tAeWi1w9Ciwfz+wbx9w8CBQmu9OKxRA+/bAffcBPXqYfmrWFAHceUOHqsP7oiTYHlZsC3tsj4pTbZJE999/P7p164ZDhw5h0aJFSEhIwLhx41C/fn0kJSVhzZo1WLBgAWrVqoX4+HgAgEqlgkJR8iGwpZWSkglJcr28Xq+DJEkwGmUYDCU4sJJTKsUqez1GowEZGRnIyMjA5cuXsG7dWrz77vsYNOhhu3KyXPJvv8jynfU6l0Zlfm8YjTIkSUJqajZUKn25nksQTH8ok5MzUYq30h2H7WHFtrDH9rBiW1ixLexJkmz5nZSU6eFoPKsi3xvevyyA5pOPIaanOS1jrBOJ3JcmQTv6acDPz7QxKQ1hYfbHpKZ6wWgs/NoZJAP23NqFpRd+x4arf8EgOV/oW6P0xRMtR2F82wloGtzMVG9KNj8r+XjfsGJb2GN7WLEt7N2J7WE0ArduCbh0ScTly6afixcV+PdfETpdyb/qrtHI6NjRiK5djejSxYiOHY2WP3dmSUluCr6SuBPfF2XB9rBiW9hje1iJYskHlpRGtUkSAcD//vc/vPTSSzh58iQ2b96MzZs32+2/++678dxzz+HVV18FAGg0Gvj4WIfDFjdCyLy/tKMIZBkleuNX9w+Jp7Rrdze++uo7h/v0eh1iY2Oxc+dWLF++BAaDAV988SmaNm2Gpk2bFypfq1ZtLF68osjzmRMjSmW1+rhWWSX9HFeVc1UFbA8rtoU9tocV28KKbVEY28OkvN8bXiuWwu+dN5zuN7RshZyXX0Pe0GGAKn+9n/x4BCGtUHlJCraL93zyOSy/uASrLq1AfE5ckbHUD2iAcW1fwJMtnkKgV5DpVAWunZ8VK7aFFdvCHtvDim1hryq2R24uEBUl4soV0S4hdPWqCK229PMehYZK6NzZmhRq21ay/Jkzq2ptVVpV8X1RntgeVmwLe2yPirv+atXrHBwcjMWLF2PFihVYu3Ytrly5AlEU0aRJEzz66KMYMWIEtmzZYilfo0YNuzWJMjOdf7tSkiRkZ2dbzkN3LlEUodFonOzVIDAwCC1atETDho3x6acfwWAwYNmy3/DBB9MLlRYEoYi6TCrz6BkiIiKq2v799wLCwvyRlMRv6VUEITMDfh9/4HCfrmt35L7yOnT9BjidfFwUUwttk6QgJOYkYs3llVh+cSlOJ50qNo6ekb3xfLsX0a9efyjEips5gYiIKo/UVODSJYUlCXT5sikpdOuWAFku+yIY9etL6NLFlBQaONAboaHZboiaiIjKQ7VKEgGmKeRGjx6N0aNHO9x/8eJFAEBISAiCgoLQoEEDy76YmBh07NjR4XGJiYnQ601TTIWHh7s3aKqSBg4cjJ9+mo2EhHj8889JT4dDRERERB6m+d9XEBMT7LblPfgQcl55HYZ7uhR7fMGRRAZJhTEbn8WOm9uKnE4OADRKDYY3fxLj2r6AFiEtSxw7ERFVPZIEREcLdkmgK1dMj5OS3LfunCDIaN1asowS6tLFiNq15fx9QFiYN5KSOCKAiKiyqlZJIqPRiMzMTAQFBTkts3//fgDAXXfdBQCoWbMmgoKCkJaWhnPnzuHhhx92eNzZs2ctj1u1auW+oKlKCwurgYSEeKSkJHs6FCIiIiLyIMXVK/CZ+4Pdtrz+DyJj0VKX6xAE+5FEidl6bLm+qchj2tfsgBHNn8RjTYcj2DvE9YCJiKjKyMsDrl2zHxF0+bKIqCgROTllHxVkKzBQRtOmEpo1M6JpUwktW0ro1MmIgAC3noaIiCpQtUkSrVixAh9++CGUSiUOHToEv4Kr4QG4cOECzpw5AwDo16+fZXuvXr3w559/Yvfu3Xj33XchOJj+YefOnQBMU9S1aNGinK6CqhK9Xo/bt28BMCWLiIiIiKj68v3wfQj5Mw8AgKxSIXvaZy4dezvzFlZeXIZszMO3A63bU7WOy4f7RmB4sycwvPkTaB7Cf5sQEd0pMjJQaFTQpUsK3LghwGh0bzKoTh0JTZtaf5o1k9CkiYQaNWRns6ISEVEVVW2SRO3bt4ckSdDpdFi5ciXGjh1rt1+r1WLq1KkATKOHbEcMDR06FH/++SeuXr2KJUuWFJqq7ty5c1i7di0A4JlnnnGYRPI0o2REal7hOcwrI6VCgMFYsjHIwV7BlW4+9RUrliAjIx0A0KNHLw9HQ0RERFTYl1/OgF6vhUrljbffnuLpcO5Yqp3b4bXFfsRP7gsTYWzUxOkxkixhw9V1+OXMfOyP3gsZMl7sZF8mNdf6WKPUYFCjhzGy+Sj0qNOz0v2/MRERlVxWFrBwoRq7d5vWDoqPd98UcQCgUslo1MiU/DEngZo1k9C4sQQH360mIqI7VLVJEjVt2hS9evXCnj178PXXX0Ov12PAgAHw9fXFv//+i++++w7nz5+HKIqYNm0avLy8LMd269YNffr0wc6dO/Hpp58iISEBw4YNg7e3N/bs2YMvv/wSer0ekZGRePLJJz14lY79dWUNJu97G0m5iZ4OpdyE+dTA5/d9hSFNhpb7uSRJQk5OTqHtsiwhKysLN29ex7ZtW7Bp03pTbGE18PTTYwuVNx0jO6zLllIpQqlUQxTd+z+DRERERIsX/4rY2BiEh0cwSVSOfD/5yO65FFYDOW++47CsKTn0F776+3OcTzln2V7bDxjZ2r5sqha4N+I+jGwxCoMbDYGf2t/doRMRkYdERwsYNcoH58+XPenv5yejWTP7kUFNmxpRv74MlcoNwRIRUZVWbZJEADBjxgw8/fTTuHLlCmbOnImZM2fa7ffy8sK0adPQu3fvQsd+/vnnGDduHE6fPo05c+Zgzpw5dvvDwsKwcOFCh9PYedqbu19Fhi7d02GUq6TcRLy5+9UKSRL9++8/6N+/p0tlmzVrjo8/noHgYMfzv8fHx7lU188//46mTZuXKE4iIiIi8jwhPh6qM//abcv+v48g+9sv3uAsOQQAI1oDPw4CQjX2dXet/SjWPLqoXOImIiLPOX1axOjRPoiLK9mXRWvXLpgIMo0MqlWLU8QREZFz1SpJFBoaij/++AOLFi3Cpk2bcO3aNRiNRtSpUwc9evTAs88+i7p16zo8NjAwEEuXLsXSpUuxbt06REVFQafToU6dOujduzeef/55hIaGVvAVUWVUu3Y4una9Fz169ESXLt0q5fSDRERERFQxFNeu2j2XvbygHTnK8ryo5FCID/DDIOCJNoXrlWUBSulZ6AvvIiKiKmz7dgXGj/dBTo7jvgSFQkaDBjKaNjUWSggFBDg8hIiIqEjVKkkEAD4+PpgwYQImTJhQ4mNVKhWefvppPP300+UQWfn5+v7vqs10cxXh7rs7YNaseZbnOp0OV69GYf782Th8+CCSkhIRGBiIzp27Fpsgql07HH/8sa7IMkqlCINBckvsRERERFSxFNftk0TG+g0AhaLI5BAAPNQU+OlhINzBDHKSVBOZmd9Dr+9TTlETEZEnLFyownvveUGS7PsSmjQxYsoUHZo1k9CwoQS12kMBEhHRHanaJYmqoyFNhuKhRkOQmpfq6VBcolQIMBjlEh0T7BXsscV51Wo1WrRoiS+++AbTp0/Ftm2b8euvC5Censa5/YmIiIiquYJJIn3DhlgXtdZpcshfDfzvQWBce8f1abWPIStrJmSZsxgQEd0pJAn4+GMvzJ5dOPvTvbsBP/+ci+BgDwRGRETVApNE1YRCVCDMJ8zTYbikqo6cEUURkyd/gCtXLuHatatYu3YV6tdviOHDn/B0aERERETkIebp5mQAa1oCH3Q6inNbNjss26s+sOQxFSICCk8iJ0nByMr6Gnl5j5dnuEREVIGSkwWcOSNi4UIVNm1SFdo/bJge//ufFl5eHgiOiIiqDSaJiNzIy8sLU6d+gueffxoGgwGzZ3+Hjh07oVGjJp4OjYiIiIg8QHH9GpJ9gAkPA6taAUCKw3LPtK2PhY/ehigWThDl5fVHVtYsSFLt8g2WiIjKhdEIXL8u4MwZBc6eFS2/Y2NFp8e89VYe3n1XBy5zTERE5Y1JIiI3a9q0GUaPfga//roAOp0OX3zxGX78cT5E0fn//BERERHRnUe8fQu7M//Bsy8BMU4WE28Z0grvdX0FT3b4BKJotNsnSX7Izv4cWu0YAOwlJCKqCrKzgQsXRFy/Dhw+7IUzZxQ4f15ETo5r93GlUsbXX2vxxBOG8g2UiIgoH5NEROXgmWfGYceObbh9+ybOnPkXa9euwmOPDfd0WERERERUQbQGLb5YMgqznnI8jXLLkFZ4+57JeKjREAQGTIBCEW23X6e7D5mZP0KS6ldEuEREVEKyDMTFmaaLO3tWYfl99aoAWTYnhAqvMVSUgAAZP/+ci/vuMxZfmIiIyE2YJCIqB2q1Gm+/PRmvvz4RADB37iz07Hk/wsJqeDgyIiIiIqvu3e9FZmY6/P0DPR3KHeVc8llM3DIW5/wuFNoX7BWMz+77EkObDoMoiFCr18Lbe7ldGZ3ufqSnrwXAkehERJWBXg9cviwWSAiJSEkp+33a21tGy5YSOnUyYsIEHerVk90QMRERkeuYJCIqJ506dcaAAQOxZcsmZGdn43//+wKffvqlp8MiIiIispgzZwHCwvyRlJQJmX1SZSbJEn76dzY+OfwR8ox5hfb3rtEd3w76GbV9wwEAohgHf//X7euQApGZ+SOYICIi8oy0NODsWfu1gy5eFKHTlX3azxo1JLRpI6FNGyNatzY9btRIgpK9c0RE5EH8M0RUjiZNegMHDx5AZmYG9uzZhf3796BHj16eDouIiIiI3CwuOxav7HgRe27vKrTPywB8Gn8XnnppI0TBnPzRw8/vFYhiil3ZrKwvIUmRFRAxERHJMrB/vwIHD5qSQWfPKnDrVtmT9KIoo3lzAS1b6tG6tYTWrU1JoVq1+I0MIiKqfJgkInLRH3+sK/ExwcEh2LRpZ6Hts2bNc0dIRERERFQJpGlT8fCaAbiRcb3QvnZxwO+rgTpLvoekuAW1egfU6h1QqfZAFDPsyublPYq8vJEVFDUREb3/vhfmzy/ZukEF+fnJliRQmzamhFDLlhLq1vVHUpKWI3WJiKjSY5KIiIiIiIioDOafnuswQTT5ODBNBsQfIiH3Ggel8rLTOiSpJjIz/weg7NMZERFR8aKiBCxYoCrRMZGRpqniWrWyJoTq15chFhh8JPBWTkREVQiTRERERERE1dTQoYORkpKEkJAwrF693tPhVElGyYjfz/5ied66BjCyLjAxGAj9DwBvALhdbD2ZmbMgy6HlFSYRERUwb54asuw4m6NSyWje3JoIatNGQqtWRgQHV3CQREREFYBJIiIiIiKiaioq6gpiY2MQHh7h6VCqJlnG4XXvo3uDGDzYBOjfGIgMKFkVBkNb5OS8Bp3uwfKJkYiICklNBZYvtx9F1Lu3AY89pkebNhKaNpWgLtssdERERFUGk0REREREREQuM0KpPAYv/Z/wTvkNDz+XhkdLsMa5JIVAp+sDna4v9Pq+kKTa5RcqERE5tHixGjk51lFECoWMr7/Wok4dLiBERETVD5NERERERERERRCEJHh5bYRKtQNq9U6IYrppR83ij5VlBQyGztDp+kKn6wuD4W4AivIMl4iIiqDXo9BaREOGGJggIiKiaotJIiIiIiIiIifU6s3w938Bopjm8jFGY13odP3yRwv1hCwHlVt8RERUPL0e+PtvBXbuVGDbNiViY+2HgE6YoPNQZERERJ7HJBEREREREZEDCsUZBAQ8C0HIKbJcjh7Ycx1IyroHD9WfDaOxKQDHi6ETEVHFuHVLwM6dSuzcqcC+fUpkZTm+L3fubECHDlIFR0dERFR5MElERERERERUgCAkIzBwlNME0ZkEYEsUsPkKsO8GkGcEtg+fCaOxWQVHSkREAKDVAocOKSyJocuXXZvac+JEfTlHRkREVLkxSURERERERGTHAKXPCCgU1+227roG/HYa2HIFiM60bvdXB+Djez9Euxp3V2iURETVmSwDV6+aRwspcfCgArm5ro/i9PeX8dJLOgwcaCjHKImIiCo/JomIiIiIiIgA5OhzsPzCEgT5f4zRd8Xa7Tt8G3jwd0BntG67N0aFkU99hyGNh0Kj0lRwtERE1U9WFrB/v8KSGLp5Uyz+IBvt2hnRp48BffoY0bGjESpVOQVKRERUhTBJRERERERE1dqZpNP47fwvWHVpBR5uno7FPez3x2QCjy03JYhCcoBnTgHjTwDNfSKQ8slozwRNRFQNyDJw/ryIHTuU2LVLgSNHFNDrXR8tFBIi4f77TYmh++83omZNuRyjJSIiqpqYJCIiIiIiqqbefvs/AAyorv8sMEpGvLf/Hfx8Zj4AoH1t4KeH7cvkGUwJosaBPTFd3QqjP5kD7/yZiQxNvSo4YiKiO5skARcvijh4UIGDBxU4dEiBpCTXRwuJoowOHaT80UIG3HWXBIVrSxMRERFVW9XzX4NERERERISnnx6LsDB/JCVlQq6GX67++cxPlgQRAMwZDHgX+BfSH6cfwLe9/otGQU3gM+9HS4IIAGRvnwqKlIjoziRJwLlzIg4dUuDAAQUOH1YgJaVkU8jVqiWhTx/TaKGePQ0IDi6nYImIiO5QTBIREREREVG1k6pNwRd/f2Z5/nhLoHMd+zJZ2ePRP/Jr0xO9Hj4/zbHbL9WrX95hEhHdUYxG4OxZ60ihw4eVSEtzffo4AFCpZHTpYkTv3qbEUKtWEoSSVUFEREQ2mCQiIiIiIqJq56u/P0daXhoAQCkCn/W1328wNEFuzn8tz71WrYDixnW7MrljninnKImIqjaDATh92pwUUuLIEQUyMkqe0alXzzqFXI8eRvj5lUOwRERE1RSTRERERERE1VRcXBy02nSkpeWiVq3ang6nwlxKuYiFZ36yPB/fAWgWal8mO/tDACrTE4MBmm++stuv79AR+t79yjlSIqKqRa8H/vlHxMGDShw6pMCRIwpkZZUuKdStmxHduxvQvbsR9erJHC1ERERUTpgkIiIiIiKqpvr3vx+xsTEID4/AqVMXPB1Ohfno4PswykYAgK8K+LCX/X69viN0uiGW515r/oDyapRdmZy3J4M9lkRU3el0wMmTChw6pMCxY8D+/X7IySn5vbF+fQn33mvITwwZUbduNVwoj4iIyEOYJCIiIiIiompj583t2H5zq+X5G92A2gWmLcrOng4gv5PTaITmf1/a7dff3R66vv3LOVIiosopPl7A1q1KbNmixP79igJJIdcSRI0aSZZRQt27GxERwaQQERGRpzBJRERERERE1UJGXjre3fOG5bmfGninuwDA2jmp0/WHXt/D8ly9fSuUVy7b1ZPzFkcREVH1IcvAuXMitmxRYutWJU6cUJS4jqZNjZZRQt27G1G7NpNCRERElQWTREQumDFjGjZs+AsA8Oyz4zF+/IvFHrNixRJ8993XAIDXX38bw4Y94bDctWtXsWPHVvzzzwncvHkDmZkZUCpVCAoKRpMmTdC9+73o23cANBpfp+caNuxhxMXFOtwnCAL8/PwRHByMdu3uxsCBg3HXXe2LjZ+IiIjoTvOfvW/hZuYNy/PRbYEAL9uOSgHZ2R/ZHaPeutnuub5NO+j6P1iOURIReZ5OBxw6pMCWLaYRQ7duiSU6vkULa1Koa1cjatViUoiIiKiyYpKIyAWvvfY2/vnnBKKjb2Px4p/RtWt3tGnTzmn5s2fP4McfvwMA9OzZ22GCKCkpEd988yV2795ZaJ9er0dubg5iY6Oxb98ezJ37I1588RUMHvxIiWOXZRmZmRnIzMzAzZs3sH79nxg+/Em89tpbJa6LiIiIqKpaeXEZVl1eYbftrW4aADk2Wx6G0djG+lSWod613e6YvGEjOYqIiO5IqanAjh2mpNDOnUpkZrp+r2vVyoi+fRVo3z4XXbsaERbGpBAREVFVwSQRkQs0Gg2mTv0EEyeOg9FoxLRpH+CXX5ZCo9EUKpuRkYEPP5wCg8GA2rXDMWXK1EJloqKu4M03JyE5OQkA0LRpMwwaNAR3390BoaGhAIDY2FgcOrQfq1evRFpaGj7/fDrS09MwevQzTuNs1+5ufPXVd3bbjEYjsrIycfVqFBYtWoizZ09j5cqlaNiwEYYMGVqWZiEiIiKqEq6nX8N/9tp/QaZfQ180Dc0uUPIlu2eKK5ehuH3LbpuuT7/yCJGIyCOuXrWuL3T4sAJGo2uJIX9/GX37GtC/vwG9e5uSQmFh/khKMkBmfoiIiKhKYZKIyEWtW7fBs8+Ox4IFcxETE41vvvkS7733oV0ZWZbx6acfIi4uFkqlEh999Bn8/f3tyqSnp+Gdd15DcnISFAoFJk16A8OGjYRQ4BupISGhaN26DZ58cjTeeus1nDnzL+bN+xFdu96Lxo2bOIxRFEWHiSt/f3+Eh0egQ4dOGD16GBIS4rF48S9MEhEREdEdT2/U46Xt45Clz7Tb/uPAdgAOWZ4bjQ2gUPQHYE0cqXduszvGGB4BY/MW5RkuEVG5MhqB48dFyzRyly65vr5Q3boSBgwwYMAAA7p1M0KtLsdAiYiIqMKUbFJZomru6aefQ9u2pmnmNm5ch927d9jtX7r0Nxw4sA8A8MILE9GmTdtCdcyd+wMSEuIBAK+++haGD3+iUILIVmBgID799Av4+vrCaDTi999/LXX8Pj4+uO++XgCA2NhoZGRklLouIiIioqpg5rHPcTz+mN225+96HE3Cjttt02qfQ8F/Hql32k81p+vTj1PNEVGVZDAA//ufGm3b+mLwYF98/72XSwmiDh2MmDIlD7t3Z+PYsWx89lkeevVigoiIiOhOwiQRUQkoFAp88MF0aDS+AIAvv/wMKSnJAIDz589i3rwfAABdu3bHk0+OKXR8UlISNm5cBwBo3botHn98hEvnDQ0Nw+DBjwIALl48D0mSSn0NSqVpAKEoivDy8ip1PURERESVXYo2Gd+e+NpuW8PARphxfzsIgs6yTZa9oNXa/7+bkJUJ1aH/Z+++w6Oqtj6O/8609NACGIp0hFAUG4KFpoKNFxuCIoiKF7tevfYG9t4VGwgoiKCggAUVRKwUQekIKBB6AoH0TDnvH5EMmySQTHry/TyPz529zj7nrFk33pvJmr3PT0aMreYAVEW2Ld19d5iefDJMSUmH/zNQeLitvn19ev75LC1fnqavvsrQ7bfnKCEhQI8cAIBqiiYRUEyNGjXW7bf/T5K0b98+PfvsE8rKytLo0Q/K5/MpLq6+HnhgdIGrg7799iv5fD5J0oABFxfrvoMGXaH335+sDz+cJocjtH91fT6ffvn3jx0nn3wKTSIAAFCtLdmxSH7bnzd2OVwac+Z7igzbYMzLzj5Ptl3PiIVNmSQrKytvbDsc8v67IhsAqpL33nNr4sTCl/7Urx/QFVfkaMKEDK1Zk6aJEzN15ZVeNWzIw4UAAKgJeCZRjeGXZe2t6CSKxLIsWVbxfhm17TqSir6Xckmdc875+uWXnzR37jdasGC+br31em3ZsllOp1MPP/yYateuXeB5ixYtlJS7iqdbt9OKdc/69Ruofv0Gxc7V5/MpIyNda9eu0YQJY7V58ybVrl1Ht956Z7GvBQAAqpdPP52pmJgwpaZmV3QqZSLbn2OMm8U2V5eGJ8iyxhjxQOBoHRJQxDvmnJyzz5Fdu06Z5AkAZWXePKceeCD/lwPbt/fnPV+oS5eAQvweIgAAqAZoEtUAHs90xcTcKYdjd0WnUmYCgfpKTX1OOTkXlts977zzXq1Y8ad27dqplSuXS5KuuupadelyQqHn/P137rdW4+LqF9pIKolly37XaaedeNg5nTodqwcfHK1GjRqX+v0BAEDV0rp1G8XFxSgpKVV2NfzCuC/gNcZuh/vfV2bzyLbNb9h75n4j10ZztVHmddeXen4AUJbWr7c0YkSEAgFzl4u33srUhRf6KigrAABQ2fBdkRogJuaWat0gkiSHY7diYm4p13vGxsZqxIjgHwsiIiI1aNCQw56TkpK7mqtu3XqHnZeZmamMjIxC/zmwZV0o1qxZpfHj31NqamrI1wAAAKgKvIc0iVz/Noks69CVU+a37CPeftMY+xI6ynvq6aWeHwCUlb17pSFDIrV/v9kguvPObBpEAADAwEoiIETZ2VmaNGlC3jgzM0NvvvmK/vvfuws9JxAIFOna/fr1lN/vL/T4ffc9rHPPvSBfvHPn4/Tcc6/ku2d6epq2bNms776bo9mzP9fs2Z9r3bo1euWVtxQTE1OknAAAAKoaX8D8Q6jLyt2e2LIKX0nkXLdWnu/nGsczR4wUT2wHUFV4vdK110Zo40bze8H9+3t15505hZwFAABqKlYS1QCpqa8oEKhf0WmUqdzt5l458sRS9NJLz+nvvzfKsqy8rds+/XSqfvnlp0LPiY2tJUnasye5THJyOByKjIw0/omOjlbDhkfpxBNP1t13P6AbbshdcfXXX+uMJhcAAKh5PvnkY7377rv65JOPKzqVMpHtN1cMuQrZbk76t0mUmqro/91uHAnUrausiy4tmwQBoAw8/bRHCxaY3wnu3NmvV17J4tlDAAAgH1YS1QA5ORcqObm/LGtvRadSJC6XJZ+veJvi23YdSc6ySagA3377tWbOnCFJOu+8/rrkkkG69tor5fP59OSTozV+/GTVqVM333nNmjXXnj3J2rMnWVlZWQoPDy/w+vPn/5b32uVyyOcLaPv2bbr00v4lzv3SSwfr448na9eunZo5c4b+858bS3xNAABQNY0a9ZC2b9+m+PhGuuiigRWdTqmybVtT1n5oxGLDYiVJDkfiIXMjZe3dI115mdy//WYcy7pyuBQRUbbJAkAp+fVXp1591XzOWoMGAU2YkKnIyApKCgAAVGo0iWoMp2w7rqKTKBLbdsi2i7YtW0XYujVRzz77hCQpPr6Rbr75dkVFReuqq67Vu++O0Z49yXryydF65pmX8p17wgknaenSJfL5fPr55x/Vu/eZ5Zy95HQ6dcwx7bVr106lpOxVamoqW84BAIBq59O/pmrJzsVG7Kxm/eRwJMrl2mjEfbuOVq0Lz5dWrjDigbp1lXHQMygBoDJLS5Nuuilcth3cHtPttjV+fKYaNSreFzEBAEDNwUJjoBi8Xq8efvg+paeny+l06qGHHlVUVLQk6corh6tTp86SpJ9//rHAbVvOPvscOZ25K54+++zT8kv8EE5n8F99i/31AQBANZPhzdBjvz5ixFrUaqkrE66S2/2DEQ/4YhXT779yHdogqldP+6Z+JrtBg7JOFwBKxUMPhWnzZvPPPHffnaMTTqi8X8IEAAAVjyYRUAxjxryqNWtWSZKGDbtGnTodm3fM6XTqgQdGKzIySpL0xhsv6++/zW+pNmrUWH37nitJWrJkoaZN+6jI9/b5fEeeVERr166RJNWtW0/R0dGldl0AAIDK4M0/XtXWNHNLuUe6Py6P0yOPZ4ERt77xyrV+gxHzxzdSymdfyXfQ73oAUJl9/bVTH3xgbjPXtatPN9546DPYAAAATDSJgCL66acFmjJlkiSpU6fOGjbsmnxzGjduoltvvUOSlJ2drVGjHpDX6zXm3HjjrYqPbyxJevXVFzVu3Dvy+/2HvfeSJYv0v//dVgrvQvrii5navn2bJKl377NK5ZoAAACVxY707Xr19xeN2OmNe6hf83Ml2flWEllfZBpjf/PmSvn8K/nbHlPWqQJAiWRnS2vXOjRrlku3324+7zYqytarr2bJWX6P7gUAAFUUzyQCimD37l164olHJElRUVF66KHH8raNO9R55/XXzz8v0Pz587R+/TqNGfOabr759rzjtWrV1gsvvKo777xFW7cm6r333tKcOV+qb99zdeKJJ6thw6Pkcrm0e/cuLV26RN99941WrQpuf3L88Sfq+ONPKvDegUBAGRkZ+eI+n087d+7Q3LnfaPLkiZKk6OgYDR06PNSSAAAAVErPLnpKGb7g70OWLI069QlZliWnc52czi3mCfMOep2QoH1TpivQML58kgWAI7BtadcuSxs2OLR+fe4/GzY49NdfDm3ebCkQKHj78Mcey1bz5jyHCAAAHBlNIuAI/H6/Ro16QPv27ZMk3XHHvYqPb3TYc+66636tWLFcyclJ+vjjSTrllO466aSuecebNj1a7747UW+//YZmzpyuLVs26913x+jdd8cUes02bdrq8suH6qyz+hU6588/l+nss8844nuqW7eeHn/8GdWtW++IcwEAAKqSL/+eaYyHJAxTx7hOkmxFR//PnLxL0srcl95ju8j97RwFFCbxd1UA5SwrS/r772Aj6EAzaP16h/bvL95zZPv29enyy71HnggAACCaRMARjRv3jpYt+12S1LfvOTr77MKbNAfUqlVb9933sO688xbZtq3HH39E48dPVq1atfPmxMTE6I477tawYdfo22+/0uLFC7Vx4walpKRIslWrVm3FxzdSly7Hq2vX7urc+biQ30NYWJhiY2upefMW6tbtVJ17bn+eRQQAAKqdlKy9SspMMmL/6XyjJCk8fKw8nnnmCf8+HtLb6Vjt//Rz1YuLk5JSyyNVADWQbUs7d1pGI+jAP1u2WLLt4jWDCtKgQUDPP58lq+SXAgAANQRNIuAIrr12pK69dmSxz+vatZsWLFh0xHlxcXEaNGiIBg0aUuBxl8shny9w2GtMmzbzsMcBAABqgr/3bTTGTsup5rVayOH4W9HRD5iTEyU9lPsya8gw2bG1yidJANVeRoa0caPD2CLuwMqgtLTS7940aBBQ69YBdegQ0LXX5qhBA5ZDAgCAoqNJBAAAANRQDRo0lMNhKS6uQUWnUio27ttgjJvENJXH6VJMzI2yrHRz8rWScncTVk6vPuWTIIBqw7al7dstLVsmLVniNhpBW7Y4Sv1+4eG2WrTIbQa1bh1Qq1YBtWmT+5+xsaV+OwAAUIPUyCbRL7/8og8++EB//PGHUlJSFBUVpXbt2unCCy9U//795XAU/Aud1+vV5MmT9fnnn2vDhg2ybVuNGzfWmWeeqeHDh6t27drl+0YAAACAEvj22/mKi4tRUlKq7GrwxfNDm0Qta7VSRMRb8nh+NCe+Lenr3Je+Vq0VaN5C7MwEoDB+vzR3rlNLlzrzVgdt2OBQRsaB/+UIL7V7xccHm0AHN4SaNLHldJbabQAAAPLUuCbR008/rbFjxxqxlJQU/frrr/r11181c+ZMvf766woPN3/Jy87O1rXXXquFCxca8fXr12v9+vX69NNP9d5776lt27Zl/h4AAAAA5HfodnPHH3WUoqIeMWL29jBZd2TnjXN6n1keqQGoomxbuuGGcE2f7i61a0ZE2GrZMrgS6OBmEI+OBQAA5a1GNYmmTp2a1yA67rjjdNttt6lNmzbasWOHxo0bp1mzZunHH3/U6NGj9cQTTxjn3nvvvVq4cKHcbrduuukmnX/++fJ4PJo/f76effZZ7dq1SyNHjtSsWbMUGRlZEW8PAAAAqNE2799kjM9qtV+WlWlOGuqV0oJDL00iAIcxdqw75AZR48bBJtDBDaFGjWwVsoEJAABAuatRTaK3335bktS2bVtNmDBBYWFhkqS4uDg9//zzkqRZs2bp008/1a233qqGDRtKkpYvX67Zs2dLku6//34NHjw475qXXnqpOnTooIEDB2rr1q2aMGGCRo4cWZ5vCwAAAICkNG+aMW4YnW2Mc3adLs+3C4yYt2u3Ms8LQNW0YoVDjzwSdtg5UVFSq1b+fM8JatkyoKiockoUAACgBGpMkyglJUWbN2+WJPXv3z+vQXSwwYMHa9asWbJtW3/++afOOussSdK4ceMkSU2aNNHAgQPznZeQkKABAwZo6tSpmjp1Kk0iAAAAVAl33HGrMjJSFRkZo+eee7mi0ymxLJ+5aijak2GM7USPMfY3byE7OqbM8wJQ9aSnS9ddF67sbPOJZUOH5ighIXdFUNu2AXXsGK3k5Ixq8Vw3AABQM9WYJpHjoLXcPp+vwDlud3AJ+YH5tm1rwYLcbxv26tVLzkKeFNmnTx9NnTpViYmJWrNmjdq1a1daqQMAAABl4ptvvtb27dsUH9+oolMpFVm+LGMc5TFXFln/pBtjX4dOZZ4TgKrpvvvCtX69+fl/5MgcjR4dXKFoWbn/AAAAVGU1Zhfc2NhYNW/eXJI0e/Zs5eTk5JvzySefSMptFnXqlPuBMTExUfv375ckdejQodDrJyQk5L1esWJFaaUNAAAAoIiy/OZKogh3qjF2rEk2xr4OHcs8JwBVz6efujR5svkcomOP9euBB7ILOQMAAKDqqjFNIkm644475HA49Ndff2n48OH69ddflZycrDVr1ujBBx/UlClTJEnXX3+9GjRoIEnaunVr3vlNmjQp9Nr169fPW4mUmJhYhu8CAAAAQEEyD9luLty9zxg7/thujFlJBECS/H5p8WKHnn3Wo3PPjdQNN4Qbx6OibL31VqY8nkIuAAAAUIXVmO3mJOnss8/Wa6+9pmeeeUaLFy/WsGHDjOPx8fG67bbbNGDAgLzY3r17817HxsYWem2Hw6GoqCilpKTkrTwqruIuVWdZO1D5lMeWEweuz/8G5KIeQdTCRD2CqEUQtShcVa/JG0tfNZpEliSP85Am0d/m9nP+Dh3y/UxU9TqUFuoRRC2CqlMttm2zNHeuS/PmOfXDDy6lpBT+pp5/PkutWuV/6FB1qkdJUQsT9QiiFkHUwkQ9gqiFiXoElVcNalSTSJLS0tIUGRlZ4LHk5GT9/vvvOuOMM1S3bl1JUnZ2cDl5eHh4gecdEBYWlu+c4qhbt3gPzc3KytKePQ45nZZcruq1KKy6vZ+Soh5BlbUWgYAlh8OhOnWijvi/FaWlXj0etH0w6hFELUzUI4haBFGLXA6HlfefcXFVtyYT/pigh3++34i1qBMuh8N8RpG2HvQ6Pl51j++Y75MXPxsm6hFELYKqYi0yM6UFC6Svv879Z+XKop131VXSf/4Tcdg5VbEeZYVamKhHELUIohYm6hFELUzUo/zUqCbRY489pokTJ0qSBg8erCuvvFJNmzbV3r17NWfOHL300kuaMmWKFi9erAkTJiguLk5Op/MIVy09e/akKhAo+nyfz6dAIKCcHJ8cDveRT6giXC6HfL5iFKKaox5BlbkWOTleBQIB7duXqbQ0b5ney7Jy/48yOTlVdv4vNNY41COIWpioRxC1CKIWpkDAzvvPpKTUI8yunOb886Wu/uLqfPFnet4g6YW8sZ0pWQftNpcx6AplJAdXFvGzYaIeQdQiqCrVwraldescmjfPqblzXfrlF6eysor+ddzwcFvDh3t1333ZSkoqeE5VqkdZoxYm6hFELYKohYl6BFELE/UIcjiKv7AkFDWmSfTzzz/nNYjuuOMOXXfddXnHGjZsqCuvvFInnXSSBg8erA0bNuiFF17QE088oYiI4DeGjrRC6MDxUFcR2LaK9YPvcDjlcLiUnZ2p8PDDf7MJQNnKzEyXy+WRZTnL7f/Aivu/GdUd9QiiFibqEUQtgqhFflWxHr9u/0XXfDVMfttvxB/q9qjOah5nxKy/Jf37Hm3LUuaQqwp8z/xsmKhHELUIqqy1SEmRFizI3UJu3jyXtm4t3i4ExxzjV69efvXq5dMpp/h14M8BR3qvlbUeFYFamKhHELUIohYm6hFELUzUo/zef41pEn388ceScp87dM011xQ4p127dho0aJDGjh2rzz77TA8++KDxHKLU1MK/XRkIBJSeni5JqlOnTilmXjjLshQeHqnMzDRFRkbJ7Q4rl/sCMGVkpCo7O0MxMXVlsWEqAADlYlXySl35xWXK8ptbyt1w3C26qcutcjofN0/YGHyZc1ZfBZoeXQ5ZAihLfr+0bJlD8+a5NHeuS7//7lAgUPTfx2vXtnXGGT716uVXz54+NW5cw/8SBQAAaqQa0yT6559/JEnHHnvsYbeQO/nkkzV27Fj5fD5t3rxZzZs3zzu2bds2nXDCCQWet3v3bnm9uVtMxcfHl1reRxIdXUteb7b27Nml8PAohYVFyOl0KPdRvVVTIGDJ7+eX8wOoR1DlqYWtQMCW15ut7Owseb1ZioyMUWRkdEUnBgBAjbAzfYcum3mh9mWnGPFB7a7Qw90elSQ5nf+YJ20Ivsy6quAvjQGoOubPd+ruu8O1cWPRVws5HLaOPz6gXr186tXLpy5dAirHHeYBAAAqpRrTJDrQwMnJySnyOTk5OWrQoIFq166tlJQUrVq1ShdccEGBc1ce9NTLhISEkiVbDA6HQ3XqNFBa2j5lZWUoM7Nq7iV/MIfDoUBxHs5UzVGPoMpWC8tyyOMJU61acYqIiKrodAAAKLaLLrpEWVnpCg+vWv8/9uyip7QzY4cR69v8HL3Q81VZliXH1k3yxH8tHbwL9L8rifxNj1ZOrzPLL1kApSolRXr44XBNnly05/I2anSgKeTXGWf4VLt2maYHAABQ5dSYJlGLFi20fv16/f7778rJyZHH4ylw3uLFiyVJLpdLzZo1kyT16NFDn332mb7//nvdddddBW4nNXfuXElS/fr11a5duzJ6FwVzOByKja2jmJja8vv9su3K80f04rIsqU6dKO3dm17j95yUqMfBKlstLMuS0+liezkAQJX2yCOPKS4uRklJVeehsGneNH3y18dGrGt8N7199vsKX7lSEW+8qrDIabImHPI78R+5/5E57GqxdACoemxbmjXLpXvuCdPu3YWvHgoPt9Wtmz+vMdS2bUD8yg4AAFC4GtMkOvfcc/XNN98oJSVFL774ou6+++58c9avX69JkyZJks4444y85xFdeOGF+uyzz7Rx40ZNmjRJV1xxhXHeqlWrNGPGDEnSsGHDKuyPxpZlyeWq2v+VWpYUHh4ut9tbZf5QUZaoRxC1AAAAkjRz/Qyle9Pyxg7LofcihumoywbKs2B+7iec1Yec9IekH6TsPmcpc8T15ZkugFKwY4elu+4K01dfFbx6qFWrgM48M3cLuW7d/IqIKOcEAQAAqrCq3VEohnPOOUdTpkzRr7/+qrFjxyoxMVFDhw5Vq1atlJGRoXnz5unVV19VRkaGYmJidNddd+Wd261bN/Xu3Vtz587V448/rl27dumSSy5ReHi45s+fr2effVZer1dNmjTR4MGDK/BdAgAAANXbB6vHG+N+iZHq8PDIYOAqSa3Nc7yfn6y0WY/Ld1LXMs8PQOkJBKQPPnBr1Kgwpabm/zJmdLSthx7K1tChXjmK/mgiAAAAHKTGNIksy9Krr76qW2+9VT///LPmzJmjOXPm5JsXFxenV155RS1atDDiTz31lK655hotX75cY8aM0ZgxY/KdN3bsWEVH8+B6AAAAoCys2bNai3b8ZsRGLAiuKlKYpIfMc7xpHZUy8htJ7DcFVCUbN1r673/D9fPPBf/Zom9fn55+OkuNGrHNAAAAQEnUmCaRJMXGxuq9997TnDlzNGPGDK1YsUIpKSkKDw9X8+bN1bt3b11xxRWqVatWvnNr1aqlyZMna/LkyZo5c6Y2bNignJwcNW7cWL169dKIESNUr169CnhXAAAAQGi6dTtBO3fuUMOGR+nnn5dUdDpHNGXaXca4YZp03rqDAndJamqek+57XDSIgKpl5kyXbrwxXFlZ+f/djYsL6IknsvV//+fjWUMAAACloEY1iSTJ4XCoX79+6tevX7HPdbvdGjp0qIYOHVoGmQEAAADlKz09XampqYqOjqnoVI7I2rJZ01LmSwct3L9qmeQOSGoiBSZGydEz3TgnJ+c0eb09yzNNACW0bZtVaINo4ECvRo/OUt26FZAYAABANVXjmkQAAAAAqp6cBXO085CdnS9fIfmeayPnLYlyuNPznZOe/pBYRQRULRMmuPM1iJo2DejZZ7PUu7e/grICAACovmgSAQAAAKj0vMsWSc2D47b1pNYrE+Squ6rA+RkZt8nnO6V8kgNQKnJypIkT3Uasb1+f3nwzUzz+FwAAoGw4KjoBAAAAADgS78rfJUkuh3TPadIfI6XIAhpEfn9D7dv3gdLTR5d3igBKaPZsl3bvNv9Mce+92TSIAAAAyhAriQAAAABUbmlpyt60Tl2Okt7rL3WJL3haZuaVSk9/TLZdp3zzA1Aqxo0zVxGdcopPCQmBCsoGAACgZqBJBAAAAKBSc//xu2r/z9bCEbkriQ7l9zdXaurL8np7lX9yAErFqlUO/fqr+SeK4cO9FZQNAABAzUGTCAAAAEClFqFXVPvO/HHbdigz8walp98vKarc8wJQOtLTpf/9L9yI1a8f0Hnn+SooIwAAgJqDJhEAAACAysu3Suo3J194XZJH9Z1fyuc7qQKSAlBasrKkoUMjtGiR04hfeaVXHk8FJQUAAFCDFLBZAwAAAABUvCxfunb7+ijMXGCgR76XPlvxKA0ioIrzeqURIyK0YIH5/dWGDQO67rqcCsoKAACgZmElEQAAAFBDPffci3K7LXm9dkWnkk9K1l59mXiGbumWbsTfXCSF+Z/RVZ1HVlBmAEqD3y/deGO4vv7a/LNE3boBTZ2aqbp1KygxAACAGoYmEQAAAFBDnX32OYqLi1FSUqrsStQn2pqaqAd+Pl/TBm0y4hv3SLHLb9IlF9AgAqqyQEC6444wzZjhNuIxMbamTMlUu3aBCsoMAACg5mG7OQAAAACVxro9a/V/M87UQ702Kvygr7QFAtLehxvq7PMfr7jkAJTYzp2W/ve/ME2aZD5wKDLS1ocfZurYY2kQAQAAlCdWEgEAAACoFGzb1h3zL9H4i7bp5MbmseR3pDbtRivbsiomOQAh83qlb791afJkl775xiW/3/z32OOxNX58pk45xV9BGQIAANRcNIkAAACAGuqPP5YqMtKtjAyvOnfuUtHpKNU/XZ9dvkkNo8249y+p7jONtGfBJRWTGICQrF3r0OTJbn38sUtJSQVvZOJ02nr33Uz16EGDCAAAoCLQJAIAAABqqCuvHKzt27cpPr6R/vhjTQVm4ldk5LOKi3xShy4UsjMl9xApp3mC5HYXfDqASiM1VZoxw61Jk9xassR52LlOp63XX89Sv340iAAAACoKTSIAAAAAFcaydis29lp5PPPyHdu8T2p0juRaKNn9PAWcDaAysG3pl1+cmjTJrZkzXcrMPPy2kJGRtvr39+m663LUsSPPIAIAAKhINIkAAAAAVAi3+2fFxAyX07k937HZ66ShM6Tdvx6YTJMIqGy2bbM0ZYpbkye79c8/BW8nd7CTTvLr8su9+r//8yo6+ojTAQAAUA5oEgEAAAAoZ7YiIl5WVNQoWZa5zZQvIN0/V3r2J8kKSA773zPYag6oFPx+afZslyZMcGvePKds+/CrhurXD+iyy7waPNinNm1YNQQAAFDZ0CQCAAAAUK7Cw99WdPRD+eLpOXXU78O9+nFz7th98N+TPawkAipSero0ZYpb77wjbdgQcdi5Lpets87y6fLLverd28/jxAAAACoxmkQAAAAAylV4+JR8sS0p7dXtvfXamhaMRfiCr222mwMqxM6dlsaOdev99z3au/fwq4aOOcavwYO9uuQSnxo0sMspQwAAAJQETSIAAAAA5ciW07nWiMzbcJLO/HCRAof8Tfncvw46KzKyHHIDcMDatQ69+aZb06a5lZNTeHMoOtrWhRd6dfnlXh1/fEDW4ftIAAAAqGRoEgEAAAAoNw7HNjkcqUZs2Of5G0R99zXUmFk788Y2T7kHypxtSz/+6NSbb3r07beH/3NB585+jRiRowsu8IkeLgAAQNVFkwgAAABAuXE61xjj1Gxpy35zzohOI/XiG38pKvugJlEUTSKgrHi90uefu/TGGx4tX+487NzzzpNGjMhQt25+Vg0BAABUAzSJAAAAAJQbl8vcam51UvC103Lq8dOf0dUdR8j9bD9jnh0VVR7pATVKZqY0bpxb77zj0datjkLneTy2Bg706vrrverePUpJSX7ZPHIIAACgWqBJBAAAANRQP/+8SHXrRmvPnrRyu6dffxrjVbtz/zPGE6t3zn5fvY8+U5Jkpacb82gSAaXL75cuuSRSixYVvnKoTh1bw4fn6OqrvWrQwGblEAAAQDVEkwgAAACooaKjYxQbG6OcHKvcVgUkZ/+sWgftHLc6STo6ppk+OO9jtavbPjfo9cqZuNk4j+3mgNI1d66z0AZRixYB/ec/ObrsMq/ozwIAAFRvNIkAAAAAlBu/vdcYh1lH66tL5ikuIi4v5pk/V4695jxfh47lkh9QU0yc6M4XO+kkv264IUf9+vnkPPyjiQAAAFBN0CQCAAAAUG4sh7mN3CmN+hoNIkkKmzbFGHtPOFGB5i3KPDegptixw9I335h/Dhg9OksjR3orKCMAAABUFJpEAAAAQA315puvye/PltMZppEjbyrz++3JSlZULZ8Raxrd3hhbaakK+3K2Ecu6ZFCZ5wbUJJMnu+X3Bx8wFBlpa8gQGkQAAAA1EU0iAAAAoIZ6883XtH37NsXHNyqXJtGfu//QmeaiIcVHHSPbHxx7Zs+UlZmZN7ZdLmX/30VlnhtQUwQC0ocfmlvNXXSRV9E89gsAAKBGclR0AgAAAABqhj93L1O0x4w5rRhjHH7IVnM5vc+UHXdIZwlAyCZNcmvzZvNPAVdeySoiAACAmoqVRAAAAADKxcZ9y+R2mjHbjgoOsrPl/vEH43j2JZeVQ2ZA9ZedLT30UJjGjTM7tR06+HXccYEKygoAAAAVjSYRAAAAgHJxdK3NxtgfcMjvb5Q3ttLSZPn9xhzvKd3LJTegOvv7b0sjRkTozz+d+Y6NGJEjyyrgJAAAANQIbDcHAAAAoFx0aLjbGG/f30TSQSuJAvlXM9gud74YgKKbOdOlM8+MKrBBdOWVORo82FcBWQEAAKCyYCURAAAAgHJxbPxeY5y47xi1igyOrYBf+Tj5XhsQiuxsafToML3zjiffsYgIW08/naVBg2gQAQAA1HQ0iQAAAACUOctKVfv6aUZs5/72RpOooJVEctAkAopr0yZL110XoaVL868eatPGr3ffzVL79jyHCAAAAGw3BwAAAKAcuFy/ynXQp48cv7Q3o4M5yV/QSqL8f+QGULjZs3O3lyuoQXTxxV59/XUGDSIAAADkYSURAAAAgDLn8fxsjBdulZxWLSNmZWflO8920CQCiiIrS3rkkTCNHZt/e7nwcFtPPJGtK67wyrIqIDkAAABUWjSJAAAAgBqqc+dj1azZ0apVq06Z38tyrDTGP22ROsXWM2LuBT8Y40BsLSkiosxzA6q6DRssjRgRoRUr8jdVW7UK6N13M9WhA6uHAAAAkB9NIgAAAKCG+uCDKYqLi1FSUqpsu2zvFbDMJtGq3dLA5glGLOzLWcY458yzxLIH4PA+/tilu+4KV0ZG/n9XLrzQq+efz1J0dAUkBgAAgCqBJhEAAACAMpajCM9WI5KScZSiPTF5Y2v/Prl/NFcS5ZxzfrlkB1RFaWnSvfeGa8oUd75jERG2Hn+c7eUAAABwZDSJAAAAAJQpp3OjHJa51ZVTHY2x59s5srzevLHt8Sinz1nlkh9Q1axY4dB114Vr/fr828u1a+fX229nqV07tpcDAADAkTkqOgEAAAAA1ZvTudYYb0uVmsceZ8Q8X842xjln9JQdHSMAQbYtjR/v1jnnRBbYILryyhx99VUGDSIAAAAUGSuJAAAAgBpqyJDLtG/fXtWqVUcTJ04pk3skZyZr8dYnNeS4YGxNktS+bodgwOeT57tvjPPYag4wZWZKd91V8PZy0dG2XnghSwMG+CogMwAAAFRlrCQCAAAAaqg///xDv/76q/78848yuf6Xf8/W6R+drMzAKiO+Llk6rsHxeWPnX+vkSEs15mSffU6Z5ARURf/8Y+m88yILbBAde6xf332XToMIAAAAIWElEQAAAIBSlZK1V/f/eLemrvtIktQ01jzeIOIkNa/VIm/sWvGncdzfpKnshg3LPE+gKvj2W6duuCFCKSlWvmP/+U+OHnggW2FhFZAYAAAAqgWaRAAAAABKzdzN3+i2eTdpR/r2vFiTQ5pEZza9RtnZwbFrxXLjuK9jp7JMEagSAgHpuec8ev55j2zbbBBFRdl65ZUsXXABq4cAAABQMjSJAAAAAJRYas5+PfzT/fpg9fh8xw5dSRQINDHG+ZpEHWgSoWbbu1e64YYIffdd/o/sbdv6NW5cltq0CVRAZgAAAKhuaBIBAAAAKJEVScs19ItBSkzbku9Y4+hY1Qrfb8QCgUbBgW3LtcJ8JpKvY+cyyROoClatcmjo0Aht3pz/EcL9+3v10ktZio6ugMQAAABQLeX/rRMAAAAAiijTl6krv7iswAZRz6a99cMVz+eL+/2N8147N66XY+9e4zjbzaGmWrfOoYsvzt8gcjptjRqVpXfeoUEEAACA0lWjVhLdc889mj59erHOmTBhgrp27Zo39nq9mjx5sj7//HNt2LBBtm2rcePGOvPMMzV8+HDVrl27lLMGAAAAKq+JK8dpa1qiEYtyR2tU98c1rOM5qlPndOOY399UUkTeOPLlF4zjgdq1FTi6WZnlC1RWmzZZuvTSCCUnmw2i+vUDevfdLHXr5q+gzAAAAFCd1agmUSiioqLyXmdnZ+vaa6/VwoULjTnr16/X+vXr9emnn+q9995T27ZtyztNAAAAoNxl+jL16tKXjFjHuM56v9+HOjq2sWJjL5DTudM8J/O6vNfOdWsV9vFk43jWwMGSZZVZzkBltGOHpUsuidT27WaD6IQT/Bo3LlNHHWVXUGYAAACo7mpUk2j06NF68MEHDzvnl19+0U033STbtnXdddepY8eOecfuvfdeLVy4UG63WzfddJPOP/98eTwezZ8/X88++6x27dqlkSNHatasWYqMjCzrtwMAAABUqA9Wva+dGTuM2JOnP6ejY5spKupBeTw/GcdycvooM/OmvHHU04/LCgTyxnZklDJuuaNskwYqmeTk3BVEmzblbxBNnZrB9nIAAAAoUzWqSeTxeOTxeAo9vmvXLj3wwAOybVsnn3yybr/99rxjy5cv1+zZsyVJ999/vwYPHpx37NJLL1WHDh00cOBAbd26VRMmTNDIkSPL7o0AAAAApeD662+S358tpzOs2Odm+bL0yu8vGrEzmvRS1/hT5PF8rsjIl41jfn9T7d//riSnJMn15zKFzZxhzMn4z/WyGzQodi5AVbV/vzRoUITWrnUa8YQEvyZNokEEAACAslejmkRH8sADD2jv3r2KiorSM888I4cj+E2ucePGSZKaNGmigQMH5js3ISFBAwYM0NSpUzV16lSaRAAAAKj0rr/+JsXFxSgpKVV2MXezKmgV0Z0n3SOn8y/FxFxvxG3bo/37J8i26+XFIp981JgTqFVbmTfcUrwkgCosI0O64ooI/fGH2SBq2TKgjz/OVJ06FZQYAAAAahTHkafUDF988YXmz58vSbrjjjsUHx+fd8y2bS1YsECS1KtXLzmdzgKv0adPH0lSYmKi1qxZU8YZAwAAABUjLSdVL//+ghE7vUlPnRLfTdHRt8jhSDXnpz0tn++EvLFr4W8K++4bY07GzbfJrlW7zHIGKhO/X/rPfyL022/m9zabNAlo2rQMNWjAM4gAAABQPmgSScrOztazzz4rSWrbtq0GDRpkHE9MTNT+/fslSR06dCj0OgkJCXmvV6xYUQaZAgAAABXv6UVP5FtF9L8T75HDsTnfc4iysgYpK+tqIxb5yvPGOFC/gTKv+U/ZJAtUMrYt3XdfmL7+2mwQNWiQ2yBq0oQGEQAAAMoPTSJJkyZN0rZt2yRJd955Z76VQlu3bs173aRJk0KvU79+fbndbkm5jSUAAACgMktLS9X+/fuVlpZ65Mn/Wp70p975800j1qtpH53SqLs8ni+NeCBQR6mpL0my8mLO1asUNucrY17GLbdLUVHFzh+oit54w61x48xn5daubWvq1Ey1bEmDCAAAAOWrxj+TyOfz6f3335eUuxKoR48e+ebs3bs373VsbGyh13I4HIqKilJKSkreyqPisKzcf2qyA++/ptfhAOoRRC2CqIWJegRRCxP1CKIWQdTC1L37Sdq+fZvi4xvpzz+PvF1ywA7orvm3KWAH8mJhzjA9ecazsiwpLOwLY35OTl9ZVqQRi3z9ZfOadeoo68qrKvy/E342TNQjqDRrMWOGS6NGhRuxsDBbEydmKiEhUMhZlQc/FybqEUQtTNQjiFoEUQsT9QiiFibqEVReNajxTaIvvvhCO3bkbpVx3XXXFTgnOzs773V4eHiBcw4ICwvLd05R1a0bU+xzqqt69ajFwahHELUIohYm6hFELUzUI4haBFGLXA6HlfefcXFHrslbi9/Skp2Ljdh9p9+nrq27SNov6UfjWHj4xQoPP+i6mzdLn041c7j5ZsU1Oyqk/MsCPxsm6hFU0losWCDdeGP++Pjxls4/PzL/gUqMnwsT9QiiFibqEUQtgqiFiXoEUQsT9Sg/ZdYkys7O1jfffKOdO3fq6KOPVq9eveRyVb6e1IFVRM2bN1ffvn0LnHPo9nNlZc+eVAUq/5fHypRl5f4PQHJyqmx2WqAeB6EWQdTCRD2CqIWJegRRiyBqYQoE7Lz/TEo6/JZzuzN26+5v7zFiLWu10jXtblBSUqo8nhmKjfXmHbNtt/bs6SbbDl436vGnFOHzBedERGjP5cNlH+He5YGfDRP1CCqNWqxfb6l//yjl5JhfB3344Sz16eNVUlIpJFoO+LkwUY8gamGiHkHUIohamKhHELUwUY8gh6N8FpaUqGuTnZ2t8ePHa/bs2RozZozi4+MlSZs2bdLw4cO1ffv2vLnx8fEaM2aM2rZtW7KMS9Hff/+tlStXSpIGDBggh6PgRzRFRETkvT7SCqEDx4+04qggtq0a/4N/ALUwUY8gahFELUzUI4hamKhHELUIohb5HakeT/w6WvuyU4zY02e8II8jTLatfM8j8npPUyAQ3KrZSkpS+AfjjTlZl1+pQN16UiX674KfDRP1CAq1Fj6fNGxYhFJSzAbRVVfl6IYbvFWyvvxcmKhHELUwUY8gahFELUzUI4hamKhH+b3/kJtEgUBAI0aM0KJFiyRJW7ZsyWsS3X///dq2bZsxf9u2bbrmmmv05ZdfKjo6ugQpl545c+bkvT7vvPMKnXfwc4hSUwv/lmMgEFB6erokqU6dOqWQIQAAAFDx9mfv07R1U4zYRW0uVY+mvf4dBeTxfGMcz84+1xhHPfuErIyMvLHtdCrj+pvLJF+gMvnqK5fWrTN3pzj7bJ+eeCKbvfYBAABQ4QpeOlMEn3/+uRYuXCjbtnXyySerXr16kqR169Zp8eLFsixLF154oRYuXKj3339fcXFxSkpK0gcffFBqyZfUgSZRp06ddPTRRxc6r3nz5nmvD21+HWz37t3yenO32DjQMAMAAACqutkbZyrLn5U3djlcGtX98eDYtVQOR7JxTk5OcCtn59o1Cp8wzjieffFABY5uVkYZA5XH2LFuY9yunV9vvZWpSrgbOwAAAGqgkJtEX375pSzL0v/93/9p/PjxatWqlSTpm29yv0HodDp19913KzY2Vqeccopuv/122batuXPnlk7mJbRnz568rebOOuusw85t0KCBateuLUlatWpVofMOXE+SEhISSp4kAAAAUAlMXfeRMT7z6LPVMOqovPGhq4h8vjYKBJrnjaNGPSDL788b2+HhSr/3wbJJFqhE1qxx6McfzW7QzTfnKCqqghICAAAADhFyk+hAQ2T48OFGfMGCBZKkLl265DVWJOnkk0+WlPu8osrg999/l/3vpn6dO3c+4vwePXpIkr7//vu88w51oAFWv359tWvXrpQyBQAAACrO1tRE/bR1gRG79JhBxtjj+dYY5+QEv4Tlnvedwr6dYxzPuOFmBRo3KeVMgcpn3DhzFVFcXED9+/sqKBsAAAAgv5CbRPv27ZNkbqu2f/9+LV++XJZl6dRTTzXmR/37VakDz+ypaAeaXJZlqVOnTkecf+GFF0qSNm7cqEmTJuU7vmrVKs2YMUOSNGzYMFlsLg0AAIBq4JO/pspW8EtSsZ5aOqtZv7yxZe2Ry7XYOCevSeT3K/qR+41j/gYNlXHT7WWXMFBJ7N8vTZliNomuvNKrsLAKSggAAAAoQMhNorB/f7Pdv39/Xuynn36S/99tJLp3727MT0xMlCTFxsaGestStWHDBklSvXr1FB0dfcT53bp1U+/evSVJjz/+uF588UVt2bJFu3fv1rRp0zR8+HB5vV41adJEgwcPLtPcAQAAgPKQnJmsiavMZwn1bzVA4a7wvLHHM1eWFcgb23aEvN7cL4yFTf1IrtXmds0Z9z4oFeH3b6CqmzrVrYyM4JcHHQ5bQ4d6KzAjAAAAIL+QH5XZpk0bLVu2TD/99JMGDcrdbmL27NmSchsvh27h9tFHufuYt23bNtRblqqtW7dKkmJiYop8zlNPPaVrrrlGy5cv15gxYzRmzBjjeFxcnMaOHVukphMAAABQ0SZOnKzISLcyMvL/4XpL6mYNnDlAm/b/Y8QP3WrO7f7VGOfknC4pt4kU9sUs45gvoaOyBl1R8sSBKuC778yP2+ec41PjxgVvXQ4AAABUlJCbRGeddZaWLl2qZ555RikpKUpKStK3334ry7J07rnn5s3bsGGDJkyYoE8//VSWZemcc84plcRLKjU1VVLxVjbVqlVLkydP1uTJkzVz5kxt2LBBOTk5aty4sXr16qURI0aoXr16ZZUyAAAAUKqOPbaL4uJilJSUqoMfu7kqeaUGzbpIO9K3G/M71OukrvHdjJjDsdMY+3zBL4s59u4xjmUOGSo5naWUPVC57dplbkF+1lk8iwgAAACVT8hNossvv1zTp0/XX3/9pZdffjkvXqdOHY0cOdKYd2BLumOPPVaXXnppCdItPXPmzDnypAK43W4NHTpUQ4cOLeWMAAAAgIr367afNeSLy7Q/Z58RbxLdVO/2fV8Oy9yx2uHYZYwDgQbBQU62efHIqFLNFajM9u41m0R167KKCAAAAJVPyM8kCg8P1wcffKCBAweqdu3aioyMVM+ePTVp0iTVrVs3b17Lli3ldDp14YUX6t1335XDEfItAQAAAJShLzbO0qUz/y9fg6h93QTNvugbtardJt85lmU2iWw72CSysswmkf3vc02BmmDPHrNJVLt2xeQBAAAAHE7IK4mk3O3XRo8erdGjRxc657777lN8fLzi4uJKcisAAAAApWzOnC/ldlvyem3tbLxLd86/VQE7YMzpGt9NE8/5SLXD6xR4DYdjtzE2VhJlZxnH7LDw0kkcqORycqT0dFYSAQAAoPIrUZOoKDp16lTWtwAAAAAQgjvvvF3bt29Tnfp1tPfGvfmO92t+rt46e5wiXBGFXCFLDsd+I5LXJPL7ZaWnm9PDWUmEmuHQreYkqU4dmkQAAACofNj7DQAAAKjh9uek5otd0X6oxvb74DANIsnj+S5fLBCIk7V/n2KHDJRz5w7jGCuJUFNs3kyTCAAAAFVDiVYS2batzz//XF999ZUSExOVlZWlQCBw2HMsy9K3335bktsCAAAAKEX+gM8Y33r8Hbqv60OyrPx/6A7KUnT0vUbE52spx4ZkxV45WK6/1hnHbLdbvmPal1bKQKU2a5bbGLdqFZCrzPfxAAAAAIov5F9T/X6/rr/+ei1YsEBSbsOoKA7/QRMAAABARYpwRejuk+8/4u/tkZEvy+n8x4hlL+mv2n37yLEvxYjbbrdSX3pddoMGAqq7QED67DPzo/YFF3grKBsAAADg8EJuEk2dOlU//PCDJCkiIkLHHnus6tWrJ4/HU2rJAQAAAChfnesfJ5fj8B8THI7Niox8wYj5tzRX5KmvyPKbOwsE4uK0f+wH8p7SvdRzBSqjRYuc2rbN3Nl9wABfIbMBAACAihVyk2jGjBmSpISEBL377ruqW7duaeUEAAAAoIIc1+D4I86Jjr5flpWZN7YDlpwX/CP5zXm+hI7aN/EjBZoeXcpZApXXjBnmx+xjjvGrffvDb8sOAAAAVBTHkacU7K+//pJlWbrnnntoEAEAAADVxPENTjjscbd7nsLCPjNi1pu29Ic5L/u8/to7aw4NItQoOTnS55+bTaIBA3xi13UAAABUViE3iQ48g+iYY44ptWQAAAAAlJ8cf06+2JFWEkVEvG0GkiQ9aIbS77xH+9+bIEVHlzBDoOrw+aQbbgjX7t2HbjXH84gAAABQeYXcJGrWrJkkKSkpqdSSAQAAAFA+bNtWas5+I3ZUVLyax7Y47Hku1wozMFrS3n+vGRmpfe9NUMZd90mOkD9qAFVOICDddlu4Pv/cbcSPP96vVq3sCsoKAAAAOLKQP7mdc845sm1bn3zySWnmAwAAAKAcfLvpa+U4cySPcv+RdONxt8g67L5YOXI4tpihn4Iv0++4RzkXDCjlTIHKzbale+4J08cfmw2iyEhbTz2VVUFZAQAAAEXjOvKUgl111VWaNWuWxo8fryZNmmjQoEFy8G1BAAAAoNLzB/x67NdHpJuDsaYxR+uqjtce9jync5MsK2AGNwRfek87vfSSBKoA25ZGjQrT++97jHhYmK0JEzJ13HGBQs4EAAAAKoeQm0RffPGFLrroIr388st69NFH9eqrryohIUF169aVy1X4ZS3L0hNPPBHqbQEAAACU0LR1U7R6zyojdvfJ9yvMGXbY85zOv81AsqR9uS8D0THydTq2FLMEKr9nn/XojTfMBpHbbWvs2EydcYa/grICAAAAii7kJtE999yTtxWFbdvau3evfv7558OeY9s2TSIAAACgAmX5svT0wseNWId6HXVxm4FHPNfhOKRJtD740ndyV+kwXxYDqpv33nPruefMxqrDYWvMmCyddRYNIgAAAFQNIX+Ka9SoUWnmAQAAAKAczNr4mRLTzOcKPdDtETkdziOe63L9ZQYO2moup/tppZEeUCWkpUlPPWU2iCzL1iuvZOmCC3wVlBUAAABQfCE3iebOnVuaeQAAAAAoB4t3LAwO5kgNnQ21YNd89Xnk7COcacvt/tYMrQ6+9HU+rrRSBCq9NWsc2rfPMmLPPJOtgQNpEAEAAKBqcVR0AgAAAADKz6rklcHBcmnngp2aPv2TI57ndK6Ry7XRDH4RfGlHRpVShkDlt327+VH66KMDGjbMW0HZAAAAAKEr1U3D9+3bp+3btysjI0ORkZFq1KiRYmNjS/MWAAAAAEJk27bZJCqGsLBZ5rW2SNbvB0/wlCAzoGrZvt1cRdSoUaCCMgEAAABKpsRNokAgoClTpmjy5Mn666+/8h1v0aKFBg4cqKFDh8rhYOESAAAAUFES07Zof86+kM71eMwmkTXDPG57zOezANXZtm3mZ9v4eLuCMgEAAABKpkRNon379unGG2/UkiVLZNsF/1K8ceNGPf3005ozZ45ef/111alTpyS3BAAAABCiQ1cRWZYlW0f+47bDkSi3e6kZnH7IJFYSoQawbWn1aocWL6ZJBAAAgOqhRE2iW265RYsXL5YkJSQkqF+/fmrVqpUiIyOVnp6u9evX6+uvv9bq1au1dOlS3XXXXXrnnXdKJXEAAAAAxbMqaYUxdjvcylHOEc/zeL40xgF/LTkWmCuS7LDwkicIVEL790vz50vTp4dp7lxXvucRSVJ8PNvNAQAAoGoKuUk0e/Zs/fbbb7IsS3fffbeuuuqqfHPOPPNMjRw5UuPHj9dTTz2lH3/8Ud9995369OlTkpwBAAAAhGB7+jZj7HK4itQkcrsXGWPvnu4K85mNIzs6uuQJApVAICCtXOnQd9+5NHeuU4sWOeX3S1Lhq+XatKFJBAAAgKop5CbR559/LsuydNlllxXYIDrYsGHD9M8//2jy5Mn65JNPaBIBAAAAFSDbn22MLcsq0nku13JjHFjfwBzXbyC7Vu0S5QZUpL17pe+/d2nu3NzG0O7dRX+e7imn+NSjh78MswMAAADKTshNopUrc/czHzRoUJHmDxo0SJMnT9aaNWtCvSUAAACAEsj2ZxljS0VpEmXJ6Tzkd/hl5vNXfO3alzAzoHwFAtKyZQ7NnevSd9+5tHSpQ4FA0ZqmktS0aUC9e/vUp49PZ53ll9NZhskCAAAAZSjkJlFKSookqVGjRkWaHx8fL0lKSkoK9ZYAAAAASiDTV/wmkcu1WpYVXCVh25asH/Yac3zHtCudBIEylJRkad48p+bOden7751KTi76aqGwMKlbN9+/jSG/WrcOqIgL8QAAAIBKLeQmUWxsrPbu3att27YpNjb2iPO3bcvd/zwmJibUWwIAAAAogUNXErXpeoxaRjZXZGThv6MfutWc399Srj82mLFjWEmEysm2pSlTXBo71qM//nDItove2WnRIrhaqH//SGVmZsq2j3weAAAAUJWE3CTq0KGDfvzxR02bNk0PPPDAEed//PHHkqT27fkACQAAAFSErENWEl16x2W6p/edSkpKLfSP3y7XImPs83ZU2PovzBhNIlRCXq90111h+vBDT5HmR0TYOvVUv/r08alXL59atsz9l8KypKgoKTOzLLMFAAAAKkbITaILLrhACxYs0KRJk9SqVSsNHjy40LmTJk3S5MmTZVmWLrjgglBvCQAAACBEqTn7tWzXUiMWG3b4HQGczvUKD59kBtdGyvJ6jZC/HdvNoXLZt0+6+uoILVhw+I+8bdr41bu3X717+9Stm1/h4eWUIAAAAFBJhNwkOv/88zV58mQtXbpUo0eP1ieffKK+ffuqVatWioyMVEZGhjZs2KCvv/5aK1eulG3bOu6449S/f//SzB8AAABAEUxdN0UZvvS8sdNy6vTGPQ57TlTUPbKsYEPItt2y3ko25nhPOEl27TqlmyxQAps3W7riigitXevMdywqytbpp/vyGkNHH83+cQAAAKjZQm4SORwOvf766xoxYoRWrlyZ98+h7H/3rejYsaPeeOMNWTzdEwAAAChXtm3r/RXvGrG+zc9VfHSjQs/xeL5SWNgcI5aZco0i3n/fiGVdOqjU8gRK6vffHRoyJEJJSQ4jHhFh67nnsvR//+eTp2i7zwEAAAA1QshNIkmqW7euPvroI02ZMkVTp07VunXr8ppCkmRZltq0aaPLLrtMl112mdxud4kTBgAAAFA8v23/RWv2rDZiV3W8Rmee2UNJSbsUF9dA33wz/6Cj2YqKuseY7/c3lP+j1rKygs81sl0uZQ+4qCxTB4ps5kyXbrwxXFlZ5hcT69cP6IMPMtWlS6CCMgMAAAAqrxI1iSTJ7XZryJAhGjJkiPbt26edO3cqLS1NkZGRio+PV61atUojTwAAAAAhen+luYqoRa2WOqNJT9266wZt375NgYC55VZExBtyuTYasfT0UQr/8EMjlnPm2bLr1iubpIEism3pjTfcGj06TLZtNojatfPrww8z1bQp28oBAAAABSlxk+hgtWrVoikEAAAAVCLJmcmaueEzIzaswzVyWI4C5zsciYqKesaIeb0nybvhVMX+NNKIs9UcKoOJE90aNSo8X7xHD5/eey9TsbEVkBQAAABQRRT8yRAAAABAtbB012J5A968cZgzTIPaXV7IbFvR0XfIstKDEdtSWtqzCv94ijEzEFtLOWf1K4uUgSL7+29LDz0Uli8+ZEiOJk2iQQQAAAAcSZFWErVv316SdPTRR+vrr782YsVlWZZWrVoV0rkAAAAAiiclO8UYt6zVWnXDC94izuOZobCwL41YVtZQ+XKOU+zkq4x49v9dJIXnX70BlBe/X7rllnBlZJhbzD3wQLZuvjlHllXIiQAAAADyFKlJZNv5928uKAYAAACgcknLSTPG0Z7oAudZ1l7FxPzPiAUC9ZWePkruX3+Wc9M/xrGsy4eUap5Acb31llu//WZ+pL3qqhzdcktOBWUEAAAAVD1FahLddNNNkmQ8b+hADAAAAEDllepNNcYxnpgC50VFPSSHY5cRS0t7RrZdV+GTJhpx3zHt5Dv+xNJNFCiGtWsdevJJc5u5Zs0Ceuih7ArKCAAAAKiaitUkOlIMAAAAQOWSnmM2iaLd+ZtElpWtiIjxRiw7u6+ysy+SlbpfYbM+M45lDRoi9vJCRfF6pZtvDld2dvBn0LJsvfpqlqILXigHAAAAoBCOUE/ctm2btm3bVuT5Pp9PixYt0hdffBHqLQEAAAAU086MncY42p3/r+iWtc8YBwLRSkt7QZKlsM+my8rIyDtmO53KunRQmeQKFMWbb3q0bJnTiI0c6dUpp/grKCMAAACg6irSSqKC9O7dWw6HQ0uWLFFERMQR56elpenKK69UXFyczj333FBvCwAAAKCIxq54Rx+unmDEYsJiC5jpM0YZGQ8qEGgqSQr7dKpxLOesvrIbNCjVPIGiSkuTXnvNY8TatvXr3nvZZg4AAAAIRchNIkmybVtWEbeZOLDqaP/+/SW5JQAAAIAieHPZa3r45/vyxU9rfEbe64cfHi2nc70iI5/Ki9l2pDIzr5MkWbt2yf3zj8b5WZcOLqOMgSP78EO3UlLMz6CvvJKl8PAKSggAAACo4o7YJAoEAvrf//6n3bt3F3j82muvlcNx+F3rvF6v1q1bJ8uy1KhRo9AyBQAAAFAkLy5+Vk8ufDRf/JYu/1Xf5ufkjS++eKDi4pZJOrhJ5JGUu5VX2OzPZQUCwWORUco58+yyShs4LK9XGjPGXEXUr59Xxx8fKOQMAAAAAEdyxCaRw+FQ7969dccdd+Q7Ztu2Fi9eXKwbDhs2rFjzAQAAABSNbdt6auGjenHJc/mO3X3y/frvCXcVcJbvkHHwI0LYzBnGkey+/aQibDUNlIUZM1zautX8guKNN3orKBsAAACgeijSdnPnnXee1q1bp127duXFpk+fLsuydP7558vlOvxlXC6Xateura5du+q0004rWcYAAAAA8rFtWw//fL/G/PFavmMPdXtUN3W5tZAzzSaRbef+bm/t3p1vq7nsCy4slVyB4rLt/M8iOukkv7p29VdQRgAAAED1UORnEt1+++3GePr06ZKk0aNHK4JvEwIAAAAV6pXfXyiwQfTk6c/qmk7/KfCc9ev/0s6dG+RySccccyCa+xGhwK3m+pxV2mkDRTJvnlOrVzuN2E035VRQNgAAAED1UeQm0aFuvPFGWZYlt9tdmvkAAAAAKKYsX1a+LeYsWXqh56u6ImFooedddNEF2r59mxo3lhITD0T/fR7RV7ONuWw1h4o0Y4b5ubNNG7/69j10q0QAAAAAxRVyk+jmm28uzTwAAAAAhGjhjl+V4UvPG1uy9Fqft3TpMYOKfS3bdks5OXL/+rMRzzn3ghLnCYRq40bLGA8e7JXDUchkAAAAAEVWLr9W5+TkaO/evVq2bJmeeeaZ8rglAAAAUGP8sOV7Y3xcgy4hNYhyeeT+fbGsjAwjmnNajxCvB5RcYqL50bVlS7uCMgEAAACql5BXEknSzp079corr2jBggXas2eP/P6iPTT0rrvuKsltAQAAABxkfuI8Y9yjSe+Qr2XbHrkXzDdi3o6dZderF/I1gZLIyZF27DBXEjVtGihkNgAAAIDiCLlJtG/fPg0aNEg7duyQbRf9W1xRUVGh3rLUpKWlacKECfr222+1efNmZWdnq1GjRurRo4euueYaNWzYsNBzMzIyNG7cOH311VfavHmznE6nmjVrpnPOOUdDhw5VeHh4Ob4TAAAA1HR7spL15+5lRuyMpj1LcEV3/ibRaWeU4HpAyWzbZikQMJtETZrQJAIAAABKQ8hNookTJ2r79u2SpISEBJ100klKTEzUd999p5NOOkknnHCC9u7dq99++03//POPLMvS8OHDdcstt5Ra8qFYs2aNRowYoV27dhnxf/75R//8848+++wzvfPOO+rcuXO+c/fu3asrrrhCGzZsMOKrVq3SqlWrNH36dL3//vuHbTIBAAAApenHxB9kK/ilrQhXhE46qmvI17P9TrmXLDJi3jPYag4V59Ct5qKjbdWuXTG5AAAAANVNyM8k+uGHH2RZlnr27KlPPvlE9957r2699VZJktPp1G233aZRo0bpyy+/1A033CDbtjVt2jSlpqaWWvLFtXv3bg0bNky7du1STEyMHnroIc2dO1dz5szRvffeq4iICKWkpOjGG29UWlqacW4gEND111+vDRs2KCoqSg8//LB++OEHzZs3T//73/8UFhamjRs36qabblIgwLfaAAAAUD5+SDRX/ZwS311hzrCQr2ftz5Tl9eaNbZdL3lO6h3w9oKQSE/NvNWdZhUwGAAAAUCwhN4k2bdokSbr66qtl/fsbeps2bRQREaFly5blNUosy9Itt9yinj17KjU1VZMmTSqFtEPz1FNPKSUlRZGRkRo3bpyuuOIKNW7cWM2aNdNVV12ll156SZK0a9cuffbZZ8a5c+bM0dKlSyVJL730ki6//HI1bNhQjRo10rXXXqtXXnlFkvTnn39q9uzZ5fq+AAAAUHP9uv0nY3x6k54lup6dHGuM/a1ay46OKdE1gZLYt8/sCMXFFX27cwAAAACHF3KTKD09XZLUokWLvJhlWWrVqpWys7O1ceNGY/6QIUNk27Z++OGHUG9ZIklJSfryyy8lSddff706deqUb07Pnj3VvHlzud1urVy50jg2btw4SdJJJ52kM87Ivyd7z5491b177jcsP/7449JOHwAAAMgnOTNZ6/auNWLdGhVn1Y83X8S3oqMxtmNrhZIaUGoyMswmUVQUTSIAAACgtITcJIqKiiow3qxZM0nS+vXrjXjbtm0lSVu2bAn1liXy9ddfy+/3KyIiQkOGDCl03ueff64VK1boiSeeyIulpKTojz/+kCT16dOn0HMPHFu8eLH27dtXSpkDAAAABftt+y/GONIVqc5xxxX5fMvKMMZ+fyMFVh5lxOwYVhGhYmWYP6aKjKyYPAAAAIDqKOQmUXx8vCTp77//NuJNmzaVbdv666+/jLht537bK+PQ3/DLyZ9//ilJ6tSpkyIP+VThPWjP9bCw/Pu3r1mzJi//Dh06FHqP9u3bS8p9ftGqVatKnDMAAABwOIc2iU5oeJLcTncRz/Zq4cJwbdkiLVqUG8nOHiwr9ZBnc8bEFnAuUH4OXUkUEcFKIgAAAKC0hNwkOvHEE2Xbtt5++22jydK6dWtJ0vz55gN0Fy5cKKnwFUhl7UDTqnnz5pKk7777TsOHD1eXLl3UsWNHnXbaaXrkkUe0c+fOfOdu3bo173WTJk0KvUfjxo3zXicmJpZS5gAAAEDBftv+szHuGt+tyOd6PN+qceNkNWki/fv9L2VlXS4rNdWYZ0dHlzhPoCRYSQQAAACUHVeoJw4aNEgffvihfvrpJ1100UW68cYb1a9fP3Xr1k1Op1MrV67Uk08+qYEDB2r9+vV66qmnZFmWEhISSjP/Itu1a5ckqVatWnrooYc0ZcoU4/ju3bs1efJkffnllxozZoy6dOmSd2zv3r15r2NjC/8mZfRBH6D3799f7BwtK/efmuzA+6/pdTiAegRRiyBqYaIeQdTCRD2CqEVQdapFas5+/Zn0hxE7pVG3Ir+3sLAZxtjrPUmBQBs5//29OU9MTLWo15FUp5+N0lCZ6nHoSqLISLtc86pMtaho1MJEPYKohYl6BFGLIGphoh5B1MJEPYLKqwYhN4lat26tG2+8Ua+99prWr1+v+fPnq1+/fqpXr54uueQSTZkyRRMmTNCECRMk5W43Z1mWBg8eXGrJF0d6erokacaMGdq9e7dOPPFE3X777erUqZPS09P15Zdf6rnnnlNKSopuuOEGffbZZ2rQoIEkKTs7O+864eHhhd7j4GMHn1NUdeuy3/sB9epRi4NRjyBqEUQtTNQjiFqYqEcQtQiqDrX4bsUX8gV8eWO3w62+HXor2lPUlT9bjZHbPVBx/gzpq9lGPOKY1oqIq/r1Kqrq8LNRmipDPdLMHRAVHx+muLj824SXtcpQi8qCWpioRxC1MFGPIGoRRC1M1COIWpioR/kJuUkkSTfddJNat26t9957T02bNs2L33///dq9e7fmzp2bF7MsSyNGjNDZZ59dkluGLCsrS1LuiqGuXbvqvffek9udu197WFiYrrjiCrVt21ZDhw7Vnj179Pbbb+uBBx6QJDmdznLJcc+eVAUC5XKrSsuycv8HIDk5VTZbjVOPg1CLIGphoh5B1MJEPYKoRVB1qsVHf3xsjE9rcoay9tvKUmohZ5hq1UrTuHG5f4CPjpYGD46U85nnFXnQl51sj0d7zjxPdlLRrlmVVaefjdJQmeqxbVukpOBnsqioTCUl+Qo/oZRVplpUNGphoh5B1MJEPYKoRRC1MFGPIGphoh5BDkf5LCwpUZNIkvr166d+/frJPui/MY/HozfeeENLly7V0qVL5XQ6deqpp+Y9r6gihIeHK+PfzazvueeevAbRwU466ST16NFD8+bN05w5c/KaRBEREXlzsrOz5XIVXLYDjagD9ysu21aN/8E/gFqYqEcQtQiiFibqEUQtTNQjiFoEVfVaZPuz9e2mb4zYuS0uKNZ7sqwsjR4tbd0qNW4sDbrQUvi4d405WZdcpkDDo6QqXKviquo/G6WtMtRj1y5zn4369e0Kyaky1KKyoBYm6hFELUzUI4haBFELE/UIohYm6lF+77/ETaIDrAI2yOvSpYvxbJ9t27Zp+/btOuGEE0rrtkUWFRWljIwMxcTEHPa5SCeffLLmzZunnTt3Ki0tTdHR0cZziNLS0hQVFVXguakHPeS3Tp06pZc8AAAAcJAFid8rzRv83dOSpX4tzivWNSwr0xi7f1gox0HP4pSkzOtvDj1JoBT4/VJycv4mEQAAAIDS4Qj1xHbt2ikhIUGZmZlHnixpz5496t27t26//fZQb1kiTZo0kZS7tdzhREcH93A/sDKoefPmebGtW7ceekqebdu25b2Oj48PJU0AAADgiL7YOMsYn3jUyWoY2bCYV8kyRp7PvjTG2Wf1lf+YdqGkB5Sa5GRLgQBNIgAAAKCshNwkkmRsMXckaf8+bTQlJaUktwxZ+/btJeU2q9IOffLpQZKSkiRJbrdbdevWlSS1adMmb6XU6tWrCz131apVknJXVbVrxwdqAAAAlL5MX6a+/NtsEp3b4oJiXiVdDsd+I+LctNO8zw23hJIeUKrmzzefD2tZturVo0kEAAAAlJYjbjcXCAT05JNPFtpYefjhh+V0Ogs8doDX69WSJUskSXFxcSGkWXI9e/bUpEmTFAgE9O2332rAgAEFzvvpp58kSZ07d5bDkdtDi46O1gknnKDFixdr7ty5uuKKKwo8d+7cuXnn1q5du9TfAwAAADBp9UQlZyUbsXNbnl+sa0RFjZJlpZvBLcGXvvYJ8nY/LdQUgVLx669O/fe/5rNemze3VcDjZQEAAACE6IhNIofDoWbNmumxxx7L99wh27Y1c+bMIt3owKqjiy++OIQ0S+7UU09V48aNtXXrVr344os67bTT8jWsvvrqKy1evFiSdOGFFxrHBgwYoMWLF+vHH3/U999/r549exrHv//+e/3888+SpKuuuqrM3gcAAABqLq/fq9eXvmzEejTppRa1Whb5Gi7XL4qIeMsM+p3S3/68YeCoeKmAZ44C5WXdOoeGDo1Qdrb5c3j99TkVlBEAAABQPR2xSSRJV1xxhZYuXapdu3blxRYtWiTLsnT88cfnrbgpiGVZcrlcql27trp27aqBAweWPOsQuFwujR49WiNGjNCOHTs0cOBA3XrrrTrllFPk8/k0c+ZMvfbaa5Kk4447ThdddJFx/kUXXaRJkyZp1apVuvXWW3Xrrbfq3HPPlSR98cUXevnl3A/rxx57rPr161e+bw4AAAA1wrR1U5SYtsWI3X7C/4pxhQzFxNwgyzK367IzIyQFdw6wD/P7PVDWdu60NHhwhFJSzAbR0KE5GjbMW0FZAQAAANVTkZpElmXp+eefN2IHnrnz7rvvKiIiovQzKwOnnXaannvuOd13333aunWr7rrrrnxzOnTooJdffjnfFnpOp1Ovvfaahg0bpi1btujpp5/W008/bcxp0aKFxowZc9imGQAAABAKf8Cvl383fyc/+ahT1K3RqUW+RlTU43K5NuQ/EDjk91d+n0UFSUuTLr88Qlu2mD+Dffv69NRT2SxwAwAAAEpZkZpEBRkwYEDeKqGq5LzzzlOXLl00btw4/fDDD9qxY4fCwsLUokUL9e/fXxdffLHCw8MLPLdx48b67LPP9P777+vrr7/Wli1b5Pf71axZM/Xt21fDhw9XVFRUOb8jAAAA1ASfb5iujfvMBs9/T/xfvi2hC+NyLVRExOuHRN2SvJLMlUU6wjNHgbLg9UpXXx2h5cvNn7/jj/drzJhMVbGPngAAAECVEPKv2U899VRp5lGuGjVqpPvvv1/3339/sc+NiorSjTfeqBtvvLEMMgMAAAAK9sayV43xsfW7qFfTM4t8flTUQ7KsQN7YtsPUsmUn1aqVobj0DCk1NTjZYiURyt+ECW59/735EbV584AmTswU38UDAAAAykaZfPrbs2ePfvzxR3377bfatGlTWdwCAAAAqDHSvGn6Y/dSI3br8XcUeRWR5JXH87MRSU+/V59+OlcrV67UF4MuN6ezpxcqwPTpZoOoXr2APvooQ/Xr24WcAQAAAKCkir2SaMmSJfryyy9Vt25d3XDDDcYxv9+vp556SlOmTJHXG3yg6CmnnKKHHnpILVq0KHnGAAAAQA3zz76/88X6NDuryOdb1v58sezswcHjPp9xzHa7i5EdUHKZmdLSpeY2c889l62WLWkQAQAAAGWpyCuJfD6f7rjjDg0ZMkQffvihfvzxx3xzbrvtNn3wwQfKycmRbdt5//zyyy+6/PLLtWbNmlJNHgAAAKgJ/t630Rg3jm6iCFdEkc+3rJR8sUCgdnDgNZtEPPwF5W3JEqe83uAKNofDVo8evsOcAQAAAKA0FLlJ9Mgjj2j27Nmy7dxvctWrV884/sUXX+ibb76RbduKiIjQQw89pFmzZum1115TmzZttHfvXt1zzz0KBAIFXR4AAABAIQ5tErWo1bJY5zsc5koi2/ZICg8GfF7zBFYSoZz9/LO5iqhz54CioysoGQAAAKAGKdJXBNesWaNPP/1UlmWpR48eevzxx/M1id58801JkmVZevTRR3X++edLklq3bq0TTjhBF1xwgdauXauvv/5a55xzTim/DQAAAKD6+qeETSLL2meMbbuWJEsjR16j1NR9qrPxb3108HFWEqGc/fKL2STq1s1fQZkAAAAANUuRVhLNnj1bgUBAJ554osaMGZOvQfT333/rr7/+kmVZatasWV6D6IA6depoyJAhsm1bX3/9dellDwAAANQAh64kal7CJlEgECtJ+vnnnzRnzhwt2LbVPIEmEcpRcrKlJUvMJlH37mw1BwAAAJSHIjWJfvnlF1mWpaFDhxZ4/ODnE5199tkFzjnttNMkSatWrSpujgAAAECN5Qv4tCJ5uRFrEVvcJlGqMbbtWB0SMIceT7GuD4TK75dGjgxXVlbweUSWZatrV1YSAQAAAOWhSE2iHTt2SJI6duxY4PFFixblve7WrVuBcxo1aiRJSkpKKlaCAAAAQE22bNfv2pedYsROaHhisa5hWZnG2LYjzeM+c9VGoFnzYl0fCNVzz3k0f765cu2cc3yqXbti8gEAAABqmiI1ifbty92eIiYmpsDjixcvliS5XC516dLlsNeyD/mWIgAAAIDCzU+cZ4zb1W2v+OhGxbqGZWUfEgkzh4c0iXxt2xXr+kAo5s516oUXzFVrDRsG9Mwzh/68AgAAACgrRWoSRUVFSZKysrLyHduwYYP27Nkjy7LUoUMHhYeHF3iNnTt3Ssp9PhEAAACAovl+y1xj3KNp72Jfw7LM3+Nt+5Df2Q/5Ipf/GJpEKFtbtli6/voI2XZwmzmn09Y772SpQQO+WAgAAACUlyI1iY466ihJ0saNG/MdW7BgQd7rU089tdBr/PLLL5KkBg0aFCtBAAAAoKZKzdmvxTsWGrFeITSJpEO3myv4i12SFIitpUDDo0K4B1A02dnStddGaO9ey4g/+GC2TjmFZxEBAAAA5alITaJTTjlFtm1r1qxZ+Y599tlnea/79OlT4Pler1dTpkyRZVnq2rVriKkCAAAANcuPWxfIbwf/aO5xeHRKfOFfzCpM/u3mCm8S+du0lSyr0ONAST39tEdLlzqN2HnneXX99d4KyggAAACouYrUJOrfv78sy9Inn3yijz76SJIUCAT00ksvafXq1bIsS+3bt1dCQkKB5z/55JPatGmTJOmss84qpdQBAACA6m3Rjt+Mcdf4bop0Rxb7Og5HkjG27YhC5/rYag5lKC1NGjvWfA5RixYBvfxyFr1JAAAAoAK4ijIpISFBAwYM0PTp0zVq1Ci98MIL8nq9ec8ocjgceuihh4xzdu3apYULF2rSpElaunSpLMtSv3791LFjx9J/FwAAAEA1tGX/ZmPcuf5xIV3H7f7JGPv9bQud6+sc2j2AovjsM7cyMsznEI0dm6nY2ApMCgAAAKjBitQkkqRHH31Ufr9fn3/+ufbv358Xd7vdeuyxx3TccccZ81977TVNnTpVkmTbthISEvI1kgAAAAAULjHNbBI1iWla7Gs4HP/I6dxkxHJyekiShp7fXznvjFGtg455Tzuj2PcAiuqDD9zG+Mwz/erQIVBB2QAAAAAocpPI5XLpmWee0VVXXaV58+Zp9+7datKkiS644AI1bNgw3/z69evLtm25XC6df/75uv/++xUTE1OqyQMAAADV2ZbULca4aQhNIo/nB2McCMTJ728vSbq/fQcd/Bt6oH6D3GcSAWVgzRqHliwxn0V0xRU5FZQNAAAAAKkYTaIDEhISCn320MHOO+88de7cWZ07d1adOnVCSg4AAACoqbJ8WdqVsdOINYk5utjXcbvnG+OcnDMk5W735f55gXns1NPEg2FQVj780FxF1KBBQGee6a+gbAAAAABIITSJiqply5Zq2bJlWV0eAAAAqNa2pSXmixV/JZEtt9tsBHm9/24n5/PJ/eMhx7qfXszrA0WTnS1NnWp+/Bw0yCtXmX0iBQAAAFAUjopOAAAAAEB+iYc0iWqF1VaMJ7ZY1/B4PpPTucOIHWgSRT94j5w7tpvHTqVJhLKxZIlTe/aYHz8vv9xbQdkAAAAAOIAmEQAAAFAJJWcmGeOGkfmfA3p4mYqOfsCI+Hyt5fe3Uvi7YxTx3ttqotyN55pI8h3TTv7WbUqSMlCozZvNbQyPOcavli3tCsoGAAAAwAE0iQAAAIBKaE9WsjGuG16vWOdHRr4mp3OzEUtPf1Ceb75W9AP35Juf+vyrPI8IZWbHDvOjZ+PGNIgAAACAyoAdoAEAAIBKKDkz9CaRw7FdkZEvGLGcnFPlX9pata/rKysQMI7ZtevId3LX0JMFjmD7drMBGR8fKGQmAAAAgPLESiIAAACgEjp0JVG9iKI3iaKiHpFlpeeNbdtSxub/qdaQy+RIT8s3346ICD1RoAgObRIddRQriQAAAIDKgCYRAAAAUAmFut2cy7VE4eGTjVhW5pWKuuxRObcmGnGaQygv27ebHz3j42kSAQAAAJVBkZpEn332mX744Qd5vd6yzgcAAACApMRUs6FTJ7xukc4LD59ojAOBGHm/PFXu35cYcW/XbrJr1S5RjkBRsd0cAAAAUDkVqUn08ssv6z//+Y/27t2bF3vttdf02muvyefzlVlyAAAAQE20NTVRS3YuMmKtarcu0rlO5z/GODPzP3L9sNaI+Y9upn3vT5Is8w/3QFnIzJR27TI/ejZqxEoiAAAAoDIoUpMoKSlJkhQWFpYXe+211/T666+zuggAAAAoZZ+unyZbwT+ix3pqqUeTXkU617LMZw4FAk3lWv6HEcsecLHsekV/xhFQEomJ+T92NmvGSiIAAACgMihSkyji373KV69eXabJAAAAAJCmrZ1ijPu3GqBwV3iRzrWsVGNsB6LzNYm8nY8tWYJAMWzebK5Yq1cvoOjoCkoGAAAAgMFVlElt27bV4sWLdfPNN6t79+6KjIzMO/bwww/L6XQW+YaWZemJJ54ofqYAAABADbAyaYVW71lpxC5uO7DI5x/aJFKyV45/dwY4wNeJJhHKzz//mN9NPPpotpoDAAAAKosiNYmuvfZaLV68WKmpqZozZ05e3LZtzZw5s8g3s22bJhEAAABwGNPWmauIGkU1VrdGpxb5/EObRI51O4xxILaWAs1bSJLefPMdhYc7lZXlDzFb4Mg2bz60ScRWcwAAAEBlUaQmUY8ePfTee+9p+vTpSkpKks/n06JFi2RZlo4//ng5HEXatQ4AAADAYezLTtFHaz4wYhe3HSiHVbTfty0rVZa134g5VvxjjH2dOktW7vZfp556uuLiYpSUlCqbxR0oI1u2mNvN8TwiAAAAoPIoUpNIkrp3767u3bvnjdu1aydJevfdd/OeWQQAAAAgdM8tflrJWclG7JK2lxX5/PDw8bKsYLfHti25Zy8z5viO7VKiHIHi2rvXbBI1bEhHEgAAAKgsitwkOlSjRo1kWRariAAAAIBSsG7PWr23/C0j1q/FeWpfL6GIV/AqIuJ1I5KTdpY8878xY736lCRNoNiysswmUXh4BSUCAAAAIJ+Qm0Rz584tzTwAAACAGsu2bT340z3yBXx5MY/Do1HdHy/yNcLCpsnp3GrEfHO6KMw+6JmikVHynhLcHeCnnxbkPZOoe/fTS/AOgMJlZprj8HBWEgEAAACVRchNooOlpKRoxowZWrhwobZv366MjAxFRESoUaNG6tKli/r376+GDRuWxq0AAACAamfOpq80b8t3Ruz6425Wi1oti3gFW5GRLxsRr7ebnJM2GLGcM3pKYWHBe1w/Qtu3b1N8fCP98ceaUFIHjoiVRAAAAEDlVeIm0ZQpU/TUU08pKytLUu63IA9Yu3at5s2bp9dff1133323Bg8eXNLbAQAAANVKtj9bD/10rxE7Kipet55wR5Gv4fHMkcu1yohlpN2kmLk3GrGcM88OPVEgRNnZ5jgigpVEAAAAQGVRoibR2LFj9eyzz+Y1hlq0aKFWrVopIiJC6enp2rBhgzZt2qSsrCyNHj1a2dnZuuqqq0ojbwAAAKBamP7XNP29b6MRe/CUUYp2Rxf5GhER5ioin6+tAj/VkyMlxYjn9Dkr5DyBUGzcaCk5mZVEAAAAQGUVcpNow4YNev7552Xbto499liNGjVK7dq1yzdv9erVGjVqlJYtW6bnnntOZ5xxhlq2LOq2GQAAAED19su2n4zxCQ1P0iVtLyvy+S7XEnk8PxqxzMxbFTZ7lhHzte+gQOMmoScKFNPOnZYGDozMt91cvXqsJAIAAAAqC0eoJ44fP15+v1/t27fXhAkTCmwQSVL79u01fvx4JSQkyO/36+OPPw45WQAAAKC6WZW80hhf2PpiWZZVyOz8IiJeMcZ+f0NlZV6qsFmfGfHsc88PPUmgmPbvlwYNitDmzeZHzj59fGrXLlBBWQEAAAA4VMhNot9++02WZenWW29V2EEPvy1IWFiYbrnlFtm2rZ9++umwcwEAAICawhfwae2e1Uasfb0ORT7f4diosDCzGZSZeYNcy1bKmbjFiGdfMCDkPIHiyMqShg2L0MqVTiPeubNfb7+dqWL0QAEAAACUsZCbRDt27JAkHXvssUWaf2De1q1bQ70lAAAAUK38vW+jsvxZRqw4TaLIyNdkWcFVGYFAtLKyhitsptk48rVqLX/7hJIlCxSB3y/dcEO4fvrJ3Nm8efOAJk3KVExMBSUGAAAAoEAhN4mcztxvhWVnZxdp/oF5xdk6AwAAAKjOViWvMMYNI49SXERckc61rGSFh39oxLKyhssO1FLYzBlGPPuCAWL5BsqabUv33humWbPcRrx+/YCmTMlQgwY8iwgAAACobEJuEjVr1kyStGDBgiLNPzCvadOmod4SAAAAqFZWH/I8ooRirCIKD58oy8rMG9u2S5mZ18u5Yrmcm/4x5uZc8H8lyhMoim++cer99z1GLDra1kcfZapFCxpEAAAAQGUUcpPojDPOkG3bevnll7V9+/bDzt2+fbtefvllWZalHj16hHpLAAAAoFr5Y/cyY1ycreZcrqXGODv7IgUCTeRe+KsR9x/dXL6OnUPOESiqcePMBpHHY2v8+Ex16hQo5AwAAAAAFS3kJtHQoUMVGxur5ORkXXzxxZo0aZKSk5ONOcnJyfrwww918cUXKykpSVFRURo2bFiJkwYAAACqOtu2tWzX70bsuPpdiny+ZaUbY58v95lDrrWrjbj3xJMK3Wruzz/XyLZt/fnnmiLfFyjIzp2W5s1zGrHRo7N1+un+CsoIAAAAQFG4jjylYPXq1dMzzzyjW265RXv27NGjjz6qRx99VLGxsYqMjFRGRob2798vKfcDsMvl0vPPP6+6deuWWvIAAABAVbU5dZOSs8wvWXVpeEKRz7esrEMi4ZIk51qz4eNv1z6k/IDimDbNpUAg2IyMjLQ1cKC3AjMCAAAAUBQhrySSpJ49e2r8+PHq0KGDbNuWbdvat2+ftm/frn379uXFEhISNGnSJLaaAwAAAP61dOcSY1wvvJ6OjmlW5PMPfh6RJNl2hGTb+VYS+Y6hSYSyZdvSxx+7jdgFF/gUHV1BCQEAAAAospBXEh3QpUsXffLJJ1q9erUWLlyoHTt2KC0tTZGRkWrUqJFOPPFEdehQ9L3VAQAAgJpg6aFbzTU4XlYh28IV5NCVRLYdLmv3bjn27DHivmPahZ4kUATLlzu0erW51dxll7GKCAAAAKgKStwkOqB9+/Zq355vKQIAAABFsWTnImPcpUHRt5rLdWiTKCLfKiI7PFyBZs0LvcKzzz4przdLbne47rzz3mLeHzWdbUtffOHSww+HGfGmTQPq3p1nEQEAAABVQak1iaqKxx57TBMnTjzivAcffFBDhgwxYl6vV5MnT9bnn3+uDRs2yLZtNW7cWGeeeaaGDx+u2rVrl1HWAAAAqE5WJ6/Swh2/GrEuDY4vxhVsORzJh8Qi5fznbyPia91WcjpVmIkTx2v79m2Kj29EkwjFsm6dQ/fdF6Yffsj/kfLSS71ylGhjcwAAAADlpcY1iVauXBnSednZ2br22mu1cOFCI75+/XqtX79en376qd577z21bdu2NNIEAABANfby788Z49phtdW98elFPt/h2CmH45Bt5XytFLZnuRELNGwYepJAAfbvl559NkzvveeWz5d/e8SoKFtXXslWcwAAAEBVUaOaRIFAQGvWrJEkPfzww/q///u/Qud6PB5jfO+992rhwoVyu9266aabdP7558vj8Wj+/Pl69tlntWvXLo0cOVKzZs1SZGRkmb4PAAAAVF0bUv7SjPWfGrERna9XlDuqyNdwOlcYY9uOUiDQIt/ziOw6dUNPFDhIICBNmeLSo4+GKSmp4GVC3br59PTT2Wrc2C7n7AAAAACEqkY1if7++29lZGRIkk444QRFRRXtg/jy5cs1e/ZsSdL999+vwYMH5x279NJL1aFDBw0cOFBbt27VhAkTNHLkyNJPHgAAANXCK7+/qIAdyBtHu2N0baf/FOsaLpe5Ot7nS5DkkGOv2SQK1KsXcp7AAb//7tB994Xr998L3rowPj6gRx7J1oABPln5FxcBAAAAqMRq1E7RB7aai4yMVOvWrYt83rhx4yRJTZo00cCBA/MdT0hI0IABAyRJU6dOLXmiAAAAqJY279+kqes+MmJXdxyhOuHFW/HjcpkriXy+jpIkay8riVB6kpOlW24JV79+UQU2iDweW7fdlq2ffkrXhRfSIAIAAACqohrVJFq1apWk3KaO8zAP8D2YbdtasGCBJKlXr16FntenTx9JUmJiYt6WdgAAAMDB3lj2inwBX944whWh/xx7Y7Gvk38lUQdJyrfdXIAmEUKUmSmddZY0ebK7wON9+/q0YEG67rsvR9HR5ZwcAAAAgFJTo7abO7CSqH379vr444/1+eefa/Xq1fJ6vWrcuLH69Omja665RnXq1Mk7JzExUfv375ckdejQodBrJyQk5L1esWKF2rVrV0bvAgAAAFXVF3/PMsZXJlyl+pH1i3kVr5zOtUbE789dSeTYudOI23VpEiE0Eya4tXRp/nirVgE99liW+vTxl39SAAAAAEpdjWkS2badt5Loo48+ktfrNY5v3LhRGzdu1CeffKI333xTxx13nCRp69ateXOaNGlS6PXr168vt9str9erxMTE0n8DAAAAqNJSsvZqR/p2I3ZF+2HFvo7TuVGWZf4u6/O1l9LS5Nz8jxH3t2hZ7OsDmZnSK694jFhUlK077sjWddd55fEUciIAAACAKqfUmkR//vmnfv/9d23fvl1paWl6/PHHJUlff/21OnfurPj4+NK6VUg2bdqktLQ0SZLP59Nll12mgQMHqnHjxtq9e7dmzpypsWPHas+ePbruuuv0ySefqGnTptq7d2/eNWJjYwu9vsPhUFRUlFJSUvJWHgEAAAAHrNu7zhi7HC61rt2m2NdxOlcbY7//KNl2HbnWLjLitsMhX1tWt6P4xo93a9cuc2fyjz7KVNeurB4CAAAAqpsSN4mWLl2qUaNGae1ac8uLA02iF198UVu3btW1116rW265RVYFPc10586dOuqoo7Rr1y49+eSTGjBgQN6xOnXq6I477lCnTp108803a9++fXr22Wf1yiuvKDs7O29eeHj4Ye8RFhYmScY5xWFZqvEPez3w/mt6HQ6gHkHUIohamKhHELUwUY8gahFUkbVYl2I+t7JVrdbyuAp+3svhuFyHNonay7Ik12rzOUX+Vq1lRRz+99fu3U9Vauo+xcTUqvE/H/x7kisjQ3r1VXOpUM+ePp1ySs1tEPGzEUQtTNQjiFqYqEcQtQiiFibqEUQtTNQjqLxqUKIm0Zw5c/Tf//5Xfr9ftm3L6XTK7/cbjaBt27bJ6/VqzJgxSk5O1ujRo0ucdCi6du2q+fPnKycnR55C9kc4++yz1atXL82bN0/ffPON9u3bJ6fTWW451q0bU273quzq1aMWB6MeQdQiiFqYqEcQtTBRjyBqEVQRtdiSudEYd4rvqLi4UPLYYIw8nk651/n7LyPu6nLcEa8/bdrHIdy/eqvp/5688IK0e7cZe+IJV4g/q9VLTf/ZOBi1MFGPIGphoh5B1CKIWpioRxC1MFGP8hNyk2jHjh2666675PP51LlzZ91+++065phj1L17d2Pe+++/r6efflp//PGHpk6dqnPOOUfdunUrceKhKqxBdECfPn00b948BQIBrVixQhEREXnHjrRC6MDxI604KsyePakKBEI6tdqwrNz/AUhOTpVtV3Q2FY96BFGLIGphoh5B1MJEPYKoRVBF1mLZtj+NcfPIVkpKSi32dWrXXi7XQb/Fp6a2UnZ2qmotWaqD1yWlt2yrzCNcn5+NIGoh7dkjPfVUlKTgVnO9e/vUpk2mkpIqLq+Kxs9GELUwUY8gamGiHkHUIohamKhHELUwUY8gh6N8FpaE3CQaO3assrKy1KlTJ33wwQfyeDzKyMjIN+/444/XhAkTNGzYMP3xxx/66KOPKrRJdCQHPztpz549atiwYd44NbXwD9mBQEDp6emScrevC4Vtq8b/4B9ALUzUI4haBFELE/UIohYm6hFELYLKuxY5/hwt2/m7EWtbp10IOfjldK43Ij5fe9kBW85VK8x4QsciX5+fjaCaWovsbGnYsAjt3m0+i+iuu7JrZD0KUlN/NgpCLUzUI4hamKhHELUIohYm6hFELUzUo/zev+PIUwq2YMECWZalm2+++Yirc8LCwnTTTTfJtm0tW7Ys1FuWCvsIlfV6vXmvIyIi1Lx587zxtm3bCj1v9+7deece3GgCAAAAvt00R3uz9xqx4xueWOzrOBy7ZFleI+b3t5Bj8yY5UlKMuK9Dx2JfHzWTbUu33hquX381v0N43nnSCSfU8K0OAAAAgGou5CbR9u3bJUmdOnUq0vyEhARJuatzKsIdd9yhrl276swzzzzsvPXrg9/MbNGihRo0aKDatWtLklatWlXoeStXBh8UfOC9AgAAAJI0Ze0kY3xKfHcdHdus2NdxOBKNsW27Zdv15frzDyMeqFtXgSZNj3i9Cy88Xx06dNCFF55f7FxQfTz9tEeffuo2YvHxAb31VgUlBAAAAKDchNwkcjqdksyVN4eTmZkpKfTn9ZRUbGysUlJSlJiYaDSCDmbbtmbPni1Jaty4sVq2bClJ6tGjhyTp+++/L3Ql0ty5cyVJ9evXV7t27Uo7fQAAAFRRyZnJ+nbT10Zs4DGDQ7qWw2GubA8EGktyyP3nMiPu63Rs7mbeR7Bhw3qtWrVKGzYU/Psxqr/Jk1164YUwIxYVZWvSpEw1blxBSQEAAAAoNyE3iZo2zf1m4sKFC4s0//vvvzfOK28XXHBB3uvHH3+8wGbPO++8o9WrV0uSrrnmGln/frC+8MILJUkbN27UpEmT8p23atUqzZgxQ5I0bNiwvPMAAACAGes/kTcQ/GJVuDNc/VsNCOlaTqe5ksjvbyRJch3aJDq2S0jXR83yww9O3XGH+SU+h8PWO+9kqmNHtpkDAAAAaoKQm0Q9e/aUbdt6+eWXtX///sPO3bx5s1599VVZlqXTTz891FuWyPHHH6/zz8/dRuPnn3/WsGHDtHDhQu3Zs0dr1qzRgw8+qOeff16SdPLJJ2vw4OC3O7t166bevXtLym0wvfjii9qyZYt2796tadOmafjw4fJ6vWrSpIlxHgAAAGo227Y1Zc2HRuycFucpNqxWSNcrcCWRbedrEnk7HxvS9VFzbNli6eqrI+TzmV9we/LJbJ15pr+CsgIAAABQ3lxHnlKwq666SpMnT1ZiYqIuvvhi3XjjjWrTpk3eca/Xq8TERH333Xd65513tG/fPkVFRWno0KGlkngoHn/8caWnp2vevHn67bff9Ntvv+Wb0717d7366qtyOMz+2VNPPaVrrrlGy5cv15gxYzRmzBjjeFxcnMaOHavo6OgyfQ8AAACoOl5a8pyW7V5qxELdak6SHI6txjgQaCzH5k1yJCcbcV/n40K+B2qGt9/2aP9+s0F0/fU5Gj68aNuJAwAAAKgeQm4S1a5dWy+//LJGjhypLVu26N5775WkvK3WOnfunDfXtm25XC4999xzqlevXglTDl14eLjefPNNzZkzR5988omWL1+u1NRU1apVS+3atdOFF16o8847r8Dt4mrVqqXJkydr8uTJmjlzpjZs2KCcnBw1btxYvXr10ogRIyr0vQEAAKBymblhhp5c+KgRi49qpB5Ne4d8TYdjnzEOBOrJ8+MPZqxuXQWaNQ/5HqgZfvvNaYz79fPq4YezKygbAAAAABUl5CaRJJ1yyin6+OOPNWrUKC1ZsqTQee3bt9dDDz2kLl0qfm90y7LUt29f9e3bt9jnut1uDR06tEJXQwEAAKDyW7brd9303X+MmCVLz/V4SS5H6L+CW1aGMbbtKLl/mGfEck7vKfGMTBxGdra0cqW5c8KIEV45Qt6MHAAAAEBVVaImkSS1bdtWH374odavX6/ffvtNiYmJSktLU3h4uBo1aqQTTzxRnTp1Ko1cAQAAgEpvW9pWXfnFIGX6Mo34I90f11nN+5Xw6unGyA5EyLNgvhHzntGzhPdAdbdypUNeb7CRaFm2jj2W5xABAAAANVGJm0QHtG7dWq1bty6tywEAAABVTro3XVd+MUg7M3YY8SHth2nksTeW+PqWZTaJrC3JciQlGbEcmkQ4gqVLza3mWrcOKDa2gpIBAAAAUKHYUAAAAAAoJbfOvUHLk/4wYqc2Ol1PnfF8gc+9LK5Dt5tzLf3LGPubNed5RDiiQ5tEXboEKigTAAAAABUt5JVEr732WrHmW5Yll8ulsLAw1a1bVy1btlRCQoIcbHwNAACAamBF0nJ9vmG6EWtZIZ4XXQAA3+RJREFUq5XG9psoj9NTCnfIlMORYkRcv5oNqZwevYt1xTvvvFuST6W4wQAquU8+cenTT83/vrt0Yas5AAAAoKYqUZOopN+GrFu3rv73v/9pwIABJboOAAAAUNF+37nYGMd6aunD8z5WnfC6pXL98PDxsizzOUeumYc2iXoV65pDhw5XXFyMkpJSZdslThGV3HvvuXXffWGybfN5RKedRpMIAAAAqKlCbhKdeuqpCgQC+vXXX2X/+4kyIiJCzZo1U2RkpDIzM7V582alpwf3TQ8PD5fP55PP55MkJScn695779W2bdt0ww03lPCtAAAAABVnS+pmY3x6kx5qVbtNKV09W5GRLxsR3/o2cm0JbjcXiI5RTp+zSul+qE5sW3r+eY+eeSYs37Grr/bqmGPYbg4AAACoqULe6+29995TTEyMbNtWy5Yt9dZbb2nJkiWaMWOGJk2apOnTp2vJkiUaP368OnbsKMuydO6552rFihX6888/9eGHH+rUU0+Vbdt6/fXXtWrVqtJ8XwAAAEC52rz/H2PcNOboUrt2ePhkOZ1bzeBo8w/7Oef3lyIjS+2eqB4CAemBB8IKbBD95z85evzx7ArICgAAAEBlEXKTaNq0aZozZ47atGmjjz/+WD169Cjw+UJdu3bVpEmT1LFjR02fPl2zZs2Sx+PRCSecoHfeeSdvRdKUKVNK9EYAAACAirT5kJVEzWKbldKVfYqMfMGIeFO6yDVxgxHLuuSyYl95x44dSkxM1I4dO0qUISonr1e66aZwvfNO/mdi3XNPtkaPzhaPiAUAAABqtpA/EkydOlWWZenOO+9UdHT0Yed6PB7997//lW3b+vDDD4M3dzh07bXXyrZtLVq0KNRUAAAAgAq3ef8mY1xaK4nCwqbK6fzHiPk/aGGO4xvJe+rpxb722Wf3VNOmTXX22T1LkCEqo0BAuuaacE2b5jbilmXr6aez9N//5qiEj5gFAAAAUA2E/EyidevWSZKOPfbYIs1v3769JGnt2rVGvFWrVpKknTt3hpoKAAAAUKEyfZnanbnLiDWNKZ2VRBERY4yxN+dEuZ/6zYhlX3Sp5HSWyv1QPUyb5tJXX5kNIpfL1uuvZ+nCC30VlBUAAACAyibklUTh4eGSpD179hRp/oF5h25JZ9u2JMnn44MKAAAAqqakzN35Yo2iG5XClQNyuZYbkezlF8i51Xw+UdbFA0vhXqhOvvjC/D5gRIStDz7IpEEEAAAAwBByk6hFi9wtLj766KMizZ86dapx3gGrV6+WJB111FGhpgIAAABUqLSctHyxaHdMia9rWUmyLPOP+s5PzG3tfG3ayt+hY4nvherDtqVFi8yVZY8+mq3evf0VlBEAAACAyirkJtEll1wi27Y1ceJEvf3224edO378eL3//vuyLEsXXHBBXjw1NVWvvPKKLMvSySefHGoqAAAAQIVKzUk1xlHuaDkdJd/+zencboxt2yHPR98asezz+4uHy+Bg//xjafdu86PeaaexgggAAABAfiE/k6h///6aPn26Fi1apBdffFFTpkxRr1691KJFC4WHhysjI0ObNm3S/PnzlZiYKNu21alTJ11xxRWSpC+++EIPP/yw0tLS5HQ6deWVV5bamwIAAADKU5p3vzGO8ZR8FZEkORzbjLGdXVfOvzcbsZzz/69U7oXqY+FCs0EZFxdQixZ2BWUDAAAAoDILuUnkcrn01ltv6a677tK3336rrVu36sMPP8w378Azh3r27KnnnntOzn8fqLto0SKlpqbK6XTq/vvvV9u2bUNNBQAAAKhQh243F+2OLpXrOhw7jLG9y/zjv79Zc/k6di6Ve6H6OLRJdPLJfhabAQAAAChQyE0iSYqMjNRrr72m3377TTNmzNCCBQuUlJSUd7x27do6+eSTddlll+nUU081zm3WrJluuukmnXPOOWrVqlVJ0gAAAAAq1KHbzZXVSiJrrdmMyj7//9hqDvkc+jyik0/mWUQAAAAAClaiJtEBXbt2VdeuXSVJOTk5SklJUUREhGJiCv9wfNVVV5XGrQEAAIAKtyp5hTGuHVanVK7rdK43xo616cY4+/z+pXIfVB/p6dLatebziE46iSYRAAAAgIKVSpPoYB6PRw0aNCjtywIAAACVkm3b+mbT10asa3y3Urm2y/WHGThoGIiKlu+440vlPqg+Vq1yyLaDq8scDlsdOwYqMCMAAAAAlVmpN4kKkpOTo/T0dG3atElz5szRXXfdVR63BQAAAMrcxn3r9c/+v43YWc36lvi6lrVfLpe5kki/B1/6jj9BcprbihXXp5/OVExMmFJTs0t0HVQey5ebPxNt2gQUEVFByQAAAACo9ErUJNq5c6deeeUVLViwQHv27JHfX7RtDGgSAQAAoLr4dtMcY9ww8ih1jOtc4uu6XMuNse2zZK2w88beE04q8T1at26juLgYJSWlyraPPB+V38qV5lZzrCICAAAAcDghN4n27dunQYMGaceOHbKL8YkyKioq1FsCAAAAlc6hTaI+R58ly7IKmV10LtcyM7DSknKCv3f7jj+xxPdA9XPoSqKOHXkeEQAAAIDChdwkmjhxorZv3y5JSkhI0EknnaTExER99913Oumkk3TCCSdo7969+u233/TPP//IsiwNHz5ct9xyS6klDwAAAFSk1Jz9+mXbT0bszFLYak6y5XbPMyLWYnNFiJcmEQ6RkSGtXm2uJOrUiZVEAAAAAAoXcpPohx9+kGVZ6tGjh958801ZlqV169bpu+++k9Pp1G233SYp90G+r776qt544w1NmzZNw4cPV3h4eGnlDwAAAFSYiavGKyeQkzd2OVzq0bRnia8bEfG6wsLMFUoHP4/If3Qz2Q0alPg+n3zysZxOW36/pYsuGlji66FivfSSR9nZ5iq2Dh1oEgEAAAAonOPIUwq2adMmSdLVV1+dt51GmzZtFBERoWXLlikQyP0wYlmWbrnlFvXs2VOpqamaNGlSKaQNAAAAVKxsf7bG/PGaETurWT/FeGJLdF23+1tFRT1gxOwMh/RxcJzTo3eJ7nHAqFEPacSIERo16qFSuR4qzrp1Dr3+useI9ejhU716PGwKAAAAQOFCbhKlp6dLklq0aJEXsyxLrVq1UnZ2tjZu3GjMHzJkiGzb1g8//BDqLQEAAIBKY+raj7QjfbsRu7nLbSW6ptP5l2Jjr5ZlHbL6Y0hASgoOc845t0T3QfVi29Ldd4fJ6w2uInK5bD32WHYFZgUAAACgKgi5SRQVFVVgvFmzZpKk9evXG/G2bdtKkrZs2RLqLQEAAIBKwR/w67WlLxmxUxudrhOPOjnka1rWPsXGDpLDkWLEs38+T9b04DgQFa2c03qEfB9UP1OnuvTTT+ZO4jfckKNjjmGrOQAAAACHF3KTKD4+XpL0999/G/GmTZvKtm399ddfRty2c7c5yMjICPWWAAAAQKXwxd8ztXHfBiN28/G3l+CKfsXEXC2Xy/wdOivrQukRrxHL6XOWxDM+8a+9e6VHHgkzYk2bBvTf/+YUcgYAAAAABIXcJDrxxBNl27befvtteb3BD66tW7eWJM2fP9+Yv3DhQkmFr0DC/7N33/FRVekfx793SmbSCCWAAUQEBERFig0b0uwFFbGsoq5lcXUt69rW3dXdtbCW39p1XUXFgopiWztVrEgRKYKCBUMLIZS0yZR7f38EMpw0kplJJuXzfr14Zc5zzzn35AExwzPnXAAAADQXDy/8t9HeP3uAhu85Mub5UlP/I5/vYyMWCg1Q4YZ7lfKJ+XN18ISTYr4PWhbHkf76V7/y8823dXfdFVBaWpIWBQAAAKBZiblIdM4558iyLH322Wc644wz9MEHH0iShg4dKrfbrWXLlunuu+/W6tWr9eGHH2rixImyLEv9+/dP2OIBAACAxra2MFffbFpkxK4edJ0sy6phxO75fC8ZbdvO1vbtU+RdtEJWWfS5Mo7Ho+CoY2O+D1qWSZO8evVVrxE74YSQjjsukqQVAQAAAGhuYi4S9e7dW1deeaUcx9GqVasqdg516NBBY8eOleM4mjx5sk4++WRde+212rx5syTp3HPPTczKAQAAgCRYsPFro93W11Yn9zotjhkdeTzm0XWFhQ/ItveUe80vRjyyT185WW3juBdais8/d+uvfzWPmUtPd3TnnWU1jAAAAACAqjy771Kzq666Sr1799bTTz+tPffcsyJ+6623atOmTZo5c2ZFzLIsXXbZZTr2WD75CAAAgOZrfqUi0eDOB8njiv3HasvaLMsqNmLh8IGSJFflIlH37jHfBy1Hbq6lSy/1Kxw2d689/HBA3bo5SVoVAAAAgOYoriKRJB1//PE6/vjj5TjRNyMpKSl67LHHtGjRIi1atEhut1tHHHFExfOKAAAAgOaq8k6iIZ0Pjms+t/tno+04btl21/Jrub8a1yJ7UiRq7UpLpYsuSq3yHKI//rFMJ58cTtKqAAAAADRXcReJdqruDPZBgwZp0KBBFe1169Zp/fr1GjJkSKJuCwAAADSaYCSoJZsWG7EhnQ+Ka06329wtZNt7aueP6a5f15jX9twrrntV1qlTZ7lclrKzOyV0XjQMx5Guv96vb791G/HRo8O68cZgklYFAAAAoDmLuUjUr18/uVwuLViwQKmpqbvtX1BQoBEjRqhTp0765JNPYr0tAAAAkDTLNy9VIBIwYoM6xfcBKJfLLARFItFCUJVnEiV4J9H06XOUnZ2p/PxCOZxS1uQ9/7xXr73mNWK9etl67LFSuWJ+2iwAAACA1iyutxJOPd5JFhUVSZK2bt0azy0BAACApFmav8Ro92rbW+387eOa0+2uvkjke/lFudfmGtfsXZ4DitbFtqUHHkgxYhkZjp57rlRZWUlaFAAAAIBmb7c7iWzb1t13311R5Knstttuk9vtrvbaTqFQSAsWLJAkZWdnx7BMAAAAIPkKAgVGu0ebveOe0+XaYLRtu6vc3y1X5k1/NOMdOym8735x3w/N05dfupWba37G7+GHA+rTx07SigAAAAC0BLstErlcLu2111664447qjx3yHEcvfPOO3W60c5dR2eeeWYMywQAAACSryhYaLTbpLSJe06Xa6PRtkvbqs2l42WVlpr3vvNfUoq5kwStx2uvmW/d+vWL6MQTw0laDQAAAICWok7PJPrNb36jRYsWKS8vryL29ddfy7IsDR48WK5aDsC2LEsej0dt27bVoYceqnHjxsW/agAAACAJtge3Ge2MhBSJ8oy27+m35fnheyNWevGlKhuT+A9bXX/9NSopKVRaWqbuu+/BhM+PxAgEpLffNp9FNHZsWJU+wwcAAAAA9VanIpFlWbr//vuNWL9+/SRJTz31lFJTUxO/MgAAAKCJKay0kygzJTPOGZ0qO4lSpn1mtEMDBqroH3fHeZ/qffzxh1q/fp1ycro0yPxIjI8/9mj79mhFyLIcnXlmKIkrAgAAANBS1KlIVJ0xY8ZU7BICAAAAWoPC4HajHe9xc5a1TZZVZgZ3eUSRndlG2//7rOTzxXUfNF+OI73wgrmL6IgjIura1UnSigAAAAC0JDFXeCZOnJjIdQAAAABN3vZKRaJ4dxK5XOuqBnfZWFT44GOy9+4Z1z3QfNm2dNNNPs2aZb5tGzuWXUQAAAAAEoNtQAAAAEAdbQlsMdpZvrZxzefxLDIDayXt2FhUetElCp58alzzo/mKRKRrr/XrlVfMXUSZmY5OPjmcpFUBAAAAaGniKhKVlpbqmWee0QcffKDc3FwFAgE5Tu3HHliWpeXLl8dzWwAAACApCgKbjXYHf4e45vN655uBL6Mvy04ZE9fcaL5CIenKK/16802zQOR2O3rggYDaxHfKIQAAAABUiLlIVFZWpt/85jf67rvvJGm3xSEAAACgOXMcp0qRqH2cRSKP52szsEuRKLzvfnHNjeaprEy6/HK/3n/fLBB5vY6eeCKgU05hFxEAAACAxIm5SDR58uSKHUE5OTk6/PDD1aFDB6WkpCRscQAAAEBTURQqVMg2nwXTPjWeIlGxPJ6lZmhHkSjSqbOc7Ow45kZzVFoqXXxxqmbONN+m+XyOJk0q1ejRkSStDAAAAEBLFXOR6L333pNlWTr88MP1+OOPUxwCAABAi1YQKKgSi+e4Oa93kSzLjgbCkhaUv4z0ZxdRaxMOSxdckKpPPjHfoqWmOpo8uVTDhlEgAgAAAJB4MReJfv75Z0nSNddcQ4EIAAAALV5BqXnUXIorRenejJjn83gWmIHFkkrLX4b77x/zvGiePvrIU6VAlJ7uaMqUUh12GAUiAAAAAA0j5iKRx1M+tEePHolaCwAAANBkBSIBo53mTZNlWTHP5/EsMwPzoy/D+/aPed76OOOMsQoEiuX3pzfK/VCztWvNP0uZmY5efbVEQ4bYNYwAAAAAgPjFXCTq2bOnvv32W61du1Zt2rRJ5JoAAACAJsdxHKNtKfYCkSS53SvNwC6PJ2qsnUS3336HsrMzlZ9fqErfHhpZpNJmocGDIxSIAAAAADQ4V6wDx4wZI8dx9PzzzydyPQAAAECT5KhSkSiOXUSSLY/nezP03Y77uN2K9Okbx9xojioXidzu5KwDAAAAQOsSc5HonHPO0WGHHaY33nhD9957r7Zt25bIdTW6kpISHXfccerbt68efvjhGvuFQiFNnjxZY8eO1aBBgzRw4ECddNJJ+ve//62tW7c23oIBAACQVPHsJHK5cmVZxWZwefmXyD59JJ8vjpWhOYpEzD9PrpjfqQEAAABA3cV83Nyjjz6qAw88UIsXL9akSZP07LPPas8991T79u0rnldUHcuy9Nxzz8V62wYzceJE/fzzz7X2KSsr06WXXqp58+YZ8VWrVmnVqlWaNm2ann76afXp06cBVwoAAIBkSOROIo9nhRnYKml9+ctw//1inhfNl13pZDl2EgEAAABoDDEXiR555BFZllVxNnskEtHPP/9cY6FlZ9/4juVoGLNnz9Yrr7yy23633HKL5s2bJ6/Xq6uuukonn3yyUlJSNGfOHN17773Ky8vThAkT9L///U9paWmNsHIAAAA0lohd6TywOHYSVXke0fLoy/C+jVckGjp0iDZu3KDOnffQ558vaLT7oqoNGyrvJOIhUQAAAAAaXsxFooMPPjiR60iagoIC3Xrrrbvtt2TJEr377ruSpFtvvVXnnntuxbWzzjpL++23n8aNG6e1a9dq8uTJmjBhQoOtGQAAAI3vs7VzjXaWLyvmuVyudWZgVfRleMDAmOetr+LiYhUWFiojI7PR7omqFi506bnnvEasWzeKRAAAAAAaXsxFoueffz6R60iav/zlL8rPz9cZZ5yhadOm1djvmWeekSR169ZN48aNq3K9f//+GjNmjKZOnaqpU6dSJAIAAGhBbMfW6z+8asRG73V8zPO5XAVmYFP5l3CfvgodNSzmedH8lJRIV16ZajyTyO12dO65oSSuCgAAAEBr0aofhzp16lTNmDFDXbt2rXU3keM4mju3/JOjw4cPl7uGA8JHjhwpScrNzdWKFSuq7QMAAIDmZ976L/Vr4Rojdlafs2Oez7IqFYl2NIsm3i/V8nxPtDz/+IdPq1ebb8uuvz6o/fe3axgBAAAAAImT0CLRtm3btGLFCs2fP78iVlJSkshbJMyaNWt01113ybIs3X333crIyKixb25urrZv3y5J2m+/ms+I79+/f8XrpUuXJm6xAAAASKqp379stPu266f9swfEPJ+75FczsFkKnDFWoSOPjnlOND+zZrk1aVKKERs8OKJrrw0maUUAAAAAWpu4P6YYDAb10ksvaerUqfrxxx8lSZZlafny8qfvXnjhhWrbtq1uvPFG7bPPPvHeLiEikYhuvPFGlZSU6MILL9Shhx5aa/+1a9dWvO7WrVuN/Tp27Civ16tQKKTc3NyErRcAAADJEwgH9NaqN4zYWX3PkWVZNYzYjXBY7qJVUrtoyC7xqfj2O+NYJZqbLVuka67xG7HUVEePPlrKZjIAAAAAjSautx95eXmaMGGCvvvuOzlO9Q9WXbNmjZYuXaqvv/5aDz30kI4+OvmfjvzPf/6jRYsWqVevXrr++ut323/Lli0Vr9u0aVNjP5fLpfT0dG3durVi51F9WFb5r9Zs5/ff2vOwE/mIIhdR5MJEPqLIhYl8RJGLqFhy8eHP72l7cJsRO7PPWTHn0zf7aVmnmjtFykb/Rk5OjpL5W9Ta/3w09n8nDz7o04YN5sEOt91Wpt69q39f1dj4eyOKXESRCxP5iCIXJvIRRS6iyIWJfESRCxP5iGqsHMRcJAqHw5owYYKWL18ut9utE088UQceeKDuuOMOo9+xxx6rN998U4FAQH/84x/17rvvqnPnznEvPFZLly7VY489Jo/Ho3vuuUc+n2+3Y8rKyipe+/3+WnqqYr5dx9RV+/aZ9R7TUnXoQC52RT6iyEUUuTCRjyhyYSIfUeQiqq65cBxHT0x72IgN22uYBu7dv4YRu7NBGnmHlG5GU8+5Wqnexv/9cbmsiq/Z2fz5kBrvv5MZM8z2scdKN97ol2XV/n6jsfH3RhS5iCIXJvIRRS5M5COKXESRCxP5iCIXJvLReGIuEr322mtavny5MjMzNWnSJB1wwAEqKSmpUiT65z//qbFjx+ryyy/X9u3b9fzzz+tPf/pT3AuPRSAQ0A033KBQKKQ//OEP2n///es0zu12N/DKyhUUFMpu5c+ntazyvwA2by5UDZvTWhXyEUUuosiFiXxEkQsT+YgiF1H1zcXsX2dqwfoFRuw3fS9Ufn5hDPfeoqysE+XpZO5KiqzeU1uyekiq/5zxsm2n4mss31NL0pj/nWzZIq1cab7pvf76Ym3e3HTeDPD3RhS5iCIXJvIRRS5M5COKXESRCxP5iCIXJvIR5XI1zsaSmItE7777rizL0pVXXqkDDjig1r4HHnigrrrqKt15552aM2dO0opE99xzj3788UcdcMABmjBhQp3HpaamVrze3Q6hndd3t+OoOo6jVv8HfydyYSIfUeQiilyYyEcUuTCRjyhyEVXXXDy44P+Mdo82e+vknmNiyGOxsrLOksezzAwvkUrmXinnLFf1wxoRfzbKNcZ/J/Pnmx9C8/sd7bef3SR/D/h7I4pcRJELE/mIIhcm8hFFLqLIhYl8RJELE/lovO8/5iLR999/L0kaNWpUnfofc8wxuvPOO5WbmxvrLeMyd+5cvfjii/L5fPrXv/4lTz2eBrvrc4gKC2v+hKVt2youLpYktWvXrsZ+AAAAaPoWbPxan679xIhdOegaeVz1/RG6TFlZ58nrnWeGV0k6Voo8sE9c64zHfff9W16vpVColb/7amSVi0QHHhhRSkqSFgMAAACgVYu5SFRaWirJLKDUJiMjQ1J5ISUZ3n33XUnlO31OPPHEWvs+8sgjeuSRRyRJM2bMUI8ePSqurVu3TkOGDKl23KZNmxQKhSRJOTk5CVg1AAAAkuWhhf822p3SOuvsvufVc5aw2rS5VCkps8zwWkmjJG2Q7C7d4llmXI499gRlZ2cqP5+jHBpT5SLRQQc1nWPmAAAAALQuMZ9r0aFDB0nS6tWr69R/+fLlkqTs7OxYb5k0nTp1Utu2bSVFv4/qLFsWPT6kf/9YH2YMAACAZFu15Qe9/9P/jNiEA6+S31O/I4VTU5+Qz/eWGcyXNFrSL+VNu0uX2BeKZicUkhYurFwkiiRpNQAAAABau5iLRDt30zz77LO77Wvbth5//HFZlqXBgwfHesu4/OMf/9DChQtr/bXT7373u4pY165dJUnDhg2TJM2ePVtODR+znDlzpiSpY8eO6tevXwN/RwAAAGgor33/stHO8rXVRfv9tt7zpKTMNNpOKE06XtJ3O9pp6XLaZMW6TDRD06Z5VFRkGTGKRAAAAACSJeYi0fnnny/HcfTRRx/prrvuUiAQqLbfxo0b9Yc//EHz58+XJJ199tmx3jIuKSkpSk9Pr/XXTl6vtyJmWeVv4E4//XRJ0o8//qiXXnqpyvzLly/Xm2++KUm68MILK8YBAACgeXEcR2+tfsOIndPvN8pIyaz3XJa11WgHvh0nLYi27R271ZNl8eJF+uKLL7R48aKkrqO1sG3p4YfNhw8dfHBEnTtz1h8AAACA5Ij5mUQDBw7URRddpGeffVbPP/+8pk6dql69elVcv/7667V27VotXbpUkUj5J+PGjh2rgw46KP5VJ8HQoUM1YsQIzZw5U3feeafy8vI0duxY+f1+zZkzR/fee69CoZC6deumc889N9nLBQAAQIyWbl6i1VtXGbHTe58Z01yWtd1oOwXmcXVOHZ/v2VAuuOBcrV+/Tjk5XbR48YqkrqU1eP99j77/3jxq7uqry5K0GgAAAACIo0gkSTfddJP8fr+efPJJlZaWaunSpRU7aN577z1Jqjia7bzzztOf//znOJebXBMnTtQll1yiJUuW6IknntATTzxhXM/OztakSZOUkZGRpBUCAAAgXm/9MM1od8/cS4M6DYlpLssqNAPbzB0jTmZyi0RoPI4jPfSQuYto330jGj2ao+YAAAAAJE9cRSLLsnTttdfq9NNP1yuvvKJ58+bp119/VXFxsfx+v3JycnTwwQdr3LhxLeIZPVlZWZoyZYqmTJmid955R6tXr1YwGFTXrl01fPhwXXbZZerQoUOylwkAAIAYOY6jN1ebRaJTe58e81HCLpe5k0hbzIKAk1n/I+zQPH3yiVuLFlXeRRSUK+YDwAEAAAAgfnEViXbaa6+9dOONNyZiqqRauXLlbvt4vV6NHz9e48ePb4QVAQAAoDEt3rRIa7b/bMTG9D4jxtnCsqxiM7Q5bDTtJB83h8ZT+VlEe+1l67TTwjX0BgAAAIDGkZDPrS1YsEBz586tEv/qq690++23a/78+Ym4DQAAANCgZv8602jvndVTB2QfGNNclrW1Ssz31gyj7bRtF9PcaF62bpXmzjV3EV11VVCehHxkDwAAAABiF1eRKC8vT+eff77OP/98TZ48ucr1JUuW6OWXX9YFF1yg3//+9youLq5mFgAAAKBp+CZvkdEe0X1UzEfNud1rjLYTseT++lcjFhw2Iqa50bzMn++W40T/HPn9js4+O5TEFQEAAABAuZiLRIFAQBdddJEWLFggx3G0cePGKn3atGmjDh06yHEczZo1SxMmTJDjONXMBgAAACTft5u+MdoHdhwU81wul1kk0q+OtMsjiUIHH6rgCSfFPD+aj3nzzF1EAwdG5PcnaTEAAAAAsIuYi0STJ0/Wjz/+KLfbrZtvvlmvvfZalT7jxo3TZ599pj//+c9yu92aP39+tf0AAACAZMsvzVdukbnTJ54iUeWdRNZP0deOy6XCf/2f5ErI6c9o4ioXiQ4+OFJDTwAAAABoXDG/K/3www9lWZauvPJKXXTRRUpJSamx7/jx43XJJZfIcRy9+eabsd4SAAAAaDDfbjKPmkvzpGmfdn1ins/t/sUM/Bx9WXrp7xTZ/4CY50bzEQpJixaZRaJDDqFIBAAAAKBpiLlI9NNP5R+FPPnkk+vU/9RTT5UkrVy5MtZbAgAAAA1mcd43Rnu/7APkcXlins/lqr5IFOnUWSU3/jnmedG8TJ3qUWmp+VwrdhIBAAAAaCpifte789lC6enpderfoUMHSVIoxANaAQAA0PQszJtvtA/sODCu+dzuVWbg5/IvxbffIadNVlxzJ8rnn3+t9u0zVFBQlOyltDiFhdItt/j16qteI77PPhG1b5+kRQEAAABAJTHvJOrSpYskacmSJXXqv2LFCklSdnZ2rLcEAAAAGkTEjujL9V8YsSGdD455PsvaLI/nRzO4TIrkdFHZmeNinjfRMjIy1aZNG2VkZCZ7KS3Kl1+6NXx4epUCkSQdeyy7iAAAAAA0HTEXiYYOHSrHcfTggw+qrKys1r7hcFgPP/ywLMvSoYceGustAQAAgAaxfPNSbSvbasQO73JkzPN5vV+ZgWJJiyUnPV2yrGrHoPkLhaS7707RmDGpWrOm6lutMWNCuvHG2t87AQAAAEBjirlIdO6558rj8ei7777TBRdcoPnz51fb79tvv9XFF1+sBQsWyLIsjR8/PubFAgAAAA3h83WfGu29s3oqJ6NLzPNVKRJ9JSksyZsS85xo2lavtnTSSWn69799sm2zEJiZ6ejRR0v1n/8ElJqapAUCAAAAQDVifiZRr169dOONN+quu+7SkiVLdMEFFygjI0Pdu3dXamqqSktLlZubq+3bt1eMufbaa9WvX7+ELBwAAABIlM8qFYni2UUkSV7vl2bg8/Ivjrfq8WPJ9PjjjygSKZPb7dOECVcleznNkuNIkyd7ddttPpWUVN0ldthhYT3ySEDduztJWB0AAAAA1C7mIpEkjR8/Xu3bt9fdd9+tzZs3q7CwUMuWLavSLysrSzfddJPOOOOMeG4HAAAAJJzt2Ppq3edGLL4iUVAez0IztHN6b1w/fifc448/ovXr1yknpwtFohjdeKNPzz1XdYeYx+Po5puDuvLKoNzuJCwMAAAAAOog7nepJ598so499lh98cUX+uKLL7Rx40Zt27ZNqampysnJ0eDBgzVy5Ej5fL5ErBcAAABIqO+3rNSWsi1GLJ4ikcezWJZV6bkzX+y82LR2EiE+v/5qVVsg6t07oscfD+jAA+0krAoAAAAA6i7mItGXX36pvffeW507d1ZKSoqGDRumYcOGJXJtAAAAQINbWfCd0c5J76Kumd1ins/rnWcGlkvaWv7SbtMm5nnR9Hz2WdUtQhdfHNRtt5UpLS0JCwIAAACAenLFOvDOO+/UiBEj9OabbyZwOQAAAEDjWlmwwmj3bR/fMzQ9nq/NwBfRl3bHTnHNjabliy/Mz9ydeGJI//oXBSIAAAAAzUfMRaLc3FzZtq1BgwYlcj0AAABAo/p+y0qj3bddfEWiKjuJdikSOdkd45obTcsXX5g7iY44IpKklQAAAABAbGIuEnm95eep86whAAAANGffbzF3EvWJYyeRy7VObneuGTR2ElEkainWr7f088/m26nDDqNIBAAAAKB5iblIdPzxx8txHD311FOJXA8AAADQaMJ2WKu3rjJi+7TrG/N8lY+ac7a7pF0eeWSzk6jFqLyLKCvLUf/+dpJWAwAAAACx8ey+S/X+/Oc/a9OmTXrxxRe1YsUKHX/88dp3333Vvn373e4u6tKlS6y3BQAAABLmu4LlCtkhI9anXZ+Y5/N6F5iB+S7JiRYOeCZRy/HBB+ZbqUMPjcjtrqEzAAAAADRRMReJdu4kchxHCxYs0IIFC3Y/SJJlWVq+fHmstwUAAAAS5tWVU4x214xuau/vEPN8lpVvBpY5RtPJzIx5bjQdq1ZZevtt863UkUeGk7QaAAAAAIhdzEWiDRs2VLx2HKeWngAAAEDTE4wE9drKl43YGfucFdeclmXuSlKg0s/Jnph//G4QAwYcqL326q6srHbJXkqz8sADPtm2VdHOyHB09tmhWkYAAAAAQNMU87vUu+++O5HrAAAAABrVhz+/r82BzUbsvH3Pj3PWSoWCMvMZNY67aRWJXnjhFWVnZyo/v1B87qtufvrJ0uuvm7+Pl1wSVDvqbAAAAACaoZjfpZ5++umJXAcAAADQqKZ897zRPjRnqHq13SeuOSvvJLKClTrw0Jpm76GHUhSJRHcRpaU5mjCBXUQAAAAAmidXshcAAAAANLb1Res089fpRuw3+45PwMyhWpvyUCRqztassfTKK14jdvHFIXXowDYsAAAAAM1TQs67CAaDmjFjhhYsWKANGzaouLhYzzzzjCTphRde0P7776+BAwcm4lYAAABA3F5d+bJsJ3oUXLo3Q6f0GhP3vFWeSVRpJ5HjokjUnL30klfhcHQXUWqqoyuuqLxdDAAAAACaj7iLRO+9957uuusubd5cfp674ziyrOgbp+eee065ubk65ZRT9M9//lM+ny/eWwIAAABxeeOH14326b3PVLo3Pe55LWuTGSis1MHVtDbyn3/+2dq2bYuystrp+edfSfZymrwffjB//846K6ROndhFBAAAAKD5iqtI9MILL+jOO++Us+Mpt9nZ2crPzzf6bNq0SY7j6J133lFRUZEee+yxeG4JAAAAxGVl/kot27zEiJ3ZZ1wCZnbk8aw2Q6sqdWlizyT69tvFWr9+nXJyuiR7Kc1CJGK2c3IoEAEAAABo3mL+KOPq1at19913y3EcjRw5Uh999JE+/PDDKv3ee+89jRo1So7jaNasWfroo4/iWjAAAAAQj1eWmTtmOqV11mE5h8c9r8u1QZZVbAa/r9Ip7vsgeWzbbPPbCQAAAKC5i/ltzbPPPqtIJKIjjjhCjz76qLp3724cM7dTly5d9PDDD+voo4+W4ziaNm1aXAsGAAAA4lG5SHRqrzFyJ+BZQW63uW3Isf3Sukqdqvl5Gc2HbZu/f01sYxgAAAAA1FvMRaIvvvhClmVpwoQJu+1rWZYuv/xySdKSJUt20xsAAABoGN9tXq7lm5YbsdN6n5mQud1u86i5SGgvqfJpZGw9adYqHzdHzQ8AAABAcxfzu9S8vDxJUp8+ferUv1evXpKkbdu2xXpLAAAAIC5vrTJ3tXfN6KaD9zgkIXO73SuNdqRs7yp9HIsiUXO2fXvlnUQ8kwgAAABA8xbzu1S/3y9JKikpqVP/7du3S5LS09NjvSUAAAAQlw9+et9on9JrjFwJKdwE5Pe/akQigapFIs4na77eecejr782f/+ysykSAQAAAGjeYn5HvPfe5W96P/nkkzr1f//9941xAAAAQGNaW5irZZvNo49P7HlKQub2+1+Vy7XJiIU2Hl2ln8MHppqlTZss3Xijz4hlZTkaPTqcpBUBAAAAQGLEXCQaNWqUHMfRQw89pNzc3Fr7Llq0SP/5z39kWZZGjBgR6y0BAACAmE1f85HRbutrq4M6H5yAmR2lpj5iRILBYYps2tPs5XZLPrPQgKbPcaQbb/Rp82bzrdNddwXUrl2SFgUAAAAACeKJdeD555+vl156SRs2bNCZZ56pCy64QP369au4/vPPPys3N1czZszQa6+9plAopOzsbJ133nkJWTgAAABQHx///IHRHtF9tDyumH8cruD1TpfHs8KIlZZeJau42Ig5aemSZT7TBk3f66979O67XiN2wgkhjR3LLiIAAAAAzV/M74pTU1P1xBNP6OKLL1ZBQYEeffRRSZK1443vCSecUNHXcRxlZGTo0UcfVUZGRpxLBgAAAOqnNFyquWvnGLHRex2XkLnT0sxdROFwXwWDo+UtmWHEm+JRc1dccZUikTK53exwqs6GDZZuucVvxDp0sHXvvWXU+wAAAAC0CHF9dLJv37566623dO+99+q9995TOFz103Qul0sjRozQTTfdpO7du8dzOwAAACAmc3NnqzRcWtF2WS6N6D4q7nk9nm+UkjLLiJWWXiXJJau4xIg31SJRdnam8vML5TjJXk3Tc889Kdq2zaoUK1OnTiQLAAAAQMsQ9/kaHTt21D333KO//OUvWrRokX799VcVFRXJ7/erS5cuGjJkiDp06JCItQIAAAAx+e+3Txjtg/c4VO387eOeNz39dqNt2x0VCJwtSXJt22pcczIy474fGtcnn5hvl844I6RTTuGYOQAAAAAtR/yHsO/Qpk0bDRs2LFHTAQAAAAnx7aZvNCfX3O0zts+4uOf1emcpJWWmESsp+b2k8uPJXOvWGtfsPfaI+55oXCXmZjCdd14oOQsBAAAAgAYSU5GooKBA69atk9vtVvfu3ZXeBI/OAAAAACTp0UUPGu1O6Z10dr/z4pzVVnr6bUYkEtlDpaVXVLSrFIm6dI3znolXVFSolBRHRUVFSk9np1Nl4bB51JzXm6SFAAAAAEADqVeR6PPPP9dDDz2kxYsXV8TcbreOPPJIXXvtterXr1/CFwgAAADE6udtP+mt1W8YsasPuVqpntS4nsHj802T1/uNESsp+bOktIq2e22ucT3StVvsN2wghx9+sNavX6ecnC5avHhFspfT5FR+5KrHw7OIAAAAALQsrrp2fPbZZ3XJJZdo8eLFchyn4lc4HNacOXM0btw4TZ8+vSHXCgAAANTL44sflu3YFe10b4Z+f/Dv45w1qPT0fxiRcLiPAoHzjVhz2EmE2lUtEiVnHQAAAADQUOr0NmfVqlW655575DiOLMvSYYcdpn333VeWZWnJkiX6+uuvFQwGdcMNN+jjjz9WdnZ2Q68bAAAAqNWmkk2a8t0LRmx8/4vULrWd8osLY57X739ObvfPRqy4+HYZP1o7jtxrKRI1Z6FQ+a9dUSQCAAAA0NLU6W3Oyy+/LNu21b59ez366KMaNGiQcf2zzz7T73//ewUCAU2dOlVXXHFFDTMBAAAAjeOxbx5SIBKoaHtdXk0YeGXc8/r9rxjtUOhQBYMnGTGruEhWSbERi+R0ifveaByOI918s0+RiPlMIr8/SQsCAAAAgAZSp+PmFixYIMuydP3111cpEEnSEUccoYsuukiO42jevHkJXyQAAABQH/ml+Xpm6X+N2Ng+Z6tLRny7eSxrizye+UaspOR6SWYxQcUlVcY6mW3iujcaz3//69Xzz6cYse7dbfXoYdcwAgAAAACapzoVidatWyepvBhUk1GjRkkqP5oOAAAASKbHv3lYJeFoocZtuXXNkOvjntfr/USWFS0UOI5fweCwKv2sQGmVmMM2lGZh+nS3/vY3nxFLTXX01FOl8nqTtCgAAAAAaCB1KhKVlJS/wc7IyKixT9eu5Z/KLCyM/Xx3AAAAIF6bSzfr6SVPGrEz+4xTz6xecc+dkjLDaIdCh0tKrdLPCgSqxJRatR+almXLpMsuS5VtmzvDHn44oIED2UUEAAAAoOWpU5EotOOJrW63u8Y+/h2fjCwrK0vAsgAAAIDYPLH4EZWEo88Dclku/XHIDQmY2alSJAoGR1bbs/JOIiclRarlZ2kkX36+pVNOkYqKzALRTTeV6dRTw0laFQAAAAA0rDoViQAAAIDmYGtgi55a8h8jdsY+Z6ln295xz+12r5Lb/asRq6lIpFJzJ5HjZxdRU3f11X799JMZO/30kP74x2ByFgQAAAAAjYAiEQAAAFqMj3/5UMWhoop2+S6iGxMyt8czz2hHIl0UiexbbV9X0XajzfOImrZffrH08cceIzZ4cEQPPBCQZdUwCAAAAABaAM/uu7Q8H330kV599VUtWbJExcXFys7O1qBBgzRu3DgNHTq0xnGhUEhTpkzR22+/rdWrV8txHHXt2lWjRo3SxRdfrLZt2zbeNwEAAIAqvslbaLRHdh+t3u32ScjcLtdmox2J9JNUfQXBveoHo23veH5nU/P881OUluZVSUko2UtJqiVLzKMAMzMdPfdcKY+RAgAAANDi1atIZDXzj9GFQiHdcMMNev/99434+vXrtX79er333ns6++yz9fe//73K91pWVqZLL71U8+aZnyBdtWqVVq1apWnTpunpp59Wnz59Gvz7AAAAQPUWb/rGaA/pfHDC5rasbUbbtrNq7OteucJoR/pWv+Mo2Q48cJCyszOVn18ox0n2apLnu+/MAxYGDoyoc+dWnBAAAAAArUa9ikSXXnqpXK7qT6izbbvi9fjx42ucw7IsPffcc/W5bcLcf//9FQWi448/Xr/97W/VrVs3rV27Vk8//bQ++OADvfLKK8rJydEVV1xhjL3llls0b948eb1eXXXVVTr55JOVkpKiOXPm6N5771VeXp4mTJig//3vf0pLS0vGtwcAANCqReyIluYvMWIDOh6YsPktq9IRck7NRSJPpSJRuIkWiVBu+XLzPU7//nYNPQEAAACgZalXkWjBggW1Xt+5++brr7+u9rrjOEnbjbRx40a98MILkqSTTjpJ//d//1dxrUOHDnrwwQd1xRVXaObMmZo0aZJ++9vfyufzSZKWLFmid999V5J066236txzz60Ye9ZZZ2m//fbTuHHjtHbtWk2ePFkTJkxoxO8MAAAAkrR66yqVhIuN2ICOgxI2v8tl7iRynDbVd3ScqjuJ+vVL2DqQeMuXm8fN9e8fSdJKAAAAAKBx1alI1KVLl4ZeR4ObNWuWQqHys9avvPLKavuceuqpmjlzprZv366ffvpJ/Xa8mX/mmWckSd26ddO4ceOqjOvfv7/GjBmjqVOnaurUqRSJAAAAkmDxpkVGOye9izqldUrY/FV3ElVfJHKtXydXodm3qe4k+uij9+X1WgqFHI0efUKyl5MUxcXSzz+bH2RjJxEAAACA1qJORaKZM2c29Doa3DnnnKNjjjlGP/30k3r16rXb/h5PeWocx9HcuXMlScOHD5fb7a62/8iRIzV16lTl5uZqxYoVFQUmAAAANI7lm5cZ7UQeNSdJLtcWo23b1ReJPIsWmv3SM2R37ZbQtSTKn/50ndavX6ecnC5avLh1FolWrHDJcaJFIpfLUZ8+FIkAAAAAtA7VP2Cohdpjjz00dOjQaq+FQiG99NJLkqSuXbuqR48ekqTc3Fxt317+SdD99tuvxrn79+9f8Xrp0qUJWjEAAADqqjBYaLS7ZiSyMOPI7f7OiNh29fP73nnTaIcPHCgl6chl7N7ixeaHwHr3tpWamqTFAAAAAEAjq9cziVqakpIS5eXlaeHChXr22We1cuVKeb1e3X777RU7idauXVvRv1u3mv+hoWPHjvJ6vQqFQsrNzW3wtQMAAMAUccJG2+vyJmxul+sXuVxbjVg4XM1OpdJSpXz4vhEqO+W0hK0Difftt+bn5g48kF1EAAAAAFqPVl0kuuyyyzR//vyKdk5Ojh544AENHDiwIrZlS/RYkTZtang4sSSXy6X09HRt3bq1YudRfVkWHzLd+f239jzsRD6iyEUUuTCRjyhyYSIfUa0lF+FKRSKPy1Ple441F17vYqNt2+3lON2qzJMya7pcxUUVbceyFDxlTLPIfXNYY0OovJPowAMjrTYXu2otf2/UBbmIIhcm8hFFLkzkI4pcRJELE/mIIhcm8hHVWDlo1UWidevWGe3169fr9ttv11/+8hcddNBBkqSysrKK636/v9b5fD5flTH10b59ZkzjWqIOHcjFrshHFLmIIhcm8hFFLkzkI6ql58LjNX+CzkxPU3Z29d9z/XNhHjXncg1RdnY1HyD64B2jaQ0bpg779a7nvRqPy2VVfK0pVy1Zaam0YoUZO/pov7Kza/+5vzVp6X9v1Ae5iCIXJvIRRS5M5COKXESRCxP5iCIXJvLReFp1kejpp59Wt27dVFRUpBkzZui+++7Td999p0suuUTPPPOMBg8eLLfbvfuJEqSgoFB2Kz/dwrLK/wLYvLlQjpPs1SQf+YgiF1HkwkQ+osiFiXxEtZZcFJWWGu1Qma38fPM5RbHmok2beUpJibZLSvZXSYk5t0pK1OHtd7RrqaropNMUqLSGpsS2nYqvlXPVGixY4FIkkl7Rtiype/dC5ecncVFNRGv5e6MuyEUUuTCRjyhyYSIfUeQiilyYyEcUuTCRjyiXq3E2lrTqIlHPnj0lSe3bt9dZZ52lAw88UGPHjlUgENA999yjl19+Wam7PLV2dzuEdl7f3Y6jmjiOWv0f/J3IhYl8RJGLKHJhIh9R5MJEPqJaei7CtnncnNvy1Pj91i8XZfJ45huRcHhAlfEpn34iq6Q4eg+XS4ETT202OW8u60ykl182n1vVt6+Unt46c1GTlv73Rn2QiyhyYSIfUeTCRD6iyEUUuTCRjyhyYSIfjff9u3bfpfXo06ePTj31VEnSokWLVFBQYDyHqLCw5k9X2rat4uLyfxRo165dwy4UAAAAVUSciNF2W4nZEe7zvSmXa6sRC4UOqtLPteYXox0eNFhOx44JWQMS75NP3Hr22RQjdvTRSVoMAAAAACQJRaJK9ttvv4rXubm56tGjR0W78jOMdrVp0yaFQiFJUk5OToOtDwAAADVooI9Zpab+12gHg0fJtveq0s+1aZPRtnO6Nsh6EL/CQunaa83d/2lpjm64IUkLAgAAAIAkaTVFoieffFLnnXeerrrqqlr77XqknN/vV6dOndS2bVtJ0vLly2sct2zZsorX/fv3j2+xAAAAqDdHZpHIsqwaetadx7NYXu88I1Zaelm1fV2VHmRjd8iO+/4NLT09XZmZmUpPT9995xbkttt8ys013wr99a9l6t07SQsCAAAAgCRpNUWivLw8LViwQLNmzdLGjRtr7Dd37lxJ5W+Yd+4iGjZsmCRp9uzZcmr4hOrMmTMlSR07dlS/fv0SuHIAAADUReWf0yzFXyTy+58y2pFIjoLBk6rt68qvtJMou+kXib74YoG2b9+uL75YkOylNJqZM9164QXzmLkjjwzrt78NJWlFAAAAAJA8raZItPNZQ+FwWPfff3+1fd599119+umnkqTTTz9dKSkpFa8l6ccff9RLL71UZdzy5cv15ptvSpIuvPDChHxqFQAAAPWT6J1ElrVVfv+rRiwQuEiSt9r+VYtEPI+oqdm6VbruOvOYufR0Rw88EJCr1bwzAgAAAIAoT7IX0FgGDBigMWPG6M0339Rbb72l7du367LLLtPee++tzZs364033tCzzz4rSdprr730hz/8oWLs0KFDNWLECM2cOVN33nmn8vLyNHbsWPn9fs2ZM0f33nuvQqGQunXrpnPPPTdJ3yEAAEDrVnXHd3xFIp/vbVlW6S7ze3YUiapnba503FxHikRNSSgkXXJJqtavN6tBf/97mbp3b5jnWQEAAABAU9dqikSS9M9//lMlJSX66KOPNGvWLM2aNatKn3333VePPPJIxXOIdpo4caIuueQSLVmyRE888YSeeOIJ43p2drYmTZqkjIyMhvwWAAAAUIPScKnR9rlTauhZN273j0Y7GBwp286psb9r6xaj7bRrH9f9kTiOI91wg09z55pvf4YPD+uCCzhmDgAAAEDr1aqKRCkpKXr44Yc1ffp0TZ06Vd9++622b9+ujIwM7bvvvjrppJM0ZswYeb1VjxDJysrSlClTNGXKFL3zzjtavXq1gsGgunbtquHDh+uyyy5Thw4dkvBdAQAAQJIKApuNdnt/vD+blRit2gpEkmQVFhptJzMzzvs3vNtv/4sCgWL5/em67bY7kr2cBvPggyl66SWzaNixo61//zsgTooGAAAA0Jq1qiLRTqNGjdKoUaPqPc7r9Wr8+PEaP358A6wKAAAA8SgIFBjt9v74dvLsetScJDlOas2dy8pkBYNm/4ymXySaNu01rV+/Tjk5XVpskeiNNzy66y6fEUtNdfTCC6Xq0oVj5gAAAAC0bjyeFQAAAM2e4zgJ30lkWSWVIjUXiayioqpragY7iVq6r75y6+qr/UbMshw9/nhAgwbZSVoVAAAAADQdFIkAAADQ7BWHihSyzWfLtGvEnURW4fYqMTudZ1Um048/WrrwQr/Kyszz5P7+9zKdeGI4SasCAAAAgKaFIhEAAACavc2VdhFJUofUeHcSFRttx0mrsa8rL8/sa1lSenpc90fsIhFpwoRUFRSYb3cuvjio3/0uVMMoAAAAAGh9KBIBAACg2SsKVj3uLcMb+3FvlrVVXu+XRsy2a9iZ5DhKv+cus29OF8myqu+PBvf00159843biI0aFdadd5bx2wIAAAAAu6BIBAAAgGbPkWO0XZZLVhzVAJ/vZeO4OcfxKBQaWX3ft99QyiezjFjZ2LNjvjfik5tr6a67fEasd++InnyyVB5PkhYFAAAAAE0URSIAAAA0e9UVieKZLTX1aSNSVnaKbHuPKj2tokKl//UWIxbJ6aLia/8Ux/0RK8eRbr7Zr5ISs0D4f/9XpgweEQUAAAAAVVAkAgAAQPPnmEUiS7HvIvJ6P5fHs9KIBQK/rbZv2r0T5d6w3ogV/fNuUZFIjv/9z6OPPjK3C11wQVCHHRZJ0ooAAAAAoGmjSAQAAIBmr/JOoniKRH7/U0Y7HO6tUOjoKv3c3y1X6pOPGbHgsOEKnjIm5nsjdtu2SbfcYh4z17Gjrb/+tSxJKwIAAACApo9TuQEAANDsOZV3EsX4PCLLKpDP97YRK99FVHW+9H/dKSsS3aHipKSoaOJ9UhzPQmpso0cfp5KSQqWlZSZ7KXG77z6f8vLMz8DdeWeZ2rZNznoAAAAAoDmgSAQAAIBmb/Gmb4x2qic1pnk8nqWyrFBF23FSFAicV21f7xefGu2Sq65RpNc+Md03We6//0FlZ2cqP7+w8ol9zUphofTCC14jNmpUWKedFk7SigAAAACgeeC4OQAAADR7L373nNEe2uXImOaxrG1G27a7ynHaV9+3zDzGLHjsCTHdE/F75RWvioujO7hcLkd33RVoTpu6AAAAACApKBIBAACgWVuav0SL8hYasfP3HR/TXJa13WjbdpuaO4cr7VLxsEk/GWxbevrpFCN2/PFh9ejRjLdGAQAAAEAjoUgEAACAZu2l7yYb7Zz0LhrefVRMc1lWodF2nFqe1VOpSOS4KRIlw+zZbq1ebb6tufTSUA29AQAAAAC7okgEAACAZqs0XKqp379ixM7t9xt5XLEVbFyuOhaJHEeWbZuxZriTaNSoYerWrZtGjRqW7KXErPIuon33jeiIIyJJWg0AAAAANC/N750sAAAAsMP7P/1P28q2GrFz+p0f83x13kkUqaYI4XbHfN9kycvbqPXr18m2m+fRbD/9ZGn6dDPvv/1tiGcRAQAAAEAdsZMIAAAAzdbXG74y2kd1HaYeWXvHPJ/LtcZoO071zySytm2rEnO83pjvi9g8+2yKHCdaEcrKcjR2LEfNAQAAAEBdUSQCAABAsxWMBI12v/b7xjGbrZSUT4xIOHxAtT09ixcZbcfvl92laxz3Rn2VlEhTppiFuXPOCSk9PUkLAgAAAIBmiCIRAAAAmi3HMY9Js+I4Z8ztXi6XK9+IBYPHVNvXW6lIFN5/gMROokb1xhtebd1q/n5ffHGwht4AAAAAgOpQJAIAAECzZTu20bYUe5EoJWW20Y5Eesi2qz+6zvONWSQKDRwU831Rf44jPf20WZQbMSKsnj2b57OVAAAAACBZKBIBAACg2XJUqSgQx04ir3eW0a5pF5Ekeb5ZaLTDAwfHfF/U39dfu7R0qduIXXIJu4gAAAAAoL4oEgEAAKDZqlwkcsX8421AKSmfG5FQaFi1PV0bN8i9fp0Ro0jUuCZNSjHa3bvbGjEikqTVAAAAAEDzRZEIAAAAzVaV4+Zi3EmUnn6nLKvYiAWD1ReJPEu/NdeQnqFI731iui/q79dfLb39tseIXXRRUG53DQMAAAAAADWiSAQAAIBmqyxcZrT9bl+95/B6P1dq6kNGLBg8So6TXW1/9+pVRjvSr5/k4sfqxvLYYykKh6PFwNRUR+edF0riigAAAACg+fLsvgsAAADQNJWEzd0/ad70es5QpMzMCbKs6LF1jpOioqJ7axzh/nG10Y707F3PezYdt932D7ndjiKR2J/l1Jjy8iy9+KLXiF1wQUjt2ydpQQAAAADQzFEkAgAAQLNVGi412qme1HqNz8j4i9zun41YcfFfFIn0r3FM1SJRr3rdsyk588xxys7OVH5+oRxn9/2T7b//9SoQiBa0vF5HV1wRTOKKAAAAAKB541wMAAAANFul4RKjnepJq/PYlJSPlJo6yYiFQoeptPQPtY5z//ij0W7ORaLmZNs2adKkFCN21lkhde3aDKpbAAAAANBEUSQCAABAs1USinUnUZEyMq4yIo6Tru3bn5DkrnlYMChX7hojRJGocTz0UIoKC6O7iCzL0R/+wC4iAAAAAIgHx80BAACgWXIcR1vKCoxYXXcSpaTMkNu9wYgVFd0h2+5Z6zjvvC9l2bYRa85FolWrftDGjT4VFpapV699kr2cajmO9K9/pejhh31G/JRTwurVi11EAAAAABAPikQAAABoln7Y8r02FK83Yj2y9q7TWJdrndEOhw9QIPDb3Y5Lfewhc1zvfeRktqnTPZuiM844RevXr1NOThctXrwi2cupwralW2/16emnzWPmLMvRNdewiwgAAAAA4kWRCAAAAM3S9DUfGe2c9C7at33/Oo6OGC3b7ijJqr7rDu7ly+Sbbt6z9JLL63g/1FcoJF1zjV+vveatcu0f/yjTAQfY1YwCAAAAANQHRSIAAAA0S9N/+dBoj9rrWFlW7YWenSwrZLQdZ/c/FqdV2kVkt2+vwDnn1+l+qJ9AQLr8cr8++MAsEFmWo/vuK9MFF4RqGAkAAAAAqA+KRAAAAGh2tpdt05frPzdiI7sfW48ZwpXaVXer7Mq1Nle+aVONWOlvL5fS0+txT9RFUZF04YWpmjvXfKvi8Th67LGAxoyp/HsHAAAAAIgVRSIAAAA0O3NyZytsR4sFKa4UHb3nMXUeX3kn0e6KRKn/eUxWOHo/JzVVpZf8rs73Q91EItI556Rq3jzzbUpqqqNJk0o1cmSkhpEAAAAAgFhQJAIAAECzM/vXmUZ7aJcjlOHNqMcMZUbLcdw19rQ2blTq5ElGLHDeBXI6dKjH/VAXb7zhqVIgysx09OKLpTrsMApEAAAAAJBoFIkAAADQ7KwoWG60j95zeL3GezwLjbZtd66xb/r9E2WVlFS0HZdLJROuqtf9UDfTp5tvTzp0sPXKK6UaMMBO0ooAAAAAoGVzJXsBAAAAQH2t3vqD0e7Xrl89RpfK6/3SiIRCR1Xb0/3jKvmff9aIBc67QPZePepxP9SF40hz55o7um6+OUiBCAAAAAAaEEUiAAAANCubSzerIFBgxHq126ceM3why4oeN+c4LoVCR1TbM+3uO2RFosecOX6/Sm64pV7rRd2sWOHSpk3m25Ojjw7X0BsAAAAAkAgUiQAAANCsrKq0iyjFlaLumXvVY4YZRiscHijHaVull+ebhfK/Nc2IlV52heycLvW4F+qq8i6iPfe01aOHk6TVAAAAAEDrwDOJAAAA0Kys2vK90d47q6c8rvr8WGsWiUKhY6p2cRyl//N2I2RntVXJH66tx32avo8+mq22bVO1dWtpspeiuXPN38Mjj4zIspK0GAAAAABoJSgSAQAAoFmpvJOod7s+dR5rWYWSvjZiweCwKv18r7+qlLmzjVjJNdfLaduuzvdqDvbYYw9lZ2fK7y+Uk8RNO//7n0fTp5s7iY46iqPmAAAAAKChcdwcAAAAmpW1Rb8a7b2zetZ5rNu9XJJd0XYct0KhQ40+rjW/KOOm641YpEtXlV5yef0Xi916/32PLr/cr0gkum3I43F01FGRWkYBAAAAABKBIhEAAACalQ3FG4x2TnpOnce63d8Z7UhkH0lpuwaUedXv5CrcbvQr+sddUmpqvdeK2n34oVuXXupXOGyeK3fVVUF17szziAAAAACgoXHcHAAAAJqV9cXrjPYe6V3qPNbjWW60w+H+Rjv1kQeU8uXnRiww7lwFTz29nqtsHiZPfkZSWJJHF1xwcaPee/p0ty65JFWhkFkguvDCoG65JdioawEAAACA1ooiEQAAAJoNx3G0sdJOoj3S96jz+Ko7ifateO1ZvEjp/7rTvN59LxXdfW8MK20e7rvvX1q/fp1ycro0apFo5ky3Lr44VcGgWSA6//yg/vWvMllWDQMBAAAAAAnFcXMAAABoNraVbVUgEjBie9TjuDmPxywShcM7ikQlJcq84lJZ4XDFNcfl0vZHnpST2Sb2BaOKTz9166KLUlVWZlaCzjknpPvuK5OLdygAAAAA0Gh4CwYAAIBmY33x+iqxTmmd6zTW7V4mlyvPiO3cSeR/dYo8q34wrpVc80eFDxsa40pRnVBIuvpqvwIBs0B01lkh/fvfAQpEAAAAANDIeBsGAACAZuOr9V8Y7Y6pneRz++owslRt2lxiRGw7U5FIT0mS9+uvjGuhAwep5E+3xLVWVLVggVu5ueZbkDPOCOmhhwJyu5O0KAAAAABoxSgSAQAAoNn46Of3jfZR3Y6u07iMjL/K41luxAKBSyXtqEwEg8a14MjRktcb8zpRvdmzzUpQ374RPfIIBSIAAAAASBaKRAAAAGgWikPFmrt2jhE7tscJux2XkvK+UlOfNGLh8AEqLv5zRduqVCRSSkrsC0WNZs/2GO3Ro8PyeGroDAAAAABocK32LdmcOXP0+uuv65tvvlFBQYFSUlK01157adiwYRo/frzat29f7bhQKKQpU6bo7bff1urVq+U4jrp27apRo0bp4osvVtu2bRv3GwEAAGgl5vw6S2WRsoq223JrxJ6jah3jcm1QZubvK0VTVVg4SdIux9SFzCKR46VIlGgFBdKiReZn1IYPjyRpNQAAAAAAqRUWicLhsG6++Wa98847RjwUCmn58uVavny5Xn31VT366KMaNGiQ0aesrEyXXnqp5s2bZ8RXrVqlVatWadq0aXr66afVp0+fBv8+AAAAWpvKR80N7XKE2vrb1TLCVmbm7+Ryba4Uf0CRSF8jYgVDZhcfRaJEmzvXI8exKtqpqY4OOYQiEQAAAAAkU6s7bu7++++vKBCNHDlSU6ZM0Zdffql33nlHf/rTn5SWlqbNmzdrwoQJ2rhxozH2lltu0bx58+T1enXddddpxowZmjt3ru644w5lZWUpLy9PEyZMUElJSTK+NQAAgBbJcRxN+e4FTfthqhE/tsfxtY7z+V5VSsosI1ZWdqqkyyrfQNa2rWaInUQJN2uW+eChww+PyOeroTMAAAAAoFG0qiLRxo0bNXnyZEnSKaecoscee0yDBw9Wu3bt1KdPH1122WWaPHmyPB6Ptm7dqv/85z8VY5csWaJ3331XknTrrbdqwoQJ6tatmzp16qSzzjpLzz77rLxer9auXVtxDwAAAMSnKFioK6Zfqmtm/V6BSMC4trvnEfl8bxntSKSLiooekmSZ/V55Sd7Fi4yYU8PRwy1Nr1691b9/f/Xq1btB7xMMSu+95zVixxwTbtB7AgAAAAB2r1UViaZPn65wuPzN6HXXXVdtnwMOOECjRpWfbT979uyK+DPPPCNJ6tatm8aNG1dlXP/+/TVmzBhJ0tSpU6tcBwAAQP0szlukkVOPqrKDSJLO7nueemb1qmW0I6/3CyNSXPxXOY5Z/HGv/kGZN//JiNltshQcPjLmdTcnb7zxPy1btkxvvPG/Br3P9Okebd1qFudOOIEiEQAAAAAkW6sqEuXl5cnv9ys7O1tdu3atsd9ee+1V0V8qP+Jk7ty5kqThw4fL7XZXO27kyPJ/TMjNzdWKFSsSuXQAAIBWw3Ec/Wfxozpx2ij9tO3HKtcv2u8S3TPs37XO4XZ/L5erwIiFQkeZncrKlHn5b2WVFBvhwv97SE6brNgWj2q99pr5KNTDDgure3cnSasBAAAAAOzk2X2XluO6667Tddddp6Kiolr7/fLLL5KkrKzyfxzIzc3V9u3bJUn77bdfjeP69+9f8Xrp0qXq169fvEsGAABoVTaXbtY1M6/QR798UOVam5Qs/Xv4wzql15jdzlN5F1Ek0lW2vaesXTazpP/zb/IuWWz0K73gYgVPPT2mtaN6W7dKH31kvu0YO5ZdRAAAAADQFLSqItFOGRkZNV7buHGjZs0qf8DxkCFDJElr166tuN6tW7cax3bs2FFer1ehUEi5ubkJWi0AAEDr8OW6z/W7j3+r9cXrqlwb0vlg/Wf0JHVvs1ed5vJ6PzfaodBQ7fosopQP31fak48bfcL99lXRP++u/8JRq3fe8SoY3CX3KY5OPTWUxBUBAAAAAHZqlUWimjiOo7/97W8qKyuTJJ133nmSpC1btlT0adOmTY3jXS6X0tPTtXXr1oqdRwAAANi9VVt+0FnvnKaySFmVa38YdJ1uPuQv8rq9dZ6v8k6i8iLRDkVFyrj2SuO64/dr+3+ekdLS6rfwZm7ChEtUWLhNmZlZevzxpxvkHpWPmhs9Oqy2bRvkVgAAAACAeqJItIu7775bs2fPliSdfPLJOuywwySpomgkSX6/v9Y5fD5flTF1ZVkyjkBpjXZ+/609DzuRjyhyEUUuTOQjilyYyEdUc8jFW6tfr1Ig6pjaUY+OelLDu4+s11wu1yq53b8YsXD48OjPWu+/L1d+vnG9+I6Jsvv3VxNOUYP4/PPPtH79OuXkdGmQPx9FRdJXX5nP8xw3Ltwk/yw2h/9OGhP5iCIXUeTCRD6iyIWJfESRiyhyYSIfUeTCRD6iGisHFIlUvoNo4sSJeu655yRJffr00T/+8Y+K6263u6ahCdW+fWaj3Kc56NCBXOyKfESRiyhyYSIfUeTCRD6imnIu5m/6ymgfvufhen3c69ojY48YZptZqb2H2rU7RJKrvLnjQ0EVjjpKGX+8Whmt8F2Iy2VVfM3OTvyfj+XLJduOtt1uaezY1Ca9Yasp/3eSDOQjilxEkQsT+YgiFybyEUUuosiFiXxEkQsT+Wg8rb5IFAwGdeutt+rtt9+WJPXq1UuTJk1Senp6RZ/U1NSK17vbIbTz+u52HFWnoKDQeBPdGllW+V8AmzcXynGSvZrkIx9R5CKKXJjIRxS5MJGPqKaei1AkpC9+NY+Hu3z/K+UJpCs/UFjv+bKypsm7y8l0gcDxKioqlrQjF5WKRMXHjFLp5qJ636clsG2n4mt+fv1zvTuzZ3slRX8u3nffiEpKSlRSkvBbxa2p/3fS2MhHFLmIIhcm8hFFLkzkI4pcRJELE/mIIhcm8hHlcjXOxpJWXSTaunWrrrrqKn399deSpP32209PPfWU2rdvb/Tb9TlEhYU1v3m2bVvFxeX/ANGuXbt6r8dx1Or/4O9ELkzkI4pcRJELE/mIIhcm8hHVVHOxeNM3KgmbVYND9hga01ota7M8ni+NWFnZCRVzWXl55dtbdhE84qgmmZfG1hA5WLjQ3JE/aFCkyee6qf53kizkI4pcRJELE/mIIhcm8hFFLqLIhYl8RJELE/lovO/f1Ti3aXrWrFmjs88+u6JAdNRRR+n555+vUiCSpB49elS8XrduXY1zbtq0SaFQSJKUk5OT2AUDAAC0UF+s+9xo923XT9mp2THNlZLygSwrujXbcdIUDB5T0fZ+/qnR307PUHjAwJjuhd1btMgsEg0e3Mq3zQMAAABAE9Mqi0Q//PCDzj77bP3888+SpHHjxumJJ54wjpjbVadOndS2bVtJ0vJKnzzd1bJlyype9+/fP2HrBQAAaMm+WGcWbg7NOTzmuXy+d412MDhSUvToYO/nc43rocOGSp5Wvbm+weTnW1qzxny7MXBgJEmrAQAAAABUp9UViX799VddfPHFKigokCRdc801+uc//ynPbv5xYNiwYZKk2bNny6lhn9fMmeUPSe7YsaP69euXwFUDAAC0TIs2LtD0Xz4yYkO7xFYk8ni+UUqKWSQqKzvR7LNwodEOHX5UTPfC7n3xhbmLKC3NUd++7CQCAAAAgKakVRWJQqGQrr32Wm3atEmSdMstt+j3v/99ncaefvrpkqQff/xRL730UpXry5cv15tvvilJuvDCC2VZVmIWDQAA0EKF7bCun3ONHEU/gON3+3XMniNjmM1RRsYNsqzoXOVHzZ1g9HLlbTTaET7Y02A+/tj8ENZBB0XYtAUAAAAATUyrepv2yiuvaOnSpZKkE044QWeddZaKi4trHbPzCLqhQ4dqxIgRmjlzpu68807l5eVp7Nix8vv9mjNnju69916FQiF169ZN5557boN/LwAAAM3dk98+rqX53xqx6w+6SR1SO9R7Lp/vZXm9Xxmx4uIb5Di7PG/SceTanG/0sTvE9uwj1C4SkaZPN3cSHXdcOEmrAQAAAADUpFUViZ577rmK1++//77ef//93Y5ZuXJlxeuJEyfqkksu0ZIlS/TEE0/oiSeeMPpmZ2dr0qRJysjISNyiAQAAWqA123/RPfPuNGL7tu+v3w+8ut5zWdZ2ZWT81YiFwz1VWnqV2a+oUFYwaMRae5HoggsuVCgUkNfrT+i8ixa5lJ9vHlowejRFIgAAAABoalpNkaigoEBr1qyJa46srCxNmTJFU6ZM0TvvvKPVq1crGAyqa9euGj58uC677DJ16FD/T74CAAC0Jo7j6OZPrldJuMSI33fMg/K6vfWeLy3tX3K58oxYcfFEST4jZuWbu4gkyWnlP7vdcMMtys7OVH5+oWp47GZMPvrIfJvRt29EPXok8AYAAAAAgIRoNUWi9u3bG7uCYuX1ejV+/HiNHz8+AasCAABofd798R1NX/OREbtwv0t08B6H1nsut/sHpaY+bsTKyo5TMHh8lb6Vj5pzfD456ewAbwiVi0TsIgIAAACApsm1+y4AAABA4jy77Gmj3Smts/5y2G0xzeXzvSbLihYgHCdFRUUTq+3r2rTJaNsdsiXLium+qNmaNZaWLzefR3TssZEkrQYAAAAAUBuKRAAAAGg0juPom7yFRuwvh92uLF/bmOZzu38x2oHAObLtXtX29axYbrTtPbvHdE/U7v33zV1E7dvbOvhgikQAAAAA0BRRJAIAAECj+WX7z9oe3GbEjuo6LOb5LMvcHRSJ7F1jX/fyZUY7vN/+Md+3pRgwoJ8sy9KAAf0SNue775pFouOOi8jtrqEzAAAAACCpKBIBAACg0SzJ/9Zod/B3UJeMrjHP53JVes6Qk11jX8+yJUY7vN8BMd8X1du0ydJXX5kVoRNPDCVpNQAAAACA3aFIBAAAgEazNH+x0d4/e4CsOJ4L5HJtNtq2XUORqKRE7h9XGyF2EiXehx965DjR38+0NEfDhnHUHAAAAAA0VRSJAAAA0Gi+3WQWiQ7oeGBc81XeSVRTkciz8jtZtl3RdixL4b77xnVvVPXee+ZRc6NGheX3J2kxAAAAAIDdokgEAACARmE7thZvWmTEBmTHXiRyuX6RZZUYMcfpUG1fz1LzqLlIz15SenrM90ZV333n0qxZlY+aCydpNQAAAACAuqBIBAAAgEaxOG+R8kvNnT8DOw2Oeb60tPuMtuOkKRLZs9q+ngVfG+3I/gNivi+qchzpppt8ikSiR835/Y5GjaJIBAAAAABNGUUiAAAANIoPf3nfaO/Tto96ZO0d01wu10/y+18wYqWlF0nyVdvf+/VXRjt08CEx3RfVe/VVj7780jxq7ve/D6pNmyQtCAAAAABQJxSJAAAA0Cg++vkDoz26x/Exz5WWdq8sK1LRdpxUlZRcV21fq2CzPD98b8TChxwa871h2rpV+vvfzeJc9+62rr46mJwFAQAAAADqjCIRAAAAGty6orVamv+tETuuxwkxzeVyrZbfP8WIlZZeKsfpXG1/b6Wj5pSaqjDHzSXM3Xf7lJ9vvq24666A0tKStCAAAAAAQJ1RJAIAAECDq7yLqK2vrQ7eI7bdPOnp91TaRZSmkpJra+zvmT/PDBx8sOT1xnRvmL74wq1nnzVzefzxIR17bKSGEQAAAACApsSz+y4AAABA7GzH1kvfTTZiI7qPlsdV/x9FLWuzfL5XjVhp6eVynI41jvHOr7STaOjQet+3pXr88f/K73crEKhfUcdxpKef9uq223xyHKsinprq6I47yhK9TAAAAABAA6FIBAAAgAb14neT9c2mRUbs+B4nxjSXx7Os0i4in0pKrq51jGttrhkYNCime7dERxxxlLKzM5WfXyjHqduYwkLpuuv8evvtqruxrrsuqO7d6zgRAAAAACDpKBIBAACgwWwu3aw7vrjNiPVq21sn9jwlpvnc7p+MdiSyjxwnu9Yxru3bzECHDjHdG9LSpS5dckmqfvqp6qnVw4aF9fvfB5OwKgAAAABArHgmEQAAABrMnV/eri1lW4zY3UfdpxR3Skzzud0/Gu1IZO/djrG2bzcDWVkx3bs1cxzpxRe9OvHEtGoLRH/4Q5mmTClVSmy/rQAAAACAJGEnEQAAABrE/A3z9MJ3zxmxU3udrmP2HBHznFV3Eu2mSBQIyApW2t3Stm3M929pPvtsbsUziQ4//Khq+xQXSzfd5Nerr1Y9Xq5tW0ePPFKqY4+t3zONAAAAAABNA0UiAAAAJFzEjuimT643YmmedP3jiLvimtflqlwk6llr/yq7iCR2Eu3iiisu0/r165ST00WLF6+ocn3dOkvnnJOqFSvcVa4NGhTRf/9byjOIAAAAAKAZ47g5AAAAJNzU71/WkvzFRuyGg29Rl4yuccwarvdxc67CbVWDFInq7OabfdUWiC69NKh33imhQAQAAAAAzRw7iQAAAJBQjuPoicWPGrG+7frp8gFXxDVvSsq7crkKjVgk0qfWMe4ffjDadkamXD6fVBisYQR2tXSpWSDKyHD0wAMBnXpqOEkrAgAAAAAkEjuJAAAAkFCfrv1EyzcvNWJ/Gfp3ed1Vn2lTH6mpTxrtUOgg2XbtO5O88+cZ7fDAQXGtobWp/Dinhx+mQAQAAAAALQlFIgAAACTUfyrtIurVtrdG73VcXHO63cuVkjLXiJWW/m634zyVi0QHHRLXOlqbcKV6UNu2HC8HAAAAAC0JRSIAAAAkzOqtP+ijXz4wYpcNuEIuK74fOyvvIrLtTiorO732QeGwvN8sNEKhgw+Oax2tTShkGW0Ph1UDAAAAQItCkQgAAAAJ8+S3jxvttr62OrvveXHNaVlb5fe/bMRKSy+SlFLrOM/ypbJKSoxYeDBFovqovJPI62UnEQAAAAC0JHwWEAAAAHFzHEfPLZuk55c/a8Qv6H+x0r3pcc2dlvZ/sqxoscdx3AoEfrvbcZ6vKx01t3dPOdnZca2ltQmFzLY3vsdKAQAAAACaGIpEAAAAiEtZpEx/nntDlQKRx+XRJQdcHtfcKSn/U1raA+b9yk6VbXfZ7VjP9yuMdngIu4jqw7alcJjj5gAAAACgJeNtHgAAAGK2sXiDLv7gfM3fOK/KtcsH/F5dMrrGPLfb/YMyM39nxBzHo9LSP9ZpvGvDBqMd2atHzGtpjSqd1CdJSkvjuDkAAAAAaEkoEgEAACAm8zfM08UfnK+NJRuqXLtm8PW6+ZC/xDy3ZRWqTZvz5HIVGvGiorsVDh9YpzlcG9YZbXuPnJjX01J9++0KZWdnKj+/UE6l+k9JiVWlf1paIy0MAAAAANAoKBIBAACg3l767nndOOc6Be2gEU/zpOnBEY/ptN5nxDG7o8zM38vjWWlEA4GzFQjU/fi6yjuJ7ByKRPXBTiIAAAAAaPkoEgEAAKDOwnZYf/n0Jk1a+t8q17pn7qVnT3hJ+2cfENc9UlMflM/3lnnf8AEqLHxQUtXdLdWKROTK22iE2ElUP9u3V811amoSFgIAAAAAaDAUiQAAAFBnd3x5e7UFoqO6HaP/HvuM2vs7xDW/y/Wz0tNvN2K23Vbbtr0gqe5nnVn5+bIiESMW6UyRqDa2LS1Z4tKsWR7NmuXW11+7jetpaY5criQtDgAAAADQICgSAQAAoE5+3LZaT377WJX4hAOv0t+G/kMeV/w/Wvp878qy7Iq241javv1p2fbe9ZrHXel5RI7bLadjx7ruQ2o1brttolavLtWaNRnatOl25efXXAVq25aj5gAAAACgpaFIBAAAgDqZ+NU/FbbDFW2vy6t/D39E4/qem7B7WNYWox0MHq9QaHS953GtXWu07ZwuYhuMVFYmffWVW7NmeTR7tlvLlj0naa2krpL+UevYMWPCtV4HAAAAADQ/FIkAAACwW9/kLdSbq6YZscsH/D6hBSJJsqxSo+047WKax7W+miJRK+Q40urV1o4j5Dz6/HO3Skrqvp/K63V06KERnX56WL/5TagBVwoAAAAASAaKRAAAANitf355u9HO8rXV1YOvS/h9LKvEaDtOakzzuCvtJIp07Rrzmpqb7dulTz4pf67Q7Nke/fpr/XZQ9expa/jwsIYPD+vwwyPKyGighQIAAAAAko4iEQAAAGo1+9eZmps724j9YdB1audvn/B7WVax0Xac9Jjmca2rvJOoZReJNm609MILXs2c6dHChS5FIvV7+pLfL/3jHwENHx7WXnvx7CEAAAAAaC0oEgEAAKBGjuPojkq7iHLSu+iyARMa5H5VdxKl1Wu8a/06+Z9/VikzPjbidpeWe9zcxo2Wjj46XVu21K0wZFmOBg4s3y303HOONm+W2rVzdNFFHCcHAAAAAK0NRSIAAADU6Kv1X+jbTd8YsRsP/rNSPbEdA1cbl2udvN45RsxxMnc/0HHk/fQTpT7zlFLe/5+sSKRKl0iXbolaZpPz1FPe3RaIOne2NXx4RMOHh3X00RF16FC+W2jKlMZYIQAAAACgqaJIBAAAgBo9v/xZo713Vk+d3e+8BriTo4yM6+RybTOiodDRNY6wtm+T79UpSn3mKXl++L7mmVNTFTryqISttCkJh6VXXvFWift8jg49NLLj2UIR7buvLat+J9ABAAAAAFoBikQAAACo1rayrXpn9ZtGbHz/38rjSvyPkD7fa/L53jdigcA5CocPrNLXvXSJUp95Sv7XX5FVUlLl+q5CBx+qojsmymmX+OcnNQWzZ7u1YYPLiP3nP6U67riw0up3Uh8AAAAAoBWiSAQAAIBqvfb9qwpEAhVtr8urcX3PTfh9LGuTMjJuMGK23UlFRROjgbIy+d55U6nPPCXv11/VOp+TlqbAmWer9KJLFDlgQMLX25S89JK5i2jQoIhOPz2cpNUAAAAAAJobikQAAACownEcvbD8OSN2/N4nqWNax4TfKyPjBrlcBUassPB+OU57udatVeqk/8r/0mS58vNrnSe8Tx+VXnypysadK6dNVsLX2dTk51v68EPzx/lzzw3Va47DDz9ChYXblJnZ8vMFAAAAAKiKIhEAAACqWLxpkZZtXmLEfrPv+ATfxVFq6sPy+6cZ0bKyMQoGT5Nn8SJljT1Nrm1ba57B7VbwxFNUevGlCh1xlFrTg3def92jUCj6/fr9jk4/vX5FoieeeFrZ2ZnKzy+U4yR6hQAAAACApo4iEQAAAKr44Kd3jfaemd11zJ4jEja/ZW1XZuaV8vneMuK23U6FhfdJkvyTn6mxQBTpvIcCF1ykwAUXyc7pkrB1NSfvvWf+KH/SSWFlsSEIAAAAAFAPFIkAAABQxZrCNUb7uB4nyGW5EjK3271MbdqcL49ndZVrRUUT5TidJEmu9euqXA8eebRKL75UweNPkrzeKtdbC8eRli93G7FTT+VZRAAAAACA+qFIBAAAgCo2FK832l0z9kzIvD7fFGVmXivLKq1yrbj4zyorO6eibZWVGdeLbr1Npddcn5B1NHcbN1rats08Wm+//SJJWg0AAAAAoLmiSAQAAIAqKheJcjJy4pwxoIyMm5WaOqnKFdtuq8LC/yoYPM6IW4GA0XbatY9zDS3Hd9+Zu7rS0x1161b/hwqdfvrJKijIV/v22Zo27X+JWh4AAAAAoJmgSAQAAIAq1lcuEqXH/twfl+tntWlzobzeRVWuhUKDtH37ZNn2XlUHVtpJ5Ph8Ma+hpVmxwiwS9e1ryxXDaYCrV6/S+vXrlNNKn+sEAAAAAK0dRSIAAAAYioKFKg4VGbHO6XvENJfHM09ZWWPlcm2tcq209BIVFU2UVH3xxyozdxLJ749pDS3Ntm3SjBnmj/H9+nHUHAAAAACg/igSAQAAwLCuaF2VWCw7idzuZdUWiBwnTYWFDxjPH6rM2rhRrtxcc5yvdReJSkqkp59O0cMPp2jrVvN5RP362UlaFQAAAACgOYvhUIqW54477lDfvn01bdq03fYNhUKaPHmyxo4dq0GDBmngwIE66aST9O9//1tbt25t+MUCAAA0sO8KlhntjqmdlOpJrdccLtdPysoaU6VAFA7voy1bZtZaIJLjKPPG6+QqNnczhfvvV681tBShkPTcc14ddli6/vlPX5UCkSQdeig7iQAAAAAA9dfqdxJNnz5dL774Yp36lpWV6dJLL9W8efOM+KpVq7Rq1SpNmzZNTz/9tPr06dMQSwUAAGgUSzZ9a7QP6DigXuNdrvVq2/Y0ud0bjXgwOFLbt0+W42TWOt735uvyvf8/IxYYc4bs7tU8t6gFs23prbc8mjjRp59+qv6zXS6Xo2uvDWrQIHYSAQAAAADqr1XvJJo5c6auvfZa2Xbd3lTfcsstmjdvnrxer6677jrNmDFDc+fO1R133KGsrCzl5eVpwoQJKikpaeCVAwAANJylm80i0f4d6l4ksqwCZWWdLrf7ZyMeCg3Vtm0v7rZAZG3apIxb/mTE7OxsFd11X53X0Nw5jjRjhlujRqXpd79LrbFAdMopIc2dW6Kbbw428goBAAAAAC1FqywS2bathx56SFdeeaVCoVCdxixZskTvvvuuJOnWW2/VhAkT1K1bN3Xq1ElnnXWWnn32WXm9Xq1du1aTJ09uyOUDAAA0qNh3EhUrK+sseTzLjWg4fIC2bXtFUtpuZ8i45U9yFRQYscKJ98vJzq7jGpq3r75y67TTUnXuuWlautRdbZ9hw8L66KNiPf10QPvsww4iAAAAAEDsWl2RaO7cuTrttNP06KOPyrZt7bdf3c62f+aZZyRJ3bp107hx46pc79+/v8aMGSNJmjp1asLWCwAA0Jg2lmzUptI8I7Z/9gF1GBlRVtb58nq/NqLhcE9t3fqGHKftbmdIeedN+d9+w4iVnXyagqeeXof7N0+BgPTJJ279858pGjUqTaeckqYvv6z+ROjBgyN6/fUSTZ1aqoEDKQ4BAAAAAOLX6p5JdOmll0qSvF6vJkyYoFNPPVWjR4+udYzjOJo7d64kafjw4XK7q/9U58iRIzV16lTl5uZqxYoV6tevX2IXDwAA0MDmrf/SaKd50rV3Vq/djktJeV8pKTOMWCTSRdu2vSXH6bTb8VZenjJvvM6I2e3bq3Di/XVYdfNh29KyZS598olbc+Z49OWXbgUCVq1j+vaN6JZbgjrhhLCs2rsCAAAAAFAvra5IZFmWRo8erWuvvVa9evVSbm7ubsfk5uZq+/btklTrzqP+/ftXvF66dClFIgAA0Ow8u+xpoz2w0yC5rN1vPne7Vxtt226nbdvelG3vtfubOo4y/3SNXJs3G+GiO++R02n3Baambt06S3PmlBeFPvnErfz8um3m33NPWzfcUKazzgqrhs8oxe1Pf7pJUlit8G0BAAAAAECt8N3g+++/r7333rteY9auXVvxulu3bjX269ixo7xer0KhUJ2KTwAAAE3Jd5uXa27ubCN2Tr/f1GmsZRUa7VBomCKRun1gxvfKS/J98K4RKzvhZJWdcVadxjc1hYXSZ5+59cknHs2Z49YPP9SvwpOdbeu664IaPz4kn6+BFrnD+PEXKzs7U/n5hXKchr0XAAAAAKDpaXVFovoWiCRpy5YtFa/btGlTYz+Xy6X09HRt3bq1YudRfViWWv0RIju//9aeh53IRxS5iCIXJvIRRS5M5COqrrl4askTRjs7NVun73NmnXLochUbbcdJr9u43F+VcetNRszOzlbR/Q/KciX+N68h/lyEw9LChS7NmePR7NluLVzoVjhc9xtYlqOBA20NGxbWMcdEdPDBEaWkJG59td/b/NqakQsT+YgiF1HkwkQ+osiFiXxEkYsocmEiH1HkwkQ+ohorB62uSBSLsrKyitd+v7/Wvr4dH/fcdUxdtW+fWe8xLVWHDuRiV+QjilxEkQsT+YgiFybyEVVbLvJL8jX1+5eN2BUHX6Fue3Ss4+zmzz5+f3v5/bvJvW1L51wtFZofrnE9+aQ67NuzjveNTTx/LhxH+v576eOPy3/Nni3V9/NBPXtKo0dLo0ZJI0ZYat/eLamBzpSrA/47iSIXJvIRRS6iyIWJfESRCxP5iCIXUeTCRD6iyIWJfDQeikR14G6oQ+ArKSgolG03yq2aLMsq/wtg82aOPJHIx67IRRS5MJGPKHJhIh9RdcnFAwseUSAcqGh7XV6N63mB8vMLqx9QSWZmgXE0WkmJTyUltYyNRJT673uVPmOGEQ6MO0dFR42S6njf+or3z8Wrr3r0r3/5tGZN3Z4rtFPbto6OPLJ8p9CwYWH16BG9uW1L+fn1X0sibNy4QVlZqdq2rVSdO++RnEU0EfydYSIfUeQiilyYyEcUuTCRjyhyEUUuTOQjilyYyEeUy9U4G0soEtVBampqxevd7RDaeX13O46q4zhq9X/wdyIXJvIRRS6iyIWJfESRCxP5iKopF47jaPKyZ4zYab3PUOe0PeqRuyKjZdsZNY71zp2jjL/9WZ5lS4x4pEtXFd15T6P8ftX3z4XjSPfem6L77qvbQ4K8XkeHHBLRsGHlRaEBA2zt+rmjpvJncvToY7R+/Trl5HTR4sUrkr2cJoG/M0zkI4pcRJELE/mIIhcm8hFFLqLIhYl8RJELE/lovO+fIlEd7PocosLCmj/Vatu2iovLz+Nv165dg68LAAAgETaWbNCvhWuM2KUH/K5ec7hceUbbcdpW6eNe/YPS//5X+T54r9o5Ch94VE5W1XHJZtvS3/7m05NP1v6goH33jRaFDjssovT0RlogAAAAAAAxokhUBz169Kh4vW7dOg0ZMqTafps2bVIoFJIk5eTkNMbSAAAA4raywNxBkuZJ08BOg+sxgyO3+0cjEonsXfHa2lKgtPv/pdRJ/5UVDlc7Q/H1Nyl0zIh63LNxhMPSH//o18sve6tc22MPW0cfXV4UOvroiDp3buUfcwMAAAAANDsUieqgU6dOatu2rbZu3arly5frlFNOqbbfsmXLKl7379+/sZYHAAAQl5UF3xntPu36ymXV/Zk7lpUvl2u7EYtEekrBoFKf+a/S7v+XXFu3Vjs23G9fFd1+p0IjRtV73Q2trEyaMMGvd981C0SW5WjixDJddFFIlpWkxQEAAAAAkAD1e+JuKzZs2DBJ0uzZs+XUcBjgzJkzJUkdO3ZUv379Gm1tAAAA8Vi5xdxJ1Lf9vvUaX3kXkeOkyPPet2p39KHK+Ost1RaI7OyOKrzvQW2Z+VmTLBAVF0vnn59apUDkdjt67LGALr6YAhEAAAAAoPmjSFRHp59+uiTpxx9/1EsvvVTl+vLly/Xmm29Kki688EJZ/KsBAABoJiofN9enff0+7OJ2rzIDv7iVNf438vy4ukpfx+dTydV/VMFXixQYf7HkaXob27dvl846K01z5phr8/sdPfdcqc48s/oj8wAAAAAAaG6a3rvyJmro0KEaMWKEZs6cqTvvvFN5eXkaO3as/H6/5syZo3vvvVehUEjdunXTueeem+zlAgAA7Jbt2PrP4se0MG++Ee/Xrm5FIpdrtdLSHpHf/6IRt5aUVts/MOYMFd96u+y9esS03sby97/7NH++24ilpzt64YVSHXFEJEmrAgAAAAAg8SgS1cPEiRN1ySWXaMmSJXriiSf0xBNPGNezs7M1adIkZWRkJGmFAAAAdbNm+y+6euYV+nzdp1Wu7dthv1rHejwLlJb2oFJS3pJlVXMMr/mII4UGD1HRPyYqfMih8Sy50cyda/6I3K6do5dfLtGgQXaSVgQAAAAAQMOgSFQPWVlZmjJliqZMmaJ33nlHq1evVjAYVNeuXTV8+HBddtll6tChQ7KXCQAAUCPHcfTyihd166c3qShUWOX6sXsdr26Ze1Y3Ul7vdKWlPaCUlLk132C7pGd3aT7+lMpOHyu5mscpx7YtrV1rHhv8xBOlFIgAAAAAAC1Sqy8SdevWTStXrqxzf6/Xq/Hjx2v8+PENuCoAAIDEyyvJ0/WzrtYHP79X5ZolS7878Er9+dC/VboSks/3utLSHpTHs6zGuR3Hq+DPR8k3eqa041FEdocOKjtzXAK/g4aXl2cpFDKLRP37UyACAAAAALRMrb5IBAAA0Bq8ueJNXfb2Zcovza9ybc/M7npoxOM6outRu0SLlJr6nFJTH5XbnVvjvLadqUDgtyotvUIpb8+Qb/XMimuRnK6J/BYaxa+/mgWilBRHHTtWc6ReCzFt2jvKzPSpsLAs2UsBAAAAACQBRSIAAIAWqiRUoq83fKVXV76kqd+/Um2fc/udrzuOnKjMlDY7IgGlpd2v1NT/yOXaWuPckcgeKi39vQKBi+U4WZIk17q1Rh+7S5dEfBuNau1a81i8Ll2c5nJSXkx6995H2dmZys8vlNNya2EAAAAAgBpQJAIAAGghSsOlmr9hnj5bN1efrZ2rhRvnK2SHqu2bnZqt+495WCfsfZIRz8y8Tn7/izXeIxzuo9LSaxQIjJPkKw9GIvK99or8Lzxn9LW7NMedRGZFaM89OWoOAAAAANByUSQCAABopgLhgBZunK9P136iz9d9qvkb5iloB3c77vi9T9L9wx5Sx7SORtyytsjne7XaMaHQoSopuU7B4PGSdhRSHEcp/3tb6f+6Q57vqz7jMdJtz3p/T8m2eLFZJOrWje01AAAAAICWiyIRAABAM1EWKdOijQuMolAgEqjz+Axvpu466h6d3fc8WZZV5XpKynuyLHPnUVnZiSopuVbh8GHRoOPIO2uG0u/+p7yLF1V7L8fvV/DEU+q8tqagrEyaMcP88fjggyNJWk3jeP31V+V2O4pELJ1xxrhkLwcAAAAA0MgoEgEAADRRwUhQi/IW6vO1c/Xpurmav+ErlYZL6zWH23JrYKdBOm6fY3V2rwuUk17zEXA+3xtGu6zsJG3fPsWIeb78Qul3/0MpX3xW4zyhQw5T0R0TFem9T73WmmyffeZWcXG0eGZZjo49NpzEFTW8v//9b1q/fp1ycrpQJAIAAACAVogiEQAAQBMRioT0zaaF+nztp/p07Sf6esNXKgmX1GsOl+XSgOwDdUTXo3Vk16N0SM5hauNro+zsTOXnF8qp4fQ0y9qilJRZRqys7PSK154li5V29z/lm/5Rzevff4BKbv2bgiNGS9XsVGrqPvzQ/NF48GBbnTpx3BwAAAAAoOWiSAQAAJBEJaESPbvsaX2SO0tfrf9SxaGieo23ZOmAjgfqiC5H6YiuR+qwnMPVxpdVz1U48vleN46acxyfQnlD5V04Q/4XJ8v/9hs1jg733kfFN/9FwZNPk1yuGvs1ZY5TtUh0wgktexcRAAAAAAAUiQAAAJIkryRPp75xnH7ctrrOYyxZ2i/7AB3R5Ugd0fVoHZYzVG397ep5Z0du9yp5vZ/u+PWZ3O51Zo+PPepw3H61zhLptqeKb7hFZWedI3ma74+Vti098kiK1q0zC1zHHUeRCAAAAADQsjXfd/MAAADNWNgO6/KPLqpTgWjf9vvpyK5H6YiuR2tol8PVzt++nnezJS2V3/+RPJ7PlOL9VC53Xq0jXM8U1zxbdkcV//EGBS64WPL56rmWpuWXXyxdfbVfX3xh/ljco4etPn3sJK0KAAAAAIDGQZEIAAAgCe748nZ9vu7Taq/1a7+vjuh6lA7vcpQO73KkOqR2qOfsttzupUpJKd8l5HV/JqlAGRl1HJ4r6Z1qZs1qq5I/XKvSS34npafXc01Ni+NIzz/v1d/+5lNxcdXnJ112WbA5PlYJAAAAAIB6oUgEAADQyN5Z/aYe++YhI9Ylvav+ccRdGtrlSHVM61jPGcPyeL4tLwi55sjr+VyulPo920iStFHSdEl3SNqxkcjx+RQ+4EAFR4xS6aW/k9O2vkfbNT3r1knjx6dqxozqfxS+4oqgLr00VO01AAAAAABaEopEAAAAjej7gpW6eubvjViKK0WTjn9egzsfVMdZQvJ4FsrrmauU0Efypn8jyxeo/2LWSpqzy6+VUrhPX4UHDVHo4oMUHjxE4X33k1JS6j93E/XGGx7ddJO0ZUvVH4O7d7f14IMBHXFEJAkrAwAAAACg8VEkAgAAaCRFwUJd/MFvVBwyd/ncedQ9uykQOfJ4FijF9ZFSiv8nT4cVsnzh+i/gFxlFoUhhJ4UHH6zQkIMUvnOIwgMHyWmTVf95m4GCAummm/x66y1vtdfPPz+of/yjrO5H8gEAAAAA0AJQJAIAAGhAjuNoYd58vf79q3pz1TTll24yrp/T7zca3//iGkaH5PO9qdTUR+T1LioP1aeGs1rRotDXqQq1G6LQ4IMUOmKIwlcfJDuni1rDg3e2bpWOPTZda9a4qlzr1MnWv/8d0OjRrXP3UKdOneVyWcrO7pTspQAAAAAAkoAiEQAAQANYvfUHvfb9q5r2w1T9tO3Havvsnz1A/zr6/2RVKtRY1jb5/c8pNfUJud25db/pCklzJGeupfCGvgp3GVp+ZNylB6ndpIO0bUuJHCeOb6qZmjbNW22BaMyYkCZODKh9+yQsqomYPn2OsrMzlZ9f2Cr/bAAAAABAa0eRCAAAIEHySvL05g+v6fUfXtWivIW19m3ra6tJxz2vVE9qRczl+kWpqY/L758sl6uoltE7LJU0R7KXZStYcojCex+h0OCDFL5jgJSeXtHNsiS53TF+V83f3Lnm996+vTRxYqnGjInhyD4AAAAAAFoQikQAAABxKAoW6r2f/qfXvn9Fn+TOlu3Yux1zUOdDdN8xD6pH1t6SJI/na6WmPiKf7y1ZVi3jf5L0tuR86VPJPpcq3PdohY4ZImccR4XVxLalzz83f+S9/37p5JPD7JwBAAAAALR6FIkAAADqKRQJadav0/X696/qg5/fU2m4dLdj9s7qqTP3GaexfcapZ9vekiJKSXlbaWmPyOv9svbBn0u6X9Kbkmyp6L57FBhf03OMsKvly13assU8zm/EiCQtBgAAAACAJoYiEQAAQB04jqOvN8zT6z+8ordWTVNBoGC3Y7JTO+r03mfqzD7jNKjTkB3PHiqS3/+E0tIek9v9cy33cyn88xB5z/ta2qWGFBw+UoELLor7+2ktPv3UPGquRw9b3bu7lJ+fpAU1Mddff41KSgqVlpap++57MNnLAQAAAAA0MopEAAAAtYjYEb3749t6YOH9Wpr/7W77p3nSdWLPkzW2zzgd3W24PK7oj1su109q23aM3O6fahzvOOkqLfqN7Jc6Ku3mR6Wt0Wt2VlsVPvDojocMoS4++8z8cfeII8KSUpKzmCbo448/1Pr165ST0yXZSwEAAAAAJAFFIgAAgGqEIiG9/sOremjh/2nV1h9q7eu23BrRfZTO7DNOx/U4Uene9Gp62WrT5vIaC0SRSBeVbr9UmuRS6v1Pyr1+XZU+RXffK5t/zK+zcFj64gtzJ9GRR0aStBoAAAAAAJoeikQAAAC7CIQDennFi3pk0QNaU/hLrX0P6nyIzuwzTqf1PkPZqdm19vX7n5fX+1WVeCh0oEq3XibXEwVKe/gRuTblVTu+7KRTVXbmuLp/I9DixS5t327uuqJIBAAAAABAFEUiAAAAScWhYk1e9owe++YhbSzZUGO/vbN66uy+5+mMfc5Sj6y96zS3ZeUrPf2vRiwS6aGivHvlfmyZMh6/Ta7Nm2scXzZytAof5Ji5+po71/xRt0+fiPbYw0nSagAAAAAAaHooEgEAgFZtW9lWPb3kST357WMqCBTU2G+/DgfouiF/0kk9T5Xb5a6xX3UyMv4il2urEQs9N1SZf7pcri1bahwXPGqYSq6/SaHDj6zX/VDuk0/M36ejj2YXEQAAAAAAu6JIBAAAWqX80nw9ufgxPb30SRUGt9fYb0jng/XHITdo1F7HyYphJ4/X+6n8/peMmPO6V/5LptQ4JnjMCBX/8SaFDxta7/uhXEmJNG+eWSQ66iiKRAAAAAAA7IoiEQAAaPEidkTfb1mpxZsWaVHeAi3OW6Sl+UsUtIM1jjmq6zBdO+RPOrLr0TEVh8oFlZF+rRnaJll/CFXbu2z0cSr5440KDzk4xvuhrEz64gu3pk3zKhiM/r65XI6OOCKcxJUBAAAAAND0UCQCAAAtiu3Y+mnban2Tt1Ar5y/T52u+1NJN36okXFKn8cfudbyuHfInHbTHIbEvIhBQyqwZSvPcKc9535vX/iJpvRkqO/4klVx/o8IHDor9nq3Y2rWWpk/3aMYMtz75xKOSkqpFvUGDbLVpk4TFAQAAAADQhFEkAgAAzZbjOFpT+IsW5y3SN5sW6Zu8hVq86Ztaj4+rjiVLp/Qao2uGXK8DsgfEtphAQCmzZ8r31jSlfPi+XHsUSksq9Zkv6bEda7csBU8+TcXX3aDI/gfEds9WKhyW5s936+OP3Zo+3aPvvtv9M6JGjmQXEQAAAAAAlVEkAgAAzYLjONpQvH5HMWiBvslbpMWbFqkgUBDznB6XR2fsc5auGXy99mnXp/4TVC4MFRVGr/1Hkn+XvhFJEyRHLpWdcYZKrr1BkX77xrz21mbTJkszZ5YXhWbP9mjbtrofAXjQQRFdfnnNRwu2ZmecMVaBQLH8/vRkLwUAAAAAkAQUiQAAQJO0qWSTFm9aWFEMWpS3UHklG+Oas1vGnhrYabAGdhqkAzsO0sBOg5Tla1u/SXYWht5+QykfvGcWhnYaL2mEGYpMzVHJ2OsVfPZU2Z33iPVbaDVsW1q82KXp0z2aPt2jb75xyXHqVhiyLEeDB9saNSqsUaPCGjDAVsyPlWrhbr/9DmVnZyo/v1COk+zVAAAAAAAaG0UiAACQdFsDW7R40zcVxaDFeYuUW/RrXHN2SuusQ7sdov5tD9DAjoM1oOMgdUzrGNtkuxaGPnxfrsJajrPrIOl+MxQJ5qhg5AJJGbHdv5XYtk2aPdtT8Xyh/HxXnce2betoxIiwRo4Ma/jwiLKzqXgAAAAAALA7FIkAAEDSfL72U/31s1u0JH9xXPO097fXgR0HaVCnwTqw02AN7DhIXTK7xL9DwraV9sB9Sn30odoLQzs4gyXn0bZyZW814kWlD4gCUVRJifTjjy6tWuXSDz9Ev373nUuRSN23/Oy/f0SjRoU1cmREQ4ZE5OEnWwAAAAAA6oW30gAAICkC4YAu//jieh8hl5nSRgM7DtKBnQZpYMdBGthpsPbM7C4r0eeJOY4ybvyjUidPqr3bHlLkzz1knR2Qu9MGWdpqXC8rG6Ng8ITErq0ZcBwpL8+qUghatcql3FyrzkfH7SotzdGwYWGNHh3RyJFh5eSwWwgAAAAAgHhQJAIAAEmxeNM3uy0QpXnSdEDHA8ufI7TjGUJ7Z/WSy6r7MWQxcRyl33pjjQUixyeFr+sr6xKX3L2+l8f6udp+tt1GRUX/asCFJl8wKP30U3kBaPVqsyBUWBh/4a5374hGjoxo9OiwDj00Ip8vAYtGhaFDh2jjxg3q3HkPff75gmQvBwAAAADQyCgSAQCApPh20yKj7XF5NCB7R0Fox6992vaR2+Vu3IU5jtJvu1VpT/3HDFtS+MIBcq7KkOfA5fJ6Vu5mmjQVFv5Xtp3TkKttNAUF0g8/uCsKQDsLQr/8YtXriLjd8fkcHXFEeVFoxIiw9t6b3UINqbi4WIWFhcrIyEz2UgAAAAAASUCRCAAAJMXiTd8Y7bF9ztZDIx5PzmJ2chyl3/l3pT3xSDTWWXLGW7Jv6SJvu293O0Uk0l2BwLkKBM6Xbe/VgItNvHBYWrOm6hFxq1e7tHlz4ndvdetmq3dvW/vsY6tXr/KvQ4ZElJaW8FsBAAAAAIBqUCQCAABJsWTTYqN9YMeByVnILtLuvVtpD/2flCLpFEkXSc7xkuVx5NbaGsc5TprKyk5TIPAbhUJHSmrg4/DitH27quwIWrXKpR9/dCkUSuyznfx+p6IAtLMg1Lu3rZ49baWnJ/RWAAAAAACgnigSAQAAOY6jkB1S0A4qGClTKBJSWaRMwUiwIhaMhBS0d8QiQYXsoMp26Vve3tHX3tEnsqOPHYr2tcsUigS1cssKYw0D6l0ksiUFZFmlsqzAjtfmL8lSSsqWXa6VSiqrGLNznKtok9xrVshzzE/SCZIGSmpffpfaSibB4JEKBH6jYPBUOU7TO66roEBaudKtlStd+uUXafHiVP3wg0sbNya+iNW5s7kjaGdBqGtXR66mXTMDAAAAAKDVokgEAEAjcRxHQXtn4cQspgQr2qHyIs2OgkvlIovRZ8c8FX3toCyPrcKSkmrn3tmnSuFnR99YWZL8HinVu+OrJ9re9bXfK7VNjbYP7h69nuaVjur5rLzu/1Yp+FQu/kTbdVtzmzZ16JQqqWPdvt9IZK8dx8mdK9veu26DGtjWrdKKFeXFoJ2/VqxwadOmytWZ+H7083od9exZ9Yi43r3tuuUZAAAAAAA0KRSJAAAtiu3YVQouFYUSO1hLkSW6y2XX4olZrNmlb6W5jViNhZ9Qg37vtRVrMlKkjp6q16vra7TrMMaXsJ8mXkzURAnnOOkqKxuz4zi5w5Ws4+S2b5dWrHBV7A4qf534nUHt25tHw+382r27Iw8/PQIAAAAA0GLwNh8AUG+2Y0d3tUQqFUYqHVdWeUfMrgWUmo4rC1Xa5WIcV1ZN4SdkB1UWDipolylsh5Odnopijd8jpaZI7WsqwNTSrkuxpnI7ccUa7BQMHq1A4DyVlZ0qKaPR7ltYqB07gtwVhaCVK11avz5xxSCXy1GPHo569zYLQb172+rQwUnYfQAAAAAAQNPFPycBQBMVtsNVd8TsOKosPeTVxs0FCu4sruw8vqzK82KqeSZMDUeQVVecMXbE2NG5Ik4k2empE6NYU5+CTHVtdzXHp1GsaZIcJ1WOnSKVSq7tQTkFpbICkkol1fS1VLLd6Yr03E/BAYcrskdfhUJHyrb3atC1FhVJ33+/83i46HFxa9cmrhiUklJeCBowwK0ePcq0zz62+vSx1aOHLZ8vYbcBAAAAAADNEP+MBaDVchynvBBTlyPIjILJLgWXGo4rC1U63qyiWLOj0FLzUWjRuWzHTnaKEqauxZo6FW/cdR9DsSa5HMdf8Uva+Tp1l9eVr9X8unycr7wAVEM/FQaV8t50+d6cppTZM2WFy3eVWbWs0W7XTmUnn6ayMWcqdPiRkttdfqEssbkoLpZ++MFlHBW3cqVLv/6auGKQ1+uoVy9bfftGf/XrZ2vvvW15vVJ2dqby84Ny2CQEAAAAAAB24J/PADQLETuiDUUb9N2m1cor2ai8kjzlleRpU8lGFQQKyp8JU01xpvKOmJ07ZnYWZxy17H8tdVmSz11eLKnP15S69N1NsWbXtp//2yRVzcWanUUXX5XiTc3FmpoLPuZrn2ovzyRAMCjPsiXyLJyvlE/mKGXmx7LKdl/dsTPbKHjiySobc4aCRw+XvN5a+zuOVFYmlZRIJSWWioutiteVv+68tvPrxo3lhaFff7XkOInJh8fjqGdPsxDUt6+tnj3t3X0rQBX33fdveb2WQqGW/f9DAAAAAED1+Gc7AEnjOI62lm0pL/aU5u0o/uwsAEVfbyrJ0+ZAfpPfWVOXgkyKO46iTQzj3Il9lj3iVPdiTbQtpdaxWFPet127Dtq8OdK4xZrG4Dhy/fKzvAvny7NwvrwL5suz9Ns6FYUkKZSSph/3P0lL9ztLS7seq+1Bv4pnWyp5r3Kxxyzy7GzbduPn0O12tPfeVYtBvXrZSklp9OWghTr22BN27DIrZJcZAAAAALRCFIkAJFxRqEibduz02Vns2bSz4FNRDCr/GrJDMd2jckGmTjtf6lO0iWGch4JMs1H7UWe1FWsqx2sv+FQe09DFGsuSpEw5TtP8x97KO3JqKsiUlFiK5G9Vx5/mK2fN19pz/TztvWm+soL59bpfQD69q5P0ss7Ru8GTVLowTVrYQN9cHFwuRz16OOrbN1JRCOrb11bv3jwzCAAAAAAANCyKRADqwFYwUqiCwFptLl2rguAGbS1br21lG1UY3KTC0GYVh/JVHCpQaWSrHJVVLa74pZ4Z0r67KcjUtdhDQaZlqN9zaWou1mRmttX27dXNl5xiTXNWUyGn/Ci16o9Wq77YU/216nbkeBXUAH2rQ/WVDtVXOk5fqa++j2n9QXn1oY7TKzpbb+tUFapNvClJGMuquRjk9yd7dQAAAAAAoDWiSAQ0ObakMllWmSwrKGmLXK7NkoI72mXVfDVfl/et3C+441o0LpUp7BQr4hTLdkrlqFSygnJZQbmskNyuiLwuW94dz3Hv2ui5QEMqL5r4JKUYX6vGUlRejNkZ8+0Si/XZNqmSUiTFX+2zLCkzM1PBYNPcPdMQaivkeL3S+vWeioJOdbt0osWe6p+tE4k0ZBHN0d76qaIgdIjmabAWyq+6HRtXnRXqq690qGbrGL2pMdqqdglcb/V8Pkfp6Y7S0qS0tOq/pqc7atPGUe/e5UfF9e5tKzW1wZcG1MvixYuUluZVSUlIAwYMSvZyAAAAAACNjCJRDFauXKmnnnpKX331lQoKCtS2bVvtv//+Ou+883T00Ucne3mol50FmapFlMoFleoLMruO21mYKasyV9U5Q7XMGa6yyvbtGy4DnGTUOGoqvlRXpKm5IFP+VUpRenobFRU5su36jt8Z94rdNA3LcaRgULvsoqm+IFNcXH0hZ3fjai/kNJ1KRJa26hDNM4pCnbQp5vny1HHHTOW/5uugGotC5YUcS6mpdq0FnbpeS08v/5qaKnn4CQotxAUXnKv169cpJ6eLFi9ekezlAAAAAAAaGf/EUU8zZszQNddco1Ao+hyVTZs2adasWZo1a5YuuOAC/eUvf0niCls2t3upUlI+kmUV1VCQiRZf6lbkqVqQQfNXU/Fl9wWZlCoFmbqN91ZTkNm1T2ILMpYlpadnKhBoPTtnki0SkT75xK1589z1Kug07I6cpsmSrbM0VSfqPR2qr9RPK2OeK+jy6ed2g/TzHgdrbdeDtbHHwQp22Utp6Za6pDn6TZp0WZqjtLSSioLOroUcr1fKzs5Ufn4x/60AAAAAAABUgyJRPSxfvlx//OMfFQqFdMAB/9/efcc1db1/AP+EPRVUXDhrxQFonbhxYq1+Ha3FuotWnNQ966irWq2r4Kziwr0nqAhqHYUqiuJCVFCobEGGzOT3B79ciIQpJJB83q+XL0PuucmTQ0jOvc89z7HGnDlz0LBhQ4SGhmLbtm3w9PTE/v37Ub9+fQwfPlzZ4aocbe2bqFixL0QinukrS9IzNZAp1oJYov3/CRI9aIj0oCEygAb0802+FJyQ0RbuA3RRsaIp4uIy/n/2jGISMkQAsHGjDn7/XX3n3enq5pxhk7ukWs5tQ29NRYf724v1PBlfNkRGi1ZIb9kaGa1aI6OpFUx1dGAKILsIFpP7REREREREREQlhUmiIti0aRNSUlJQt25d7N27F4aGhgAAU1NTuLi4YNq0afDw8MCff/6JAQMGwMjISMkRqxZd3dNqnyCSznhJTEvDh7RUpGUCqRlAamH+z3H70/3SMgFtDWPoa5lAX8sUhlqVYKRdBcbaZqigawYTneow0auBynrVYaxdFSKRbo7kjWISMiIRABgjI4OzZ0jxzp4t+1+XOjpZyRpjYxH09DLzXCMnv/Vz8iq/VpTSapUbHS1UO3HlylnJoJats/5v0RISk9JfS4iIiIiIiIiIiLKV/bNeZcTLly9x7do1AMD48eOFBJGUSCTCvHnzcPnyZcTFxeHKlSsYNGiQEiJVXenpraGvv0Nhz5ddQkz+2i75rfdS9JJlOnJn2AC6qFy5MqKj0/7/vqyEjNuTvZhxzanA11BR1wRV9auiqkE1VDWoCjMD6e1qqFqhKsz+/3ZlvcrQ0ij8xwGTNKRuWrbMxNOnmp/9ONJETkHr4BQmoSMvkSMSScurJSvt7zS9dVvoXrkkc59EVxcZVs2Q3io7KSSuW0+a/SUiIiIiIiIiIiVhkqiQ/v77bwBZyaBu3brJbVOjRg00adIEjx8/hqenJ5NEJSw1dQg+fBBDR+cqADEKU7JMmnyRn9D5dP9PS5ZpKOV15iSdPQMkyNw/vMkoaIg0cPu/mzDQMshO/ORIBpnpV4Welp4ywiZSOStWpKJ+fQkePdKAnp5s0iavZE9eiRxVl7DdFRkuG6ERHYOMJk2Q0bI1MiytAR0dZYdGRERERERERESfUIPTVSXj6dOnAICaNWuiUqVKebZr2rQpHj9+jMePHysqNDUiQmrqMKSmDlN2IEonEokwrMlIDGsyUtmhEKkFQ0Pg55/TlB1GuSAxMkbyvEXKDoOIiIiIiIiIiApB+VMlyomwsDAAQK1atfJtV7NmTQBAeHg4MjK4uDYREREREREREREREZVNTBIV0vv37wEAFStWzLedsbExAEAikeDDhw+lHhcREREREREREREREVFxsNxcIaWmpgIAdHV1822np5e9BkxaWtFKE4lEXMNb+vrVvR+k2B/Z2BfZ2Bey2B/Z2Bey2B/Z2BfZ2Bd5U/c+4XtDFvsjG/siG/tCFvsjG/tCFvsjG/siG/tCFvsjG/tCFvsjm6L6gEmiQtLU1Cz156hUybjUn6O8qFyZfZET+yMb+yIb+0IW+yMb+0IW+yMb+yIb+yLL8+fPIJFIIBKJhBnx6o7vDVnsj2zsi2zsC1nsj2zsC1nsj2zsi2zsC1nsj2zsC1nsD8VhkqiQ9PX1ARQ8OyglJUW4XdCso0/FxiZALC56bKpEJMr6AIiJSYBEouxolI/9kY19kY19IYv9kY19IYv9kY19kY19ISurPyogJiYB0dEJyg5HqfjekMX+yMa+yMa+kMX+yMa+kMX+yMa+yMa+kMX+yMa+kMX+yKahoZiJJUwSFZL0ysqEhPwPnqXrEGlqaha4ftGnJBKo/Rtfin0hi/2RjX2RjX0hi/2RjX0hi/2RjX2RjX0hi/2RjX0hi/2RjX2RjX0hi/2RjX0hi/2RjX2RjX0hi/2RjX0hi/2huNevoZinKf/q168PAPjvv//ybffu3TsAQLVq1aChwe4lIiIiIiIiIiIiIqKyiTOJCsnCwgIA8PbtWyQmJsLIyEhuuydPngAAmjRporDYiIiIiIiKY+tWF2RmpkJTUxcTJkxRdjhERERERESkYJzqUki2trYAgMzMTFy7dk1um3fv3uHp06cAgM6dOysqNCIiIiKiYtm61QVLly7F1q0uyg6FiIiIiIiIlIBJokKqXbs2WrVqBQBwdnbOtTaRRCLB6tWrIRaLYWpqigEDBigjTCIiIiIiIiIiIiIiokJhkqgI5s+fDw0NDQQHB2PYsGG4efMmYmNj8fjxYzg5OcHDwwMA4OTkBAMDAyVHS0RERERERERERERElDeuSVQE1tbWWLlyJRYtWoTAwECMHTs2VxsHBwcMHz5cCdEREREREREREREREREVHpNERfTtt9/C0tISu3btgo+PD2JiYmBgYAArKysMGzYMPXv2VHaIREREREREREREREREBWKSqBgaNWqENWvWKDsMIiIiIiIiIiIiIiKiYuOaRERERERERERERERERGqISSIiIiIiIiIiIiIiIiI1xCQRERERERERERERERGRGuKaREREREREaqpZs+aoW7cOKlY0VXYoREREREREpARMEhERERERqSk3tyOoUsUY0dEJkEiUHQ0REREREREpGsvNERERERERERERERERqSEmiYiIiIiIiIiIiIiIiNQQk0RERERERERERERERERqiGsSERERERGpqREjhiA+/j0qVjTF/v1HlB0OERERERERKRiTREREREREaurhQ3+8e/cfatSoqexQiIiIiIiISAlYbo6IiIiIiIiIiIiIiEgNMUlERERERERERERERESkhpgkIiIiIiIiIiIiIiIiUkNMEhEREREREREREREREakhJomIiIiIiIiIiIiIiIjUEJNEREREREREREREREREaohJIiIiIiIiIiIiIiIiIjWkpewAKJtIlPVPnUlfv7r3gxT7Ixv7Ihv7Qhb7Ixv7Qhb7Ixv7Ihv7QpaRkRGMjY1hZGSk9n3C94Ys9kc29kU29oUs9kc29oUs9kc29kU29oUs9kc29oUs9kc2RfWBSCKRSBTzVERERERERERERERERFRWsNwcERERERERERERERGRGmKSiIiIiIiIiIiIiIiISA0xSURERERERERERERERKSGmCQiIiIiIiIiIiIiIiJSQ0wSERERERERERERERERqSEmiYiIiIiIiIiIiIiIiNQQk0RERERERERERERERERqiEkiIiIiIiIiIiIiIiIiNcQkERERERERERERERERkRrSUnYAlMXT0xOTJ0/GoEGDsHr16gLbe3l54cCBAwgICEBSUhKqVq2K9u3bw8HBAV9++aUCIlacJ0+eYM+ePfD19UV0dDQMDQ1hZWWF7777Dn369IFIJFJ2iArz+vVr7N69G7dv30ZERAS0tbVRr1499OzZE6NGjYKRkZGyQyx1J0+exPz584u0z5QpU+Dk5FRKEZUNYrEYp0+fxtmzZ/H8+XMkJCTA1NQUbdu2hYODA6ysrJQdokJcuXIFU6ZMKbBd79698eeffyogorJp5syZOH/+PNq2bYv9+/crOxyFCQgIwN69e+Hr64uYmBhUqFABFhYW6N+/PwYOHAgNDfW6dubFixfYv38/fHx8EB4eDgCoVq0abGxsMGrUKDRs2FDJESpPUcdl5c3z58+xc+dO+Pj4IDY2FiYmJrCyssKwYcPQpUsXZYenVCtWrMD+/fuxatUqfPvtt8oORymuX7+OEydO4MGDB4iNjYWOjg7q1q0LW1tbjBo1CpUqVVJ2iApx+fJlHD16FI8ePUJSUhKqVKmCFi1awN7eHu3bt1d2eEqXnJyMQYMGITg4WC3G2lLSz4iCLFq0CCNGjFBARMqVmJiIffv2wdPTE2/evEFqaipq1qwJW1tbjB07FtWqVVN2iKVu3rx5OHXqVJH22bdvH2xsbEoporLhzp07cHNzg7+/P+Li4mBoaIjGjRtj0KBB6N+/v9qNu69cuYJjx47h0aNHSEhIQKVKldCmTRuMGjUKzZs3V3Z4paooY6v09HQcOnQIZ8+excuXLyGRSGBubo6ePXvCwcEBJiYmigm6lBVnvPnff/+hX79+MDExgZeXVylHqBhF6YfY2Fi4ubnh2rVrCAkJQWpqKkxNTdG8eXN8//33sLW1VVDUpaMofREZGYndu3fj2rVrCAsLg46ODurUqYNevXphxIgRMDY2LlYMTBKVAW/evMHixYsL3X7t2rXYuXOnzH1hYWE4fvw4zp49i99++w3/+9//SjpMpdixYwfWr18PiUQi3BcXF4ebN2/i5s2bOHfuHNavXw99fX0lRqkYHh4emDt3LlJSUoT70tLS8PjxYzx+/BgnT57Erl27ULduXSVGWTapevIsPj4eEyZMgJ+fn8z9kZGROH/+PNzd3bF48WL88MMPSopQcQICApQdQpl34cIFnD9/XtlhKNzu3buxdu1aZGZmCvfFxMTgzp07uHPnDg4fPowdO3aozIFHQfbv34/Vq1cjIyND5v6QkBCEhITg5MmTWLhwIYYOHaqkCJWnqOOy8ubq1auYOnUq0tPThfuioqLg7e0Nb29vjBw5EgsXLlRihMrj6emJAwcOKDsMpcnIyMC8efNw7tw5mfvT09Px5MkTPHnyBEePHsXmzZvRokULJUVZ+tLT0zF79my4u7vL3P/u3Tu8e/cOFy9exJAhQ7B06VK1uljtU6tXr0ZwcLCyw1C4x48fKzuEMuPZs2cYN24cIiMjZe4PDg5GcHAwzpw5g7/++gvNmjVTUoRll6GhobJDKFW///47XF1dZe6Li4vDP//8g3/++Qfnzp3D5s2boaenp6QIFSclJQVz5szBpUuXZO6PiIjA+fPncf78eUydOhWTJk1SUoSlqyhjq9TUVPz000/w9fWVuT8oKAhBQUHCOS8LC4vSCFVhijPeTElJwaxZs5CUlKQyx6tF6Ye7d+9iypQpeP/+vcz9kZGRuHLlCq5cuYJBgwZh5cqV0NTULI1wS1VR+sLX1xeTJ0/Ghw8fhPtSU1OFc8MHDx7E5s2bi/XdyySRkoWEhODHH39ETExModofPnxYSBANGDAAY8eOhZmZGQICArB27VoEBgZi/vz5aNCgAZo2bVqaoZe648ePY926dQCA6tWrY/r06ejYsSMyMjJw+fJlbNy4EV5eXpg4cSL27Nmj3GBL2cuXLzFr1iykp6ejWrVqmDVrFtq0aYPk5GS4u7tj+/btePv2LSZMmIAzZ85AR0dH2SGXmv79+6N37975tomKisIPP/yA9+/fo02bNhg+fLiColM8iUSCiRMnws/PDyKRCKNHj4a9vT2MjIxw//59rFmzBmFhYVi6dCmaNm2q8gdpT548AQD069cPy5Yty7OdlpZ6fv2Fh4dj6dKlyg5D4by8vITZIJaWlpg6dSqaNm2K+Ph4nD9/Hjt37oS/vz9mzJiR64BWFXl5eWHFihUAAAsLC0ydOhVfffUVPn78CH9/f6xfv1743JBeDawuijouK2+ePHmCGTNmID09HdbW1pgzZw4aNmyI0NBQbNu2DZ6enti/fz/q16+v0t+d8nh5eWHatGkQi8XKDkVp1q1bJySIevTogZ9++gn169dHVFQUrl+/ji1btiAmJgYTJkzA2bNnVXaGwLp164QE0ddff40xY8agVq1aCAsLw65du+Dh4YEjR46gRo0amDhxopKjVY5r167hyJEjyg5D4cRiMZ49ewYAWLJkCQYMGJBnW1U+HgOyjrdGjx6NuLg4GBsbY/r06ejatSsyMjLg7e2NjRs3Ii4uDpMnT4a7u7tKX7S3bNkyLFq0KN82d+7cwZQpUyCRSODo6KjSVR6OHTsmjKe/+uorTJs2DQ0bNkR4eDh2796N8+fP4+bNm1i2bBl+++03JUdb+hYvXiwkiFq0aIGpU6eicePGiImJweHDh7F//35s2rQJEokEkydPVnK0JauoY6v58+fD19cX2tramDJlCvr16wcdHR1cv34da9euRWRkJCZMmIDz58/DwMCglKMvHcUZbyYlJcHJyQn37t0rxcgUqyj9EB4ejgkTJiAhIQEmJiaYOnUqunTpAl1dXbx48QKbN2/G3bt3cerUKVSpUgWzZs1SwCsoOUXti0mTJiEhIQFVqlTBtGnThNnt//zzD9avX4/IyEhMnDgRFy5cKHpCUUJKc/nyZUmrVq0kFhYWwr+5c+fm2T45OVnSrl07iYWFhWTatGm5tsfHx0t69eolsbCwkDg4OJRm6KUuMTFR0rZtW4mFhYWkc+fOkvDw8Fxt7t69K2natKnEwsJCcurUKcUHqUALFiyQWFhYSJo3by55+fJlru1Hjx4V3kNnz55VQoRlR2ZmpmTYsGESCwsLSfv27SWRkZHKDqlUHT58WPjdHzp0KNf28PBwScuWLSUWFhYSJycnJUSoWB06dJBYWFhI9u3bp+xQyhyxWCwZPXq0zHfOiBEjlB2WQnzzzTcSCwsLiZ2dnSQhISHXdldXV6FP7t+/r/gAFUzaH71795YkJibm2h4XFyfp3r27xMLCQtK3b18lRKgcRR2XlUeOjo4SCwsLSa9evXL97sViseTnn3+WWFhYSNq2bSv3b0UVZWZmSjZt2iRp3LixzO/+xIkTyg5NocLDw4Vx9cyZM+W2efjwodBm6dKlCo5QMcLDwyWWlpYSCwsLyfTp0+W2mTBhgsTCwkLSunVrSUpKioIjVL6YmBhhvCX99+effyo7LIUICgoSXvOzZ8+UHY5SzZgxQ2JhYSH56quvJA8fPsy13dvbW+grNzc3JURYdkREREhsbGyEsXdmZqayQypVPXv2lFhYWEj69esn9zNS+t5p1KiR3PM8qsTPz0/4OxgzZowkLS0tV5sdO3ZILCwsJFZWVpLXr18rPshSUJyx1cOHD4V2Bw8ezLX98ePHwvfz1q1bSzP8UlHc8WZgYKDk66+/ltmnW7duCoq65BWnH5YsWSKxsLCQWFtby/3uFYvFksmTJ0ssLCwklpaWkoiIiNJ8CSWmOH3x66+/Cn3x9OnTXNsfPnwoPN727duLHJN6FQEtI169eoWJEydiypQpSEhIQO3atVGhQoUC9ztz5gxiY2MBADNmzMi1vUKFCsJaHLdu3UJoaGjJBq5A169fR1xcHABg7ty5cq9UbNWqFfr16wcgqyydKnv48CEAoE2bNvjiiy9ybR80aBB0dXUBAP7+/gqNraxxdXXF3bt3AWRd1WVmZqbkiEqXtCZ6p06d5JaTq1atGvr27QsNDQ08f/5c0eEpVEREBKKjowEA1tbWSo6m7Nm7dy/u3LmDBg0aqHzd65xevXqFoKAgAMDo0aPlXsma80pgVf8MlZZrAIDx48fLLXdSsWJFjBkzBkDWukVhYWEKjVHRijsuK29evnyJa9euAZD/uxeJRJg3bx40NDQQFxeHK1euKCFKxfr7778xYMAAbN68GWKxGJaWlsoOSWk8PT2F8pPTp0+X28ba2ho9e/YEAOG9pGq8vb2FUox5XdHdv39/AMCHDx/w+vVrhcVWVixcuBDR0dFquWaXtNScgYGByq0DXBTR0dHCbLuJEyfKHXd37doV9erVg7a2ttqX6Fu4cCHev38PQ0NDrFmzRqXX4omLi8ObN28AZH1WSs9R5CQtZSyRSITzHKrqzJkzAABtbW2sWLEC2traudqMHTsWtWrVQlpamkpUyCnu2Gr37t0AgFq1asHe3j7X9qZNm2LgwIEAsmarlSfF6ZP4+HisWrUKgwYNwqtXr2BgYCD3XGB5Utz3hnQmXt++fdGoUaNc20UiEaZNmwYgq2TwrVu3Sizm0lLcvpAen3Xv3h2NGzfOtd3a2lp4nxTnvIbqfjuVYUuWLBEWGfv6669x7NixQi0qdf36dQBZpWFq164tt023bt2E+otXr14toYgVTzqQ1NbWRo8ePfJs16lTJwBZJz5U+SSWdCD56doRUiKRSPi9l8f6myUlNDQULi4uAIDevXsLJzJU1YsXL/DixQsAwLhx4/Jst3DhQgQEBOSqg6xqcn5ulPdymyXtxYsXWL9+PbS0tLBmzRq5B2yq6osvvoCvry8OHDiAvn37Fthe1UsRhoaGComy/MpP5lzf7tO1BlRNccdl5c3ff/8NIGvM0K1bN7ltatSogSZNmgDIShqoup9++gmBgYHQ1taGk5MTNm7cqOyQlCYyMhJ6enqoUqUKzM3N82wn/WxQ1c+FH374AdevX8eePXvQoEGDAtur+nfGp44dO4arV6/C3Nwcv/zyi7LDUThpWeOmTZuq9THXpUuXkJmZCX19fYwYMSLPdmfPnkVAQIBalBTLy8WLF4XzODNnzkSNGjWUHFHpypkAy+vcRc5EiSonzIDs41Nra+s8f/caGhro0KEDAODGjRsKi620FGdsJZFIhHFqznOan5KeGwwNDRVKf5YHxemTffv2Yc+ePUhPT0fTpk1x+PDhcn+hZ3H6QboGkUgkyvfYtU6dOsLt8jBGLe4xiKenJ06dOpXnBV05FWeMql6j2jLE0tISM2bMEJIchSH9EMyvfq2xsTFq1aqFkJCQcn3FTnx8PADA1NQ038UMK1WqJNx+/vx5vge15VmzZs3w7Nkz+Pn5ITg4GPXq1ZPZ7u7ujuTkZABAy5YtlRBh2bB27Vp8/PgRenp6mDdvnrLDKXXSK6+0tbVz/d7T09OFAbiq10SXkn7mWVhYwMfHB0eOHIGfnx8+fPiAypUro3379hg7diwaNmyo5EgVKy0tDbNnz0ZqaiqcnJxUugZ6XipWrIjWrVvnuX3v3r0AZA/SVFXXrl1x7949JCUl5fv9GhISItxWxVk1nyrOuKy8efr0KQCgZs2aMuOnTzVt2lRY+FTViUQi9OrVC9OmTUODBg3K9Sz8zzV9+nRMnz4diYmJ+baTfjZUrFhREWEpRfXq1VG9enW529LT03Hw4EEAgLm5ea4xuSp78+YNfvvtN4hEIqxatUql15jJi/RzsUmTJjh69CjOnj2Lp0+fIj09Hebm5ujRowfGjh0LU1NTJUdauqTHINbW1rnWBcl5DKJOFyXJk5qairVr1wLIOj6RV/VB1VSoUAH16tVDcHAwLly4gLFjx+Y6Fj1x4gSArGNYVa/+ID2vVbNmzXzbScdlYWFhSExMLNefr8UZW4WGhuLDhw8AkO+MipwXggYEBMidSVEWFXe8aWZmhokTJ2LIkCEqcVFKcfrB1NQUd+7cQVpaGiQSSZ7tpDMYgfJx7Frc94Senl6+F0TfunVLqBpSnOPa8v8uK4eWL19e5AOKzMxMhIeHA0CBiZCaNWsiJCSkXB/oSkugSBMfeZF+6QIQ+kcVOTo64vLly4iLi8OYMWMwc+ZMtGrVCqmpqfD09BRmz3To0AG9evVScrTKERAQAA8PDwDA8OHDCxyIqQLpLCJzc3Po6Ojg2bNn2LlzJ27evIn379/DyMgI7dq1w/jx4/O96kJVSA/cAwMD8dNPP8lsCw8Px6lTp3Du3DksWrRILQ7SpDZt2oSnT5/C2toaEyZMUHY4ZUJaWhpiYmIQGBiIQ4cOwdvbGwAwYcIE1K9fX8nRKYa8MnNSYrEYR48eBQBUrlxZ5fukOOOy8kg647pWrVr5tpN+f4aHhyMjI0MlDkrz4u7urvLv76LK78RURESE8HnZqlUrRYWkdMnJyYiMjISfnx/27NmD58+fQ1tbG7/++qtK/33klJmZiTlz5iA5ORmjR4+GjY2NskNSOIlEIswkOnz4sFCWUOrVq1d49eoVTpw4ga1bt+Krr75SQpSKIT0GkX53Xr16FW5ubnjw4AGSk5NhZmaGnj17YuLEiXLLxquLgwcP4r///gMAzJo1S21mn82cORNTp07Fixcv4ODgACcnJzRs2BBRUVE4cOCAMMacOHEiqlatquRoS5d0vJ2UlJRvu5zntSIiIsp1kqg4Y6ucVYHyG6eamZlBW1sb6enp5ep8Z3H6ZNCgQRg/frxKXfD7OePugvrh8OHDwu3yMEYtqWOQzMxMoczn2bNnhVKM7du3F8ozFoV6jGrLmOKciIiPj4dYLAZQ8JV70hIp0kx8eWRhYQEASExMhL+/f57TKn18fITbBV35WJ7Vrl0bbm5uWL58OXx8fHKtSaWvr4/Jkydj/PjxKj9lOy+7du0CkHXFmnQtDVUXFRUFADAxMcHJkyexePFimQPWxMREeHp6wtvbGwsXLsSwYcOUFapCSJNE6enpaN++PcaNG4fGjRvj48ePuHHjBpydnREbG4slS5agcuXKapFQ/ffff+Hq6go9PT38/vvvanMyqyBbt27Fli1bhJ+1tbWxdOlSfPfdd0qMquzYtWuXMHv5hx9+UPnvFXVIEAHZ5RoKO46USCT48OFDvrOOyjsmiApPIpFg8eLFSE1NBQCVH1PkNG7cOGG9SyCrLOPGjRtVOgnwqe3bt+P+/fto0KABZs6cqexwlCIkJEQ43szIyMCQIUNgb28Pc3NzREVF4dy5c3B1dUVsbCwcHR1x4sSJPEvEl3fSUj4VK1bE4sWLceTIEZntUVFROHToENzd3bFt2za0aNFCGWEqVUZGhrC+TNOmTWFra6vcgBTIzs4OLi4uWLNmDe7evYvRo0fLbK9RowamTZtWrBOY5Y2FhQWePHmC+/fvIyUlJc9Z/L6+vsLt8n5eqzhjK+kYFch/FoiGhgYMDQ0RFxdXrs53FqdPCrqoqzwqrXH3/fv3hVnebdq0Ec4nl2Ul1Re+vr748ccfZe4bOXIkZs2aJXcNtILwbFERbdmyBZs2bSrSPoMGDcLq1as/63mlB2QA8i0PA2RP6865jzJ8Tl9169YNenp6SElJwW+//Ya9e/fmet3Pnj3DqVOnhJ8/vZqrLCmJ901CQgIMDAwgEolyTbP8+PEjHj16hNevX5eLKbcl/XcUFhYmrLfz3XffoUqVKp8do6J8Tl9Ir0h68+YNFi1ahKpVq2L27Nno3LkzNDU1cefOHfz+++8ICQnBsmXLUKdOnTJdSulz+iIxMREGBgbQ1tbGgAEDsGLFCohEIqHdsGHD0KlTJwwePBjx8fFYvnw5bG1ty/SVOZ/7d5KYmIi5c+dCLBZjxowZhVpboawq6c8M6VWdUunp6di4caNw0qesK82xyKVLl7BhwwYAWcmTsWPHFitGRVHWuKw8ko4LCyr/k3O8lZaWVqoxUfmxatUqXLt2DQDQr18/tGvXTrkBKdCn3xnv3r3Dr7/+ioULF+ZbylRVBAQEYMuWLWq5rmFOERERqF69OiIjI7Fq1SqZE9ympqaYOXMmrK2t4eTkhPj4eKxduxZ//vmn8gIuRdJjkNOnTyMqKgqtW7fG9OnTYW1tjaSkJLi7u+OPP/5AXFwcJk2ahDNnzqj8jJFPXbx4Uah04ujoqORoFE96bCZPTEwM/Pz80KVLF5W+EAUA+vTpg9OnTyMuLg7r1q2Tu5bb0aNH8fLlS+Hnsnxeq7SUx/OdVDa8evUKkydPRmZmJnR1dbFw4UJlh6RQn45RgeySnnPnzi1yoki1Lw1VIap+Fe+nKlWqJAymHjx4gKFDh+LatWuIjY3Fu3fvcOjQIYwePRrGxsbCtO3iZEnLi4sXL2LUqFHw9vaGjY0NDhw4gIcPH+Lff/+Fi4sL6tevjxs3bmDYsGG4d++essNVODc3N2RmZkJTUzNXmTFV9vHjRwBAbGwsTExMcPjwYXzzzTcwNjaGgYEBevTogUOHDqFq1aqQSCRYs2aNkiMuPUZGRrh06RL8/f2xbNkymQSRVJ06dYRyaxEREcLimKpq+fLlCAsLQ7t27TBq1Chlh1OmTJkyBffv34efnx927NiBRo0aITIyEosXLxau+lRH7u7umDlzJjIzM2FkZARnZ+d8y9JR+aIuZW6oZEkkEqxatUpYu83CwgLLli1TclSKtWvXLjx69Ah37tzBihUrYGJigqdPn2Ls2LHw8/NTdnilKiUlBbNnz0Z6ejomTpyolusaStnY2OD69evw9/fPcwaEnZ0dunXrBgC4cuWKTAkpVZKSkgIga8aQjY0N9uzZg9atW0NXVxeVKlXC8OHDsWPHDmhoaCA2NhY7duxQcsSKJx1P1qtXD71791ZuMAq2YsUKzJkzB0+ePMHQoUNx8eJFPHr0CDdu3MDChQuho6ODI0eOYMSIEYiOjlZ2uKWqa9euaN++PQBg3759mDp1Kh4+fIj4+Hi8fPkSf/zxB5YsWSJTllGVz2vlhWNUKo6goCCMHj0aMTExAIClS5eWi4vmS1KXLl1w584dPHr0CCdPnoSdnR2Sk5Oxf/9+/Pzzz0V+PM4kKqKhQ4cW+UteWrbjc+S8CqOgjLl0e0EZ+NL2uX01adIkRERE4MiRI3jy5AnGjx8v07Zq1arYvHkz7O3tAeS/voKyfU5fxMXFYdGiRUhPT0fXrl2xdetWIWmoq6uLXr16oV27dvjhhx8QFBSEBQsW4OLFi2X6i7Yk/44kEgnOnz8PIOvgraA1u8qaz+kLfX194b5x48bJrflduXJljBkzBqtXr8bz588REhKCunXrfl7QpaQk3hcFve979OiB33//HUDWors9evQoWpAK9Dn94eHhgdOnT8PY2BirVq2SmzQrT0r6uzdn+RdbW1u0bt0agwcPxqtXr/Dnn39iwIABZXrR6dIYi7i5uWHlypUQi8UwNDTE9u3by8VUfWWNy8oj6XdGQbODpCf/AC46ru7S0tLwyy+/4OzZswCABg0awNXVtUyPuUvDF198ASDrIrbvv/8ezZs3x+DBg5GSkoI1a9bI1MFXNWvWrMGrV6+4rmEOBc1C79GjB7y9vSEWixEQEICOHTsqKDLF0dPTE9YOnjdvntyT2m3atIGtrS28vb1x+fJltbq6+/Xr10IZ7IEDB6rVBb+3b9/G/v37AWStTZRzFlW1atUwcuRItGnTBkOHDsXLly+xfv16/Pbbb8oKVyE2bNiAiRMn4v79+/Dw8BDWUZb66quvMGbMGOGEbl4zsFRZzvMa5eV8JynX3bt3MXnyZMTFxQEAFixYgEGDBik3KCUwMzMTbltaWsLZ2RmzZ8/G2bNn4eXlhVu3bhVpHMIkURGZmpoq5cSRoaEhNDU1kZmZiYSEhHzbSmtzmpiYKCCyvH1uX4lEIixbtgzdu3eHm5sbHj16hI8fP8Lc3Bx2dnZwcHBAZmamUHot5x9HWfM5fXHx4kWhLu38+fPlDjKNjY0xa9YsTJgwAcHBwfjnn3/K9AFJSf4dPXjwQKiL3bdv3xJ5TEX6nL7IeZImvwWE27RpI9wOCgoqs0kiRXy+1qhRQ7gdGxtbqs/1uYrbH5GRkViyZAkA4JdffhEWoS/PSvu9YWhoiIkTJ2L27NlISkqCj48Pvv7661J7vs9Vkv0hFovx22+/CQf0JiYm+Ouvv9CsWbMSefzSpqxxWXkkTY4VdhypqalZ4PpFpLri4uIwZcoU/PvvvwCyDjx37typ8qWBCsPCwgL9+/fHsWPHcP/+fcTGxqpkv/z99984cOAAdHV1ua5hEZSnsWZxGRoaIjk5GcbGxmjatGme7dq2bQtvb29EREQgMTERRkZGCoxSeS5fvizcLo/Hp5/j6NGjALL+DvIqWdy4cWP88MMPcHV1xZkzZ7Bo0SKZJIGqMTU1xf79+3H06FGcPn0aQUFB0NDQwJdffomBAwfC3t5eKJ0PlO3zWqUl5zpE+Y1TxWKxUO6S43/1dfbsWfzyyy9IS0uDhoYGli5dKkweoKwEvfQCr6tXrzJJpIo0NDRQu3ZtBAcHy605mNO7d+8AQCVODAJZU3S7du0qd9utW7eE26q6+HBwcDCArKsX81tcu23btsLtV69elekkUUmSDsK1tbVhZ2en5GgUK+dihvld1ZjzgEzVa/dKJJJ8Z83krPGsqgcjN2/eFK6omTdvHubNm5dnW19fXzRq1AhA1noT3377rSJCLJMsLS2F26GhoUqMRHE+fvyIGTNmwMvLC0DWDKu//vpLZb9P1V39+vXh6+tb6HFktWrV1OrqZ8r25s0bjBs3ThiDdu7cGZs2bVK7GUT5sbS0xLFjxwBkfWeoYpLowoULALLGjt98802+bV1cXODi4gIg64SEKi64LcWxZtYxSFRUVIGzTXMeg6SkpKhdksja2hp16tRRcjSKJf3eaN68eb4VHtq2bQtXV1dkZGTgzZs3wvGIqtLW1sbw4cMxfPhwudufP38OIOucj7Iv9laGnOe5/vvvP7Rq1Upuu6ioKOEzNmdCntRHzvVo9fX1sW7dujJdHUYZqlevjipVqiA6OrrI5zV45FeOSL84nzx5kmebhIQE4U3QpEkThcRVmuLi4pCZmZnn9ps3bwLIuvJAWgpC1Ui/BIuyeLQ6LTQtfQ/Y2NjIXIGiDnLWW83vwz9nrWdVXTR2//796NSpE6ysrPLti5yLguaXdCXV4eXlBQcHB/Tu3VumjNanirJgqipITEzE6NGjhQRR8+bNceTIESaIVJi0fODbt2+FGcrySMeZqjCOpKJ78eIFhgwZIpzos7e3x7Zt29QmQbRjxw4MGzYMU6ZMybedun1nUNaVuTY2NujZs2e+7YKCgoTbqvqdKv1+iI2Nzff7RHoMoq2trZKJVHliY2OFUnO9evVScjSKx3MXuWVmZgoX8OVFek6jefPmCoio7KlataqQHMvvfKf0bwtAvrMYSTUtXbpUSBBVqVIF+/btU6sEUUhICCZMmIC+ffvCx8cn37bFLcvIJFE50qVLFwDA06dPER4eLreNt7e3kFTp3LmzwmIraS9fvoSVlRVsbGxw584duW1SU1OFtWi6detWptfg+RzSg4vExEQ8fPgwz3Z3794Vbjdo0KDU4yoL4uPj8eLFCwAoN6WRSlKHDh2EGuCf1jbOSTrjTk9PT2UX8qtSpQqioqKQkZGB69ev59lOOu1WJBKV68/I/PTv3x9+fn75/pNendWqVSvhvv79+ys58tKRmZmJ27dvIzg4WEiIyPP3338Lt3POKlJFaWlpcHR0hL+/P4Cs79B9+/ahcuXKSo6MSpOtrS2ArL+Ja9euyW3z7t07PH36FED5HkdS8bx9+xYODg5CiaypU6di+fLlalVmLDIyEvfu3RNKZOVF+p1haGioshedLFu2rMDxhNT48eOF+8rb+qCFVaFCBcTFxSE0NFQmEZSTRCIRZmCZm5ur7EWM0iofYrEYnp6eebaTHoM0a9ZMbWam+vn5CeXw1fH4VHruws/PL9/kj/TchZaWVpkthV4Sjh49CisrK3Tu3DnPhOqzZ88QEBAAAAUmoVWZdJx67do14W/oU9JjOTMzM5U9r0HyrVmzBgcPHgSQdbHv4cOH1e4ztkKFCrh+/TqCgoLg7u6eZ7v79+8LZRuLel5DPb6pVYSdnR0MDAyQmZmJ1atX59r+4cMHYZp/ly5dynWioF69esKVBG5ubnLbrF27FpGRkdDQ0ICDg4MCo1MsOzs7IRGwatUquYOtpKQkrFu3DkDWyfJ27dopNEZlefLkiVoPwitWrCism3LmzBlh3YCc3rx5I6w1Ymdnp7JlHmxtbYW1M7Zs2SK3Bvzdu3dx6NAhAFl9Ubt2bYXGqChaWlowNDTM9580qa6pqSncp6onATt37ix8n7i4uAgLLef06tUr7Ny5E0DWrF1Vv4pvzZo1uHfvHgCgd+/ecHFx4ZXwaqB27dpCgtjZ2TlXzXeJRILVq1dDLBbD1NQUAwYMUEaYpCTp6emYNm0aoqKiAGStgzlp0iQlR6V40gsmMjIyhLH1py5cuCBc9T1o0KB8S/6WZzo6OgWOJ6S0tbWF+/IrxVae/e9//xNur1y5Uu5JzL/++ktItI8dO1Zl+6Jjx45CMnDDhg0yVQukPDw8hESAOi0mLp3pIBKJYG1treRoFE9amjIuLg4bNmyQ2yYoKEg42dulSxeVrgbSokULiMVipKWlCSVKc0pJScHixYsBZM2myfk5o26knxOvXr0S3h85PXnyBKdPnwYAjB49WmU/Xym3K1euYNeuXQCyzhUfOHBAZc/l5MfU1BSdOnUCAJw8eVLuBSvJyclYvnw5gKyLxIt6PMckUTlSoUIFTJ06FQDg7u4OJycnPHnyBLGxsbh58yaGDx+OkJAQ6OrqCu3KK01NTfz4448AsmZHzZs3D8+ePUNsbCwePHiAn3/+WTjxPWbMGJUuiVK9enVMmDABQNYVOfb29rh8+TIiIyMRHR0NDw8P2NvbC3VsFyxYoDYn+3KWDlO3es9Sc+fOhampKTIzM/HTTz9h+/btCAkJQUxMDM6dO4fhw4cjISEBFSpUwKxZs5QdbqkxMDDAzJkzAWSVtvj+++/h4eGBiIgIvH37Fjt27MBPP/2EjIwMmJmZ4ZdfflFyxKQoenp6mDNnDoCszwx7e3t4enoiMjIS//33Hw4cOIChQ4fiw4cP0NXVxYoVK1T6atfnz58LF1/UrVsXixcvRmpqKpKSkvL8l1/ZVypf5s+fDw0NDQQHB2PYsGG4efOmUBrHyclJmJXq5OQEAwMDJUdLinTkyBHhSuY+ffrg+++/z/dzQbpwtKpp1qwZBg4cCCDrApwJEybg3r17iI2NxYsXL7BmzRrMnj0bQNZnqJOTkxKjJUVq2bIl+vXrBwC4ffs2Ro8eDV9fX8TGxuLZs2dYtGiRkFhs27Ythg4dqsxwS5WWlhaWLVsGDQ0NhIeHw97eHmfOnEFERATCwsKwbds24bjjq6++Uqs1L6XHp5UrV1bZi/Py06dPH+GCVVdXVzg5OeHff/9FbGwsQkNDsX//fgwbNgzJyckwNjYWxuiqqmHDhsIMmfXr12PHjh0ICQlBdHQ0vLy88MMPP8Df3x8aGhpYtmxZget8qbL27duje/fuALIS8Rs2bMDbt28RFRWF48ePw8HBAenp6ahVq5ZKf76SrLS0NCHpoaOjgzVr1kBfXz/f8akql7CcM2cO9PX1kZqaiuHDh+PAgQN48+YNYmJicOXKFQwZMkS4WGHWrFmoXr16kR5fNS8bVmGjRo3Cy5cvcfToUVy+fFlYFFFKS0sL69atg5WVlZIiLDkODg54/PgxLl68iFOnTuHUqVO52owYMUI4MazKJk+ejKSkJLi6uuLp06dyD0i1tbWxYMEC9O3bVwkRKkdYWJhwW5WvQMqPmZkZ9u7di/Hjx+Pdu3dYv3491q9fL9PG1NQUzs7OqFatmpKiVIwhQ4YgOjoazs7OCA0NlZssr1WrFrZv367yfUGyvvvuO8TExGDDhg148eIFJk+enKuNiYkJNm7cqPKzEvft2ydc/RwSEoKOHTsWah8bG5vSDo0UwNraGitXrsSiRYsQGBiIsWPH5mrj4OCQ58LKpLr27t0r3HZ3d8+3jIWU9AIlVbN8+XIkJyfj8uXL8Pb2hre3d642TZo0gYuLi1ouMK7OVq5ciaSkJHh7e8PHx0fumgAdOnSAs7OzSl9wAgCdOnXCH3/8gQULFiAsLEzuyX5LS0ts2rRJZcvCyyM9PjU2NlZyJMohEong7OyMqVOn4vbt23LPWQFZ1U/+/PNPlV23K6dVq1Zh1KhRCAoKwrp163LNUtXV1cWyZcvQrVs3JUVYdqxevRpjx47Fo0ePsG3bNmzbtk1me5UqVeDq6qqWCVh15e7uLpT/TUtLg729fYH7TJkyRWUv4mnYsCE2b96M6dOnIy4uDsuWLcvVRlNTEzNnzsTIkSOL/PhMEpUzGhoaWL58Obp27YpDhw4hICAACQkJMDU1hY2NDcaNG6cytTk1NTWxYcMGdO/eHceOHcPTp0+RnJyMSpUqoVWrVhgxYgRat26t7DAVQiQSYe7cuejTpw8OHjyIf//9F1FRUdDQ0ECNGjXQvn17jBw5Ui0GWTnlLJWjrkkiIKs81sWLF+Hm5oZLly4hODgYYrEYNWvWRI8ePTB8+HC1SYpMnjwZXbp0wf79+3H37l1ERkZCX18f9erVg52dHUaMGAF9fX1lh0lK4OjoiE6dOmHfvn3w9fVFZGQkdHV1UbduXXTv3h3Dhw+HqampssMsddJ1iEh9ffvtt7C0tMSuXbvg4+ODmJgYGBgYwMrKCsOGDVPrevjqKjY2Fm/evFF2GGWGjo4OnJ2d4enpiWPHjuHhw4f48OEDjIyM0KRJE/Tt2xcDBw4UykGT+tDT08PWrVtx+fJlnDhxAo8ePUJCQgIqVqyIxo0bY9CgQejbt6/alEHq27cvWrRogd27d+PGjRsIDw+Hrq4u6tevj/79++O7775TmwoXUtLjU3U+Nq1QoQJ27dqFy5cv4/Tp0wgICEBcXBz09PRQr149YdwtLRWu6ipXrozjx49j3759cHd3x+vXr5GZmQlzc3N06tQJP/74o1qWzpKnYsWKOHToEA4dOoRz587h5cuXSEtLg7m5Obp164Zx48ZxDVU1w2PX3Dp27Ijz589j3759uHbtGt6+fQsgqwpVu3btMGrUqGIvPyOS5LUiGBEREREREREREREREaks1Z4DTURERERERERERERERHIxSURERERERERERERERKSGmCQiIiIiIiIiIiIiIiJSQ0wSERERERERERERERERqSEmiYiIiIiIiIiIiIiIiNQQk0RERERERERERERERERqiEkiIiIiIiIiIiIiIiIiNcQkERERERERERERERERkRpikoiIiIiIiIiIiIiIiEgNMUlERERERERERERERESkhpgkIiIiIiKiMi8zM1PZIZQoVXs9RERERERUPmkpOwAiIiIiIkVo1KiRcFskEuH69euoVq1aofadNGkSrl69Kvz8/PnzEo+vLHJ2doaLi0uR9zM3N4eXl1eJxJCYmIj169ejWbNmGDhwoMy27t27IywsDIMGDcLq1atL5PkU4fbt2/jrr7+we/dumftPnjyJ+fPnAwCuXr2KWrVqKSO8IgkNDUWPHj2Kte++fftgY2NTwhEpX159UhZeb3HfY0+fPs319weUjddERERERJ+HM4mIiIiISO1IJBK4u7sXqm1CQgJu3LhRyhFRXvr06YMDBw5ALBYrO5QSceTIETg4OCAkJETZoRAREREREXEmERERERGpJw8PD/z4448Ftrt8+TLS09NLP6Ay7sKFC6hRo0ah2mpolNy1aJGRkXluMzc3h6amJipXrlxiz1fa8ns9RkZGqFOnDgBAW1tbUSGVmPHjx2P8+PGFbq+np1eK0ZQNS5cuxf/+9z8A5fv1NmrUCH5+fgCAu3fvwtHRUckREREREVFJYZKIiIiIiNSKhYUFAgMD8eDBA4SHh6N69er5tr948SIAwMzMDFFRUYoIsUzS09ODoaGhssOQsX//fmWHUKLs7OxgZ2en7DCKTVtbu8y9R5RNR0dHJfpEQ0NDeB3lOdlFRERERLmx3BwRERERqRVbW1sYGhpCIpHAw8Mj37axsbH4559/oKWlhV69eikoQiIiIiIiIiLFYJKIiIiIiNSKrq4uunfvDgAFJonc3d2RkZGBdu3aoVKlSgU+dlBQEBYtWoSePXuiWbNmaN26NQYPHoy//voLHz9+zHffFy9eYMWKFRgwYADatm0LS0tL2NjY4Pvvv4ezszPi4uJy7RMaGopGjRqhUaNGCA0NRUREBFasWIGePXvC2toa7dq1w/jx48vEmkofP36Eq6srfvjhB7Rq1QpWVlbo1KkTHB0dcfr0aWRmZsq07969Oxo1aiT8PH/+fDRq1AgjR47M1WbevHky+86bN0+4XyKR4Pjx4xgyZAhatmyJNm3aYOjQobh06ZLQ/t27d1i8eDG6du0KKysr2NraYvHixYiOjs7z9YSFhWHdunUYPHgw2rVrB0tLS7Rp0wYDBgzA77//jvDwcJn2J0+eRKNGjeDi4iLsL/3d+fj4yLSR/j7luXbtGiZPnoxOnTrBysoKNjY2GDlyJA4dOpRnWcSRI0eiUaNGcHZ2hlgsxqFDhzBkyBC0bt0aX331FQYOHIgdO3YgJSUlz9db2nK+l0NCQrBnzx507doV1tbW6N69O3bv3g0g+3d+7Ngx3Lx5E/3794eVlRU6duyImTNnyjxmYmIiduzYAXt7e7Ru3RrW1tbo1q0bZs+ejQcPHsiNw8fHR4gjLS0N69evR4cOHdCsWTP07t0b586d++zXKv09d+zYEQBw8+ZNjBo1Cm3atBE+Mw4cOJDn70PaB4cOHUJiYiJ+//13dO/eHdbW1ujRowdmzpyJp0+fFioWHx8fjB07Fq1bt0aLFi3w3XffYc+ePUhLS/vs10lERERE5QPLzRERERGR2unbty/OnTtXYMk5aam5fv365XnSXmr37t1Yu3atTLIjNTUVjx49wqNHj3Do0CHs2LEDX375Za59XVxc4OLiAolEInN/XFwc4uLi8PDhQ5w4cQKHDh3Kc12gR48eYcmSJYiPjxfuS0tLw7Vr13Dt2jVMnz4dEyZMyPc1lJa4uDiMHDkSgYGBMvdHRUXh+vXruH79Ok6cOIEdO3ZAX1+/xJ5XLBZj6tSpMgkhAPDz84Ofnx+WLFmCpk2bwtHRUabfwsPDceTIEdy+fRunTp2CsbGxzP7Hjh3D0qVLcyVlPnz4gA8fPuDZs2c4fvw49u7di6ZNm5bIa/n48SNmzpyJq1evytwfFxcHX19f+Pr64uDBg9i2bRvMzc3lPkZaWhrGjBmDO3fuyNz/9OlTPH36FO7u7nBzc1N6eTRXV1ccPnxY+DksLAxmZmYybR48eCDzO4iOjpaJ29/fHz///HOuZN1///2Hs2fP4uzZsxg9ejTmzZuX5xpaK1eulIkjODgYtWrV+uzXl9Phw4fx66+/yvztSz8zTpw4gZ07d+aZoE5ISMCQIUMQFBQk3BcaGorQ0FBcvHgRS5cuhb29fZ7PffDgQbi6uso8d0BAAAICAnD8+HG4urqiatWqJfAqiYiIiKgs40wiIiIiIlI7HTt2RIUKFfItORcREYF79+5BR0enwFJzx44dw+rVq5GZmYm2bdvC1dUVd+7cwbVr17BixQqYmZkhLCwMY8eORWxsrMy+Hh4ecHZ2hkQiQceOHbF3717cuHEDN27cwN69e9G1a1cAWbNd/vzzzzxjmD9/PiQSCRYtWgQvLy/cunULa9euhYmJCQDA2dkZb9++LXwnlaD169cjMDAQBgYGWLx4Ma5cuYJ//vkHp06dQv/+/QEAvr6+2Lt3r7DPhQsX4OfnJ/y8dOlS+Pn54a+//ir083p4eODSpUvo3bs3Tpw4gZs3b2LTpk2oUKECAGDjxo2YNGkS9PX1sW7dOty6dQuenp4YPnw4AODt27c4cOCAzGM+fPgQixYtQnp6OqysrLB9+3Z4e3vj1q1bOHz4MAYOHAggK2G0evVqYb/+/fvDz88P48ePBwDUrFlTSFa1bt26wNcyY8YMIUHUp08fHDlyBD4+Prh48SIcHR2hpaWFwMBAjBkzBomJiXIfY//+/bhz5w4GDRqEkydPwsfHB0ePHkWHDh0AAE+ePBFm7CjT4cOH0bZtW5w5cwY3btzAsmXLcv0NHj9+HJUrV4arqytu376NLVu2CLPMQkJC4OjoiPDwcBgaGmLOnDnCe27v3r1o27YtAGDv3r1Yv359vnH07t0bly5dgpeXF3799Ve0aNGixF5nfHw8li9fDhMTE6xatQq3bt3CpUuXMGbMGIhEIjx+/Bg///xznvtv3boVQUFB+Prrr3H69GncuXMH27dvR/369SEWi7Fo0aJcCcGcdu3ahapVq+KPP/7A7du3cfnyZTg6OkJDQwMvXryAk5MTxGJxib1eIiIiIiqbOJOIiIiIiNSOjo4OevbsiZMnT8LDwwM//vhjrjYXL16ERCKBra0tjIyM8nyshIQErFq1CgDQs2dPODs7y8xM+P7779G+fXsMGDAA4eHh2LJlCxYuXChs37lzJwCgYcOG2LZtG3R0dIRt1apVQ9u2bTF48GA8fvwYf//9d55xpKen4+DBgzIzV/r374+KFSvC0dERGRkZuHLlCsaMGVNwB8mRkpKCpKSkQrU1MDCASCQSfr58+TIAYNy4cUICBgBMTU2xZs0aREREwMfHBxcuXBBmO306o0hHR6fIM1xSU1NhZ2cnk1z7+uuvERoairVr1yI+Ph76+vo4d+4cateuLbRZvHgxAgIC4O/vj9u3b8vMwNq1axckEgkqVaoEV1dXVKxYUdhWpUoVtGjRAomJifD09MS///6LlJQU6OnpQUtLC1paWtDW1gYAiESiQr8eb29veHl5AQDGjBmDuXPnCttMTEwwc+ZMWFlZ4eeff0ZwcDC2bNmCOXPm5Hqcjx8/yt1/+/btsLOzw7t373Dp0iVMmTKlUHF9Kj09vdDvEW1tbZn3ek4GBgZwcXER+nbIkCFy261atUpIcPXo0UO4f926dYiLi4OOjg727dsHKysrYVu7du3Qpk0b/Pzzz/D09MTOnTsxYMAANGzYMNfjm5ubY/369dDSyjpsHjp0aKFeW2Glp6fDwMAAbm5uwgzDKlWqYO7cuahSpQrWrFmDf//9F56enujZs2eu/ZOTk/Htt98Knz8A0LVrVzRv3hzffvst/vvvP6xevRpnzpyR+/wmJiY4ePCgMDuqcuXKmDlzJszMzLBy5Uo8ePAAly5dQp8+fUr0dRMRERFR2cKZRERERESklr755hsAWWWr3r17l2v7hQsXAGSVpsvP2bNnhRPjeZWuqlWrFkaMGAEgaz2SjIwMAFnl0Lp27YqBAwdi0qRJck+aa2hoCDNN3r9/n2ccXbp0kVvarEOHDsLjFlQyLz99+/ZFy5YtC/UvLCxMZl/p+iby1vgRiURYtGgR9u7di23bthU7vryMGzcu1305Z+707t1bJkEk1bx5cwBZM8pyatmyJQYPHozJkyfLJIhyks5UEYvFMmXsiuvo0aMAADMzM8yYMUNum969ewuJhKNHj+Za4wnIei85Ojrmul9HR0dItnzOe2T79u2Ffo9s3749z8fp2LFjnn0rZWJigvbt2+e6//3790JScsSIETIJIilNTU0sW7YMOjo6kEgkMiXlcurVq5eQICotY8aMkVuC0sHBQUje5LUOkqGhIX755Zdc95uamsLJyQkA8OzZM5lydDmNHz9ebvm8kSNHok6dOgCA06dPF+p1EBEREVH5xSQREREREaml9u3bw8TEBBKJJNeaNW/evMGjR49gYGCAbt265fs4Pj4+ALJOzFaqVAlJSUly/zVr1gwAkJSUhGfPngHIOmk/ZcoU/P7770LSKiexWIzAwEDhxL00uSSPNKnxKW1tbeGE+8ePH/N9LaWlTZs2AIADBw5g8uTJuHjxokzypGHDhmjXrl2ea+kUl7a2Npo0aZLr/sqVKwu381ozSDp77NN1h0aPHo2VK1cKSb9PBQcH4+XLl8LP+f3OCuvff/8FAHTv3l2YiSSP9D2UkJAgvMdyqlOnDkxNTeXuK+0TZb1HcpL3O/tU48aNZWarSd27d09YY6d379557l+5cmUhmefr61vsOD6XvL97IOuzQVpqMq+ScfnNcsz5uXX79m25bezs7OTeLxKJ0KVLFwCy/UlEREREqonl5oiIiIhILWlpaaF37944cuQI3N3dZUrOSWcR9ezZE3p6evk+jjSB8/79e7Rs2bJQzx0eHp5rhkN0dDTu3LmDoKAgvH37FiEhIXj16hWSk5ML9Zh5LW4PQJhJ9Dkne69evSp31kFhzJs3D/7+/nj//j08PT3h6ekJTU1NWFtbo3PnzujVqxcaNWpU7NjyYmRkJDepkjO5YGxsLHdfeTPCckpISMDt27cRGBiIN2/e4O3bt3j58iU+fPgg0+5zT7AnJiYiISEBANCgQYN82+bc/u7dO1haWspszytBBJTMe2TKlCnCDJbPkd97uaA2OWcFFqa/bt68KXcmYWHj+Bza2tr44osv8txet25dAFlrF338+DFXCUYLC4s89zU1NUXFihURHx+P8PDwXNt1dHTy/XuWPndCQgI+fPhQ4MwuIiIiIiq/mCQiIiIiIrXVp08fHDlyBP7+/nj37h1q1KgBIGs9IgDo169fgY+RmJhY5OfNuU9qaip+++03HDt2LFeJMF1dXdjY2EAsFguzSfJS2mWxPkf9+vVx/vx5bNu2DRcvXkRMTAwyMzPx4MEDPHjwAM7OzmjdujVWrlyJevXqldjzGhgYlNhjSYnFYjg7O2PXrl1ITU2V2aatrY0WLVqgQoUKuH79eok8X841fgp6PTmTCPLWBspvFlJZoqurW+w2Of+2CttfeSViCxPH5zAyMpI7G0oq5+8zISEhV5KoQoUK+T6+np4e4uPj5X5GffpY+W1PSUlhkoiIiIhIhZXdI0kiIiIiolJmY2MDMzMzREVFwcPDAw4ODggKCkJgYCBMTEyEdVryI51p1Lx5c2HtmKKYPn06rl69CgCwtLSEra0tGjZsiC+//BJffPEFtLS0sGHDhgKTRGVdlSpVsHDhQixYsACPHj3CrVu3cPv2bdy/fx8ZGRm4e/cufvzxR7i7uxd4AluZVq1ahX379gEAvvjiC3Tv3h2NGjVCgwYN0LBhQ+jo6ODYsWMlliTKmegoaFZZURJKqurT/sprphiQ3V/Ker99mmT8lDQ+kUgEExOTIu8vfb/Im0GWkpJSqOcGCk5GEREREVH5xjWJiIiIiEhtaWhoCOuWeHh4AMieRdS7d+9CzbyoWbMmACAsLCzfdvLKePn5+QkJopEjR+LkyZOYOnUqvvnmG1hYWAizg96/f1/IV1T2aWhooHnz5pg0aRLc3Nzw999/Y+DAgQCySoVduXJFuQHm4927d3BzcwMA9OrVC+fPn8fs2bPRv39/WFpaCiXbSvL3ZWRkJJykz7nWkTxBQUHCben7Ut3kXNeqoP6SbldWXyUnJyM2NjbP7a9fvwYAmJmZCe+tnN6+fZvnvtHR0UKZQnlrfaWmpiI6OjrP/V+9eiU8d1lO2hIRERHR52OSiIiIiIjUWp8+fQAA/v7+CA8Ph7u7O4DClZoDgNatWwPIOinr7++fZ7vt27ejdevW6N+/P968eQMAuH//vrB9yJAhcvcTi8Xw8fGR+bk8efjwIYYOHYq2bdvixYsXubZXqlQJCxYsEH6OiIhQZHhF4u/vL/T/4MGDoampKbfdnTt3hNufJgfzKy8mj0gkQqtWrQAAXl5eSE9Pz7OtNNFpaGiY73o1qqxly5bCelKXLl3Ks11MTIwwO69FixYKiU2eGzduyL0/MzNTmI3WtWtXuW3+/vvvPNeQkiafRSIRbG1t5ba5efOm3PvT09Ph5eUFIPvzjYiIiIhUF5NERERERKTWWrVqherVq0MikWDLli149eoVqlWrVuiTowMHDhSu8l++fDk+fvyYq82bN2+we/duJCQkIC0tDbVr1wYAmSRDzlkgObm4uCA4OFj4Ob8kQVlUo0YNPHz4EPHx8di/f7/cNk+fPhVu16lTR2abdDZVWXjdOdd9yuv3deLECdy+fVv4OS0tTWa79Hf+6f35sbe3BwBERUVh/fr1ctt4enoKiYGBAweWm/WHSlqlSpXQs2dPAICbmxsCAgJytRGLxVi6dCnS09MhEokwePBgRYcpcHFxQXx8fK77t2/fjnfv3gGAMNPuU2/fvpX7NxUdHQ0XFxcAQIcOHVCtWjW5+zs7O8t9bmdnZ0RFRQHIO3lNRERERKqDaxIRERERkVoTiUT4+uuvsWfPHmFNoW+++UaYjVCQKlWqYOrUqVi7di0ePXoEe3t7ODk5oWXLlkhLS4OPjw82btyIuLg4iEQi/PLLL8Jsko4dO0IkEkEikWD58uVIT09H27ZtIRKJEBgYiAMHDsDb21vm+ZKSkqCrq1uynVAIKSkpMuuUFERPTw+ampowMzND//79cfLkSRw5cgTp6ekYOnQoatWqhaSkJPj6+mLDhg0AshJK3bp1k3kcExMTREdH4+rVq+jVqxdEIpHcNVYUoVWrVtDT00NKSgpcXFygr68PW1tb6Onp4fXr1zh+/DhOnz4ts8+nfSZdWyY6Ohq3bt2ClZUV9PT08v2ddu/eHd27d4eXlxdcXV0RHh6OH3/8EXXr1kVsbCzOnj2LnTt3AgBq166NGTNmlOjrLqz09PQivUe0tLRK5b08Z84c/PPPP/jw4QNGjx6NSZMmwc7ODkZGRggMDMTWrVuF2V4ODg5o1qxZicdQWG/fvsWwYcMwe/ZsNG/eHLGxsTh48KBQ1nDQoEHCTDJ5Vq1ahYiICHz//feoUKEC7t27h99//x2RkZHQ0dHBwoUL5e4nEokQFhaGYcOGYc6cObC2tkZMTAz27dsnfA727dsX7du3L/kXTURERERlCpNERERERKT2vvnmG+zZs0co3dS3b98i7T927FgkJSVh69atCAwMhJOTU6422traWLJkCTp37izc17BhQ4wbNw47duxATEwMZs+enWs/Y2NjfP/993B1dQUABAcHo1KlSkWKryQUtU82b94szOhYsGABXr16hQcPHuDkyZM4efJkrvZVqlTBtm3bcq29YmNjgwsXLuD69eto3749zM3NhVJYimZqaop58+Zh6dKl+PjxI5YtW5arjY6ODsaMGYNt27YBAEJCQmSSEG3atIGmpiYyMzMxZswYAFkn+r/99tt8n/uPP/7ArFmz4OXlhYsXLwprZ+VkaWmJTZs2wcjI6HNeZrFt374d27dvL3T7Hj16YMuWLSUeR+3atbFr1y5MmjQJUVFRWLNmDdasWZOr3ZgxYzBz5swSf/6i6N+/P86ePYvx48fn2tanTx8sXbo0z33btm2LN2/eYOfOnUKSUKpChQr4888/8cUXX8jdV19fH46Ojti4cSMcHR1zbbe1tcXKlSuL+GqIiIiIqDxikoiIiIiI1F7z5s1hbm6OsLAw1K1bF9bW1kXaXyQSYerUqejduzfc3Nzg6+uLiIgIiMVi1KxZE+3atcOoUaPQoEGDXPvOnDkTlpaWOHToEJ48eYKkpCQYGBigTp066Ny5M4YNG4YKFSrgyJEjSEpKwpUrV9CyZcuSeukKYWxsjAMHDuDYsWNwd3dHYGAgEhISYGhoiDp16qBbt24YNWoUjI2Nc+27ePFiaGlp4fr160hOTgYApKamKmU2FQAMHToU9erVw549e+Dv748PHz5AT08P5ubmsLGxwYgRI1CvXj24u7sjJCQEV65cwf/+9z9h/y+//BLr16/H5s2bERISAj09Pbx//77A5zU0NMTWrVvh6emJkydP4uHDh4iLi4OpqSm+/PJLDBgwAN98802uJJu6atasGTw8PODm5oarV6/i9evXSE9PR/Xq1dG6dWsMGTJEqTOIpKZOnQo7Ozu4urri2bNn0NfXR+PGjTFs2DD06NEj3zWszM3NsWnTJmzevBlXrlxBXFwczM3N0a1bN4wePTrPMnNSEydOxJdffik8t0gkQuPGjWFvb48BAwYUef0sIiIiIiqfRJK8VrokIiIiIiIioiIJDQ1Fjx49AMifJXby5EnMnz8fAHD16lXUqlWrSI/fvXt3hIWFYdCgQVi9enXJBF0EPj4+GDVqFABg3759sLGxUXgMRERERFRyCldonYiIiIiIiIiIiIiIiFQKk0RERERERERERERERERqiGsSEREREREREZWCtLQ0JCUlAQD09PSgqamp5IiKRywW4+PHjwCAlJQUJUdDRERERCWJM4mIiIiIiIiISsGSJUvQsmVLtGzZEnfv3lV2OMX2/Plz4XU4OjoqOxwiIiIiKkFMEhEREREREREREREREakhkUQikSg7CCIiIiIiIiIiIiIiIlIsziQiIiIiIiIiIiIiIiJSQ0wSERERERERERERERERqSEmiYiIiIiIiIiIiIiIiNQQk0RERERERERERERERERqiEkiIiIiIiIiIiIiIiIiNcQkERERERERERERERERkRpikoiIiIiIiIiIiIiIiEgNMUlERERERERERERERESkhv4PpP7+CgorYOEAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# before plotting a CDF, we transform the data into a cumulative distribution function.\n",
    "# x-axis - mean error, y-axis - percentage of stations that have a mean error less than or equal to the mean error of a station.\n",
    "# for example, if there are 115 stations, and 10 stations have a mean error less than or equal to the mean error of station A, then the percentage of stations that have a mean error less than or equal to the mean error of station A is 10/115 = 0.087\n",
    "# we do this by sorting the mean errors from lowest to highest, and then for each mean error, we calculate the percentage of stations that have a mean error less than or equal to that mean error.\n",
    "\n",
    "# for CHIMERE\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "mean_errors = stations_mean[\"Error Mean\"].sort_values().values\n",
    "percentages = []\n",
    "for i in range(len(mean_errors)):\n",
    "    percentages.append((i + 1) / len(mean_errors))\n",
    "percentages = 100 * np.array(percentages)\n",
    "ax.plot(mean_errors, percentages, label=\"CHIMERE\", linewidth = 3, color = \"blue\")\n",
    "# for NN\n",
    "mean_errors = stations_mean_NN[\"Error Mean\"].sort_values().values\n",
    "percentages = []\n",
    "for i in range(len(mean_errors)):\n",
    "    percentages.append((i + 1) / len(mean_errors))\n",
    "percentages = 100 * np.array(percentages)\n",
    "ax.plot(mean_errors, percentages, label=\"NN\", linewidth = 3, color = \"red\")\n",
    "# for RF\n",
    "mean_errors = stations_mean_RF[\"Error Mean\"].sort_values().values\n",
    "percentages = []\n",
    "for i in range(len(mean_errors)):\n",
    "    percentages.append((i + 1) / len(mean_errors))\n",
    "percentages = 100 * np.array(percentages)\n",
    "ax.plot(mean_errors, percentages, label=\"RF\", linewidth = 3, color = \"green\")\n",
    "# for XGB\n",
    "mean_errors = stations_mean_XGB[\"Error Mean\"].sort_values().values\n",
    "percentages = []\n",
    "for i in range(len(mean_errors)):\n",
    "    percentages.append((i + 1) / len(mean_errors))\n",
    "percentages = 100 * np.array(percentages)\n",
    "ax.plot(mean_errors, percentages, label=\"XGB\", linewidth = 3, color = \"yellow\")\n",
    "ax.set_title(\"CDF of Mean Estimation Errors For All Stations\", fontsize = 20)\n",
    "ax.set_ylabel(\"Percentage of Stations\", fontsize = 20)\n",
    "plt.legend(fontsize=20)\n",
    "if pollutant == 'NO2':\n",
    "    ax.set_xlabel(\"Mean Estimation Error [ppb]\", fontsize = 20)\n",
    "    plt.xticks(np.arange(-10, 13.1, 1), fontsize=20)\n",
    "    plt.xlim(-10, 13)\n",
    "else:\n",
    "    ax.set_xlabel(\"Mean Estimation Error [micro gram/m^3]\", fontsize = 20)\n",
    "    plt.xticks(np.arange(-24, 29.1, 2), fontsize=20)\n",
    "plt.yticks(np.arange(0, 101, 10), fontsize=20)\n",
    "ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=2) # passing a dotted vertical line through zero\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}